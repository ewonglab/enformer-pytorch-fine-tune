Global seed set to 42
2023-10-04 23:29:01,715	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-04 23:29:12,753	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-04 23:29:12,757	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 23:29:12,799	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=173893)[0m Starting distributed worker processes: ['174632 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=174632)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=174632)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=174632)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=174632)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=174632)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=174632)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=174632)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=174632)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=174632)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/lightning_logs
[2m[36m(RayTrainWorker pid=174632)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=174632)[0m 
[2m[36m(RayTrainWorker pid=174632)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=174632)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=174632)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=174632)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=174632)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=174632)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=174632)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=174632)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=174632)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=174632)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=174632)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=174632)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=174632)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=174632)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=174632)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=174632)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=174632)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=174632)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=174632)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=174632)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=174632)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=174632)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=174632)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=174632)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=174632)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=174632)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=174632)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=174632)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=174632)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-04 23:33:17,702	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000000)
2023-10-04 23:33:19,895	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.192 s, which may be a performance bottleneck.
2023-10-04 23:33:19,895	WARNING util.py:315 -- The `process_trial_result` operation took 2.194 s, which may be a performance bottleneck.
2023-10-04 23:33:19,895	WARNING util.py:315 -- Processing trial results took 2.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-04 23:33:19,895	WARNING util.py:315 -- The `process_trial_result` operation took 2.194 s, which may be a performance bottleneck.
2023-10-04 23:36:36,894	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000001)
2023-10-04 23:39:55,239	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000002)
2023-10-04 23:43:13,714	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000003)
2023-10-04 23:46:31,095	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000004)
2023-10-04 23:49:48,491	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000005)
2023-10-04 23:53:06,426	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000006)
2023-10-04 23:56:23,959	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000007)
2023-10-04 23:59:41,543	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000008)
2023-10-05 00:02:59,166	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000009)
2023-10-05 00:06:16,738	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000010)
2023-10-05 00:09:34,736	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000011)
2023-10-05 00:12:52,207	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000012)
2023-10-05 00:16:09,645	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000013)
2023-10-05 00:19:26,952	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000014)
2023-10-05 00:22:44,308	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000015)
2023-10-05 00:26:01,730	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000016)
2023-10-05 00:29:19,435	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000017)
2023-10-05 00:32:37,225	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000018)
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=174632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           ptl/train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         ptl/val_accuracy ‚ñÉ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             ptl/val_aupr ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÅ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÇ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         ptl/val_f1_score ‚ñá‚ñÅ‚ñà‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             ptl/val_loss ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:              ptl/val_mcc ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:        ptl/val_precision ‚ñÅ‚ñà‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           ptl/val_recall ‚ñà‚ñÅ‚ñá‚ñÖ‚ñà‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           train_accuracy ‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:               train_loss ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       ptl/train_accuracy 0.56824
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           ptl/train_loss 0.56824
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         ptl/val_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             ptl/val_aupr 0.69606
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:            ptl/val_auroc 0.7768
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         ptl/val_f1_score 0.78723
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             ptl/val_loss 0.58596
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:              ptl/val_mcc 0.51439
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:        ptl/val_precision 0.70476
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           ptl/val_recall 0.89157
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                     step 1220
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       time_since_restore 3980.7165
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:         time_this_iter_s 197.7616
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:             time_total_s 3980.7165
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:                timestamp 1696426555
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:               train_loss 0.55366
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5feaa4cd_1_batch_size=8,layer_size=8,lr=0.0000_2023-10-04_23-29-12/wandb/offline-run-20231004_232936-5feaa4cd
[2m[36m(_WandbLoggingActor pid=174627)[0m wandb: Find logs at: ./wandb/offline-run-20231004_232936-5feaa4cd/logs
[2m[36m(TorchTrainer pid=210274)[0m Starting distributed worker processes: ['210404 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=210404)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=210404)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=210404)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=210404)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=210404)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=210404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=210404)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=210404)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=210404)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5dd87dcb_2_batch_size=8,layer_size=32,lr=0.0003_2023-10-04_23-29-28/lightning_logs
[2m[36m(RayTrainWorker pid=210404)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=210404)[0m 
[2m[36m(RayTrainWorker pid=210404)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=210404)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=210404)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=210404)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=210404)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=210404)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=210404)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=210404)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=210404)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=210404)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=210404)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=210404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=210404)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=210404)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=210404)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=210404)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=210404)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=210404)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=210404)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=210404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=210404)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=210404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=210404)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=210404)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=210404)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=210404)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=210404)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=210404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5dd87dcb_2_batch_size=8,layer_size=32,lr=0.0003_2023-10-04_23-29-28/checkpoint_000000)
2023-10-05 00:39:51,691	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.384 s, which may be a performance bottleneck.
2023-10-05 00:39:51,693	WARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.
2023-10-05 00:39:51,693	WARNING util.py:315 -- Processing trial results took 2.388 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:39:51,693	WARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:           train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:               train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         ptl/val_accuracy 0.51875
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         ptl/val_f1_score 0.68313
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             ptl/val_loss 0.693
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:              ptl/val_mcc 0.00593
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:        ptl/val_precision 0.51875
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                     step 61
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:       time_since_restore 218.82112
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:         time_this_iter_s 218.82112
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:             time_total_s 218.82112
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:                timestamp 1696426789
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:               train_loss 0.69731
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_5dd87dcb_2_batch_size=8,layer_size=32,lr=0.0003_2023-10-04_23-29-28/wandb/offline-run-20231005_003617-5dd87dcb
[2m[36m(_WandbLoggingActor pid=210401)[0m wandb: Find logs at: ./wandb/offline-run-20231005_003617-5dd87dcb/logs
[2m[36m(TorchTrainer pid=211621)[0m Starting distributed worker processes: ['211752 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=211752)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=211752)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=211752)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=211752)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=211752)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=211752)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=211752)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=211752)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=211752)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ac43595e_3_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-36-10/lightning_logs
[2m[36m(RayTrainWorker pid=211752)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=211752)[0m 
[2m[36m(RayTrainWorker pid=211752)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=211752)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=211752)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=211752)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=211752)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=211752)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=211752)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=211752)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=211752)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=211752)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=211752)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=211752)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=211752)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=211752)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=211752)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=211752)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=211752)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=211752)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=211752)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=211752)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=211752)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=211752)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=211752)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=211752)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=211752)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=211752)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=211752)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=211752)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=211752)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=211752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ac43595e_3_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-36-10/checkpoint_000000)
2023-10-05 00:43:46,882	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.508 s, which may be a performance bottleneck.
2023-10-05 00:43:46,883	WARNING util.py:315 -- The `process_trial_result` operation took 2.513 s, which may be a performance bottleneck.
2023-10-05 00:43:46,883	WARNING util.py:315 -- Processing trial results took 2.513 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:43:46,884	WARNING util.py:315 -- The `process_trial_result` operation took 2.513 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:           train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:               train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             ptl/val_loss 0.69978
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                     step 61
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:       time_since_restore 218.04575
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:         time_this_iter_s 218.04575
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:             time_total_s 218.04575
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:                timestamp 1696427024
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:               train_loss 0.61299
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ac43595e_3_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-36-10/wandb/offline-run-20231005_004013-ac43595e
[2m[36m(_WandbLoggingActor pid=211748)[0m wandb: Find logs at: ./wandb/offline-run-20231005_004013-ac43595e/logs
[2m[36m(TorchTrainer pid=213756)[0m Starting distributed worker processes: ['213889 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=213889)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=213889)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=213889)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=213889)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=213889)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=213889)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=213889)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=213889)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=213889)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_d80ce4b6_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-40-06/lightning_logs
[2m[36m(RayTrainWorker pid=213889)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=213889)[0m 
[2m[36m(RayTrainWorker pid=213889)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=213889)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=213889)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=213889)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=213889)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=213889)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=213889)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=213889)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=213889)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=213889)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=213889)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=213889)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=213889)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=213889)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=213889)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=213889)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=213889)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=213889)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=213889)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=213889)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=213889)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=213889)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=213889)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=213889)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=213889)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=213889)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=213889)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=213889)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_d80ce4b6_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-40-06/checkpoint_000000)
2023-10-05 00:47:42,147	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.507 s, which may be a performance bottleneck.
2023-10-05 00:47:42,149	WARNING util.py:315 -- The `process_trial_result` operation took 2.512 s, which may be a performance bottleneck.
2023-10-05 00:47:42,149	WARNING util.py:315 -- Processing trial results took 2.512 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:47:42,149	WARNING util.py:315 -- The `process_trial_result` operation took 2.512 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:           train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:               train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             ptl/val_loss 0.69554
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                     step 61
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:       time_since_restore 217.91338
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:         time_this_iter_s 217.91338
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:             time_total_s 217.91338
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:                timestamp 1696427259
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:               train_loss 0.65285
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_d80ce4b6_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-40-06/wandb/offline-run-20231005_004408-d80ce4b6
[2m[36m(_WandbLoggingActor pid=213886)[0m wandb: Find logs at: ./wandb/offline-run-20231005_004408-d80ce4b6/logs
[2m[36m(TorchTrainer pid=215362)[0m Starting distributed worker processes: ['215764 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=215764)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=215764)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=215764)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=215764)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=215764)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=215764)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=215764)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=215764)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=215764)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_84b7058b_5_batch_size=4,layer_size=32,lr=0.0537_2023-10-05_00-44-01/lightning_logs
[2m[36m(RayTrainWorker pid=215764)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=215764)[0m 
[2m[36m(RayTrainWorker pid=215764)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=215764)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=215764)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=215764)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=215764)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=215764)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=215764)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=215764)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=215764)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=215764)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=215764)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=215764)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=215764)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=215764)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=215764)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=215764)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=215764)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=215764)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=215764)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=215764)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=215764)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=215764)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=215764)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=215764)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=215764)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=215764)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=215764)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=215764)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=215764)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=215764)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_84b7058b_5_batch_size=4,layer_size=32,lr=0.0537_2023-10-05_00-44-01/checkpoint_000000)
2023-10-05 00:51:40,508	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.633 s, which may be a performance bottleneck.
2023-10-05 00:51:40,509	WARNING util.py:315 -- The `process_trial_result` operation took 2.636 s, which may be a performance bottleneck.
2023-10-05 00:51:40,509	WARNING util.py:315 -- Processing trial results took 2.636 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:51:40,509	WARNING util.py:315 -- The `process_trial_result` operation took 2.636 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:           train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:               train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             ptl/val_loss 0.70313
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                     step 121
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:       time_since_restore 220.19798
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:         time_this_iter_s 220.19798
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:             time_total_s 220.19798
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:                timestamp 1696427497
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:               train_loss 0.59791
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_84b7058b_5_batch_size=4,layer_size=32,lr=0.0537_2023-10-05_00-44-01/wandb/offline-run-20231005_004804-84b7058b
[2m[36m(_WandbLoggingActor pid=215761)[0m wandb: Find logs at: ./wandb/offline-run-20231005_004804-84b7058b/logs
[2m[36m(TorchTrainer pid=217501)[0m Starting distributed worker processes: ['217631 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=217631)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=217631)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=217631)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=217631)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=217631)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=217631)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=217631)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=217631)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=217631)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_a41792ea_6_batch_size=8,layer_size=16,lr=0.0007_2023-10-05_00-47-57/lightning_logs
[2m[36m(RayTrainWorker pid=217631)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=217631)[0m 
[2m[36m(RayTrainWorker pid=217631)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=217631)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=217631)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=217631)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=217631)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=217631)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=217631)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=217631)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=217631)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=217631)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=217631)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=217631)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=217631)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=217631)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=217631)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=217631)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=217631)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=217631)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=217631)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=217631)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=217631)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=217631)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=217631)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=217631)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=217631)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=217631)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=217631)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 00:55:34,254	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=217631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_a41792ea_6_batch_size=8,layer_size=16,lr=0.0007_2023-10-05_00-47-57/checkpoint_000000)
2023-10-05 00:55:36,809	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.554 s, which may be a performance bottleneck.
2023-10-05 00:55:36,811	WARNING util.py:315 -- The `process_trial_result` operation took 2.558 s, which may be a performance bottleneck.
2023-10-05 00:55:36,811	WARNING util.py:315 -- Processing trial results took 2.559 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:55:36,811	WARNING util.py:315 -- The `process_trial_result` operation took 2.559 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=217631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_a41792ea_6_batch_size=8,layer_size=16,lr=0.0007_2023-10-05_00-47-57/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                    epoch ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: iterations_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       ptl/train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           ptl/train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         ptl/val_accuracy ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         ptl/val_f1_score ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             ptl/val_loss ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:              ptl/val_mcc ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:        ptl/val_precision ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           ptl/val_recall ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                     step ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       time_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             time_total_s ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                timestamp ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           train_accuracy ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:               train_loss ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       training_iteration ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       ptl/train_accuracy 0.84556
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           ptl/train_loss 0.84556
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             ptl/val_loss 0.69439
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                     step 122
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       time_since_restore 413.42417
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:         time_this_iter_s 194.90614
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:             time_total_s 413.42417
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:                timestamp 1696427931
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:               train_loss 0.66892
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_a41792ea_6_batch_size=8,layer_size=16,lr=0.0007_2023-10-05_00-47-57/wandb/offline-run-20231005_005203-a41792ea
[2m[36m(_WandbLoggingActor pid=217628)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005203-a41792ea/logs
[2m[36m(TorchTrainer pid=219924)[0m Starting distributed worker processes: ['220055 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=220055)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=220055)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=220055)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=220055)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=220055)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=220055)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=220055)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=220055)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=220055)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_43398961_7_batch_size=4,layer_size=16,lr=0.0069_2023-10-05_00-51-55/lightning_logs
[2m[36m(RayTrainWorker pid=220055)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=220055)[0m 
[2m[36m(RayTrainWorker pid=220055)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=220055)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=220055)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=220055)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=220055)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=220055)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=220055)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=220055)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=220055)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=220055)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=220055)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=220055)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=220055)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=220055)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=220055)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=220055)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=220055)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=220055)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=220055)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=220055)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=220055)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=220055)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=220055)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=220055)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=220055)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=220055)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=220055)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=220055)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=220055)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=220055)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_43398961_7_batch_size=4,layer_size=16,lr=0.0069_2023-10-05_00-51-55/checkpoint_000000)
2023-10-05 01:02:47,093	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 01:02:49,703	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.610 s, which may be a performance bottleneck.
2023-10-05 01:02:49,704	WARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.
2023-10-05 01:02:49,705	WARNING util.py:315 -- Processing trial results took 2.614 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:02:49,705	WARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=220055)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_43398961_7_batch_size=4,layer_size=16,lr=0.0069_2023-10-05_00-51-55/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                    epoch ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: iterations_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       ptl/train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           ptl/train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         ptl/val_accuracy ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         ptl/val_f1_score ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             ptl/val_loss ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:              ptl/val_mcc ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:        ptl/val_precision ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           ptl/val_recall ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                     step ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       time_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             time_total_s ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                timestamp ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           train_accuracy ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:               train_loss ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       training_iteration ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       ptl/train_accuracy 1.13005
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           ptl/train_loss 1.13005
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             ptl/val_loss 0.69405
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                     step 242
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       time_since_restore 419.32859
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:         time_this_iter_s 199.27529
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:             time_total_s 419.32859
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:                timestamp 1696428368
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:               train_loss 0.67676
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_43398961_7_batch_size=4,layer_size=16,lr=0.0069_2023-10-05_00-51-55/wandb/offline-run-20231005_005913-43398961
[2m[36m(_WandbLoggingActor pid=220052)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005913-43398961/logs
[2m[36m(TorchTrainer pid=222342)[0m Starting distributed worker processes: ['222479 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=222479)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=222479)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=222479)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=222479)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=222479)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=222479)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=222479)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=222479)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=222479)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/lightning_logs
[2m[36m(RayTrainWorker pid=222479)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=222479)[0m 
[2m[36m(RayTrainWorker pid=222479)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=222479)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=222479)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=222479)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=222479)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=222479)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=222479)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=222479)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=222479)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=222479)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=222479)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=222479)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=222479)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=222479)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=222479)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=222479)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=222479)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=222479)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=222479)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=222479)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=222479)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=222479)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=222479)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=222479)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=222479)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=222479)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=222479)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=222479)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=222479)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:10:06,709	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=222479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/checkpoint_000000)
2023-10-05 01:10:09,283	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.573 s, which may be a performance bottleneck.
2023-10-05 01:10:09,283	WARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.
2023-10-05 01:10:09,283	WARNING util.py:315 -- Processing trial results took 2.577 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:10:09,283	WARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.
2023-10-05 01:13:28,916	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=222479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/checkpoint_000001)
2023-10-05 01:16:51,842	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=222479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/checkpoint_000002)
[2m[36m(RayTrainWorker pid=222479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                    epoch ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: iterations_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           ptl/train_loss ‚ñà‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         ptl/val_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÜ‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         ptl/val_f1_score ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             ptl/val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:              ptl/val_mcc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:        ptl/val_precision ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           ptl/val_recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                     step ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       time_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             time_total_s ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                timestamp ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           train_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:               train_loss ‚ñà‚ñà‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       training_iteration ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       ptl/train_accuracy 0.69519
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           ptl/train_loss 0.69519
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         ptl/val_accuracy 0.51875
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:            ptl/val_auroc 0.44805
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         ptl/val_f1_score 0.68313
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             ptl/val_loss 0.69244
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:              ptl/val_mcc 0.00593
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:        ptl/val_precision 0.51875
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                     step 484
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       time_since_restore 827.95305
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:         time_this_iter_s 202.64973
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:             time_total_s 827.95305
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:                timestamp 1696429214
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:               train_loss 0.65723
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_0eff4eed_8_batch_size=4,layer_size=32,lr=0.0001_2023-10-05_00-59-07/wandb/offline-run-20231005_010631-0eff4eed
[2m[36m(_WandbLoggingActor pid=222476)[0m wandb: Find logs at: ./wandb/offline-run-20231005_010631-0eff4eed/logs
[2m[36m(TorchTrainer pid=226383)[0m Starting distributed worker processes: ['226520 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=226520)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=226520)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=226520)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=226520)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=226520)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=226520)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=226520)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=226520)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=226520)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/lightning_logs
[2m[36m(RayTrainWorker pid=226520)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=226520)[0m 
[2m[36m(RayTrainWorker pid=226520)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=226520)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=226520)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=226520)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=226520)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=226520)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=226520)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=226520)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=226520)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=226520)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=226520)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=226520)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=226520)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=226520)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=226520)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=226520)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=226520)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=226520)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=226520)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=226520)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=226520)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=226520)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=226520)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=226520)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=226520)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=226520)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=226520)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=226520)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=226520)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:24:10,591	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000000)
2023-10-05 01:24:13,017	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.425 s, which may be a performance bottleneck.
2023-10-05 01:24:13,018	WARNING util.py:315 -- The `process_trial_result` operation took 2.429 s, which may be a performance bottleneck.
2023-10-05 01:24:13,018	WARNING util.py:315 -- Processing trial results took 2.429 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:24:13,018	WARNING util.py:315 -- The `process_trial_result` operation took 2.429 s, which may be a performance bottleneck.
2023-10-05 01:27:32,251	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000001)
2023-10-05 01:30:54,215	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000002)
2023-10-05 01:34:16,168	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000003)
2023-10-05 01:37:38,041	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000004)
2023-10-05 01:40:59,816	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000005)
2023-10-05 01:44:21,790	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000006)
2023-10-05 01:47:43,721	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000007)
2023-10-05 01:51:05,941	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000008)
2023-10-05 01:54:27,984	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000009)
2023-10-05 01:57:50,044	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000010)
2023-10-05 02:01:12,203	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000011)
2023-10-05 02:04:34,421	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000012)
2023-10-05 02:07:56,436	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000013)
2023-10-05 02:11:18,321	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000014)
2023-10-05 02:14:39,952	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000015)
2023-10-05 02:18:01,562	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000016)
2023-10-05 02:21:23,606	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000017)
2023-10-05 02:24:45,271	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000018)
[2m[36m(RayTrainWorker pid=226520)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           ptl/train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         ptl/val_accuracy ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         ptl/val_f1_score ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             ptl/val_loss ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:              ptl/val_mcc ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÖ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:        ptl/val_precision ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÉ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           ptl/val_recall ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           train_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:               train_loss ‚ñà‚ñá‚ñÑ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       ptl/train_accuracy 0.40867
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           ptl/train_loss 0.40867
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         ptl/val_accuracy 0.75625
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             ptl/val_aupr 0.88862
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:            ptl/val_auroc 0.90236
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         ptl/val_f1_score 0.80597
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             ptl/val_loss 0.62189
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:              ptl/val_mcc 0.56255
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:        ptl/val_precision 0.68644
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           ptl/val_recall 0.9759
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                     step 2420
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       time_since_restore 4050.7699
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:         time_this_iter_s 201.73945
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:             time_total_s 4050.7699
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:                timestamp 1696433287
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:               train_loss 0.28503
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_ffcf8c9d_9_batch_size=4,layer_size=8,lr=0.0002_2023-10-05_01-06-23/wandb/offline-run-20231005_012037-ffcf8c9d
[2m[36m(_WandbLoggingActor pid=226517)[0m wandb: Find logs at: ./wandb/offline-run-20231005_012037-ffcf8c9d/logs
[2m[36m(TorchTrainer pid=243915)[0m Starting distributed worker processes: ['244045 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=244045)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=244045)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=244045)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=244045)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=244045)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=244045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=244045)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=244045)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=244045)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_01f9a4a5_10_batch_size=4,layer_size=8,lr=0.0033_2023-10-05_01-20-30/lightning_logs
[2m[36m(RayTrainWorker pid=244045)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=244045)[0m 
[2m[36m(RayTrainWorker pid=244045)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=244045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=244045)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=244045)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=244045)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=244045)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=244045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=244045)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=244045)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=244045)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=244045)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=244045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=244045)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=244045)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=244045)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=244045)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=244045)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=244045)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=244045)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=244045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=244045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=244045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=244045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=244045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=244045)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=244045)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=244045)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=244045)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=244045)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:32:02,713	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=244045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_01f9a4a5_10_batch_size=4,layer_size=8,lr=0.0033_2023-10-05_01-20-30/checkpoint_000000)
2023-10-05 02:32:05,267	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.554 s, which may be a performance bottleneck.
2023-10-05 02:32:05,269	WARNING util.py:315 -- The `process_trial_result` operation took 2.558 s, which may be a performance bottleneck.
2023-10-05 02:32:05,269	WARNING util.py:315 -- Processing trial results took 2.559 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:32:05,269	WARNING util.py:315 -- The `process_trial_result` operation took 2.559 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=244045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_01f9a4a5_10_batch_size=4,layer_size=8,lr=0.0033_2023-10-05_01-20-30/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                    epoch ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: iterations_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       ptl/train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           ptl/train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         ptl/val_accuracy ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         ptl/val_f1_score ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             ptl/val_loss ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:              ptl/val_mcc ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:        ptl/val_precision ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           ptl/val_recall ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                     step ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       time_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             time_total_s ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                timestamp ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           train_accuracy ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:               train_loss ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       training_iteration ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       ptl/train_accuracy 0.96362
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           ptl/train_loss 0.96362
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         ptl/val_accuracy 0.48125
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             ptl/val_aupr 0.51875
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             ptl/val_loss 0.69341
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:              ptl/val_mcc -0.00593
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                     step 242
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       time_since_restore 419.02283
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:         time_this_iter_s 198.757
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:             time_total_s 419.02283
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:                timestamp 1696433724
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:               train_loss 0.68804
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-28-58/TorchTrainer_01f9a4a5_10_batch_size=4,layer_size=8,lr=0.0033_2023-10-05_01-20-30/wandb/offline-run-20231005_022829-01f9a4a5
[2m[36m(_WandbLoggingActor pid=244042)[0m wandb: Find logs at: ./wandb/offline-run-20231005_022829-01f9a4a5/logs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
2023-10-05 02:36:17,056	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-05 02:36:22,433	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-05 02:36:22,436	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-05 02:36:22,461	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=250130)[0m Starting distributed worker processes: ['250853 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=250853)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=250853)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=250853)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=250853)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=250853)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=250848)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=250848)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=250848)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=250853)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=250853)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=250853)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/lightning_logs
[2m[36m(RayTrainWorker pid=250853)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=250853)[0m 
[2m[36m(RayTrainWorker pid=250853)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=250853)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=250853)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=250853)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=250853)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=250853)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=250853)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=250853)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=250853)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=250853)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=250853)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=250853)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=250853)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=250853)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=250853)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=250853)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=250853)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=250853)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=250853)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:40:18,331	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000000)
2023-10-05 02:40:20,973	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.641 s, which may be a performance bottleneck.
2023-10-05 02:40:20,974	WARNING util.py:315 -- The `process_trial_result` operation took 2.644 s, which may be a performance bottleneck.
2023-10-05 02:40:20,975	WARNING util.py:315 -- Processing trial results took 2.644 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:40:20,975	WARNING util.py:315 -- The `process_trial_result` operation took 2.644 s, which may be a performance bottleneck.
2023-10-05 02:43:39,974	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000001)
2023-10-05 02:47:02,186	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000002)
2023-10-05 02:50:23,899	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000003)
2023-10-05 02:53:45,631	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000004)
2023-10-05 02:57:07,397	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000005)
2023-10-05 03:00:29,285	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000006)
2023-10-05 03:03:51,103	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000007)
2023-10-05 03:07:13,067	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000008)
2023-10-05 03:10:34,938	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000009)
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000010)
2023-10-05 03:13:56,690	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 03:17:18,556	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000011)
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000012)
2023-10-05 03:20:40,375	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 03:24:02,149	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000013)
2023-10-05 03:27:24,103	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000014)
2023-10-05 03:30:46,104	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=250853)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_02-36-12/TorchTrainer_8feb3aa0_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_02-36-22/checkpoint_000015)
2023-10-05 03:34:08,058	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 356, in <module>
    trainer_2.test(model, dataloaders=[test_dataloader_2])
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 742, in test
    return call._call_and_handle_interrupt(
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 785, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 932, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 469, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
