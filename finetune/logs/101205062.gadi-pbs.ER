Global seed set to 42
2023-11-15 01:34:46,588	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 01:34:56,341	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 01:34:56,344	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 01:34:56,373	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=3493123)[0m Starting distributed worker processes: ['3493842 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3493842)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3493842)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3493842)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3493842)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3493842)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3493842)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3493842)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3493842)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3493842)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/lightning_logs
[2m[36m(RayTrainWorker pid=3493842)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3493842)[0m 
[2m[36m(RayTrainWorker pid=3493842)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3493842)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3493842)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3493842)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3493842)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3493842)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3493842)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3493842)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3493842)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3493842)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3493842)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3493842)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3493842)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3493842)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3493842)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3493842)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3493842)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3493842)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 01:38:29,931	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000000)
2023-11-15 01:38:35,448	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.517 s, which may be a performance bottleneck.
2023-11-15 01:38:35,450	WARNING util.py:315 -- The `process_trial_result` operation took 5.519 s, which may be a performance bottleneck.
2023-11-15 01:38:35,450	WARNING util.py:315 -- Processing trial results took 5.520 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:38:35,450	WARNING util.py:315 -- The `process_trial_result` operation took 5.520 s, which may be a performance bottleneck.
2023-11-15 01:41:21,903	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000001)
2023-11-15 01:44:13,053	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000003)
2023-11-15 01:47:05,783	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 01:49:56,424	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000004)
2023-11-15 01:52:47,620	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000005)
2023-11-15 01:55:38,724	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000006)
2023-11-15 01:58:29,703	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000007)
2023-11-15 02:01:20,096	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000008)
2023-11-15 02:04:10,685	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000009)
2023-11-15 02:07:01,373	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000010)
2023-11-15 02:09:52,066	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000011)
2023-11-15 02:12:42,670	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000012)
2023-11-15 02:15:33,138	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000013)
2023-11-15 02:18:23,622	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000014)
2023-11-15 02:21:14,263	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000015)
2023-11-15 02:24:04,987	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000016)
2023-11-15 02:26:55,543	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000017)
2023-11-15 02:29:46,191	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000018)
[2m[36m(RayTrainWorker pid=3493842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       ptl/train_accuracy █▁▇▄▃▇█▆▁▇▃▂▇█▇█▇▇▆
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         ptl/val_accuracy ███▁▁▁▁██▁▁▁▁▁▁▁▁▁█▁
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         ptl/val_f1_score ▁▁▁████▁▁█████████▁█
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             ptl/val_loss ▅▁▁▃▇▄▄▂▁▅▅█▄▅▅▃▃▃▂█
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:              ptl/val_mcc ███▁▁▁▁██▁▁▁▁▁▁▁▁▁█▁
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:        ptl/val_precision ▁▁▁████▁▁█████████▁█
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:           ptl/val_recall ▁▁▁████▁▁█████████▁█
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       ptl/train_accuracy 0.49314
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:           ptl/train_loss 0.69424
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         ptl/val_accuracy 0.49265
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         ptl/val_f1_score 0.65672
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             ptl/val_loss 0.69402
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:              ptl/val_mcc -0.00383
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:        ptl/val_precision 0.48889
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                     step 1020
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       time_since_restore 3433.96393
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:         time_this_iter_s 170.37543
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:             time_total_s 3433.96393
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:                timestamp 1699975956
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_fc8ed87c_1_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0088_2023-11-15_01-34-56/wandb/offline-run-20231115_013521-fc8ed87c
[2m[36m(_WandbLoggingActor pid=3493837)[0m wandb: Find logs at: ./wandb/offline-run-20231115_013521-fc8ed87c/logs
[2m[36m(TorchTrainer pid=3507483)[0m Starting distributed worker processes: ['3507614 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3507614)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3507614)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3507614)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3507614)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3507614)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3507614)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3507614)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3507614)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3507614)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_66c47901_2_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0001_2023-11-15_01-35-12/lightning_logs
[2m[36m(RayTrainWorker pid=3507614)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3507614)[0m 
[2m[36m(RayTrainWorker pid=3507614)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3507614)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3507614)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3507614)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3507614)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3507614)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3507614)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3507614)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3507614)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3507614)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3507614)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3507614)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3507614)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3507614)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3507614)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3507614)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3507614)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3507614)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3507614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_66c47901_2_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0001_2023-11-15_01-35-12/checkpoint_000000)
2023-11-15 02:36:12,160	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.052 s, which may be a performance bottleneck.
2023-11-15 02:36:12,161	WARNING util.py:315 -- The `process_trial_result` operation took 3.067 s, which may be a performance bottleneck.
2023-11-15 02:36:12,162	WARNING util.py:315 -- Processing trial results took 3.067 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:36:12,162	WARNING util.py:315 -- The `process_trial_result` operation took 3.067 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         ptl/val_accuracy 0.50735
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             ptl/val_aupr 0.73425
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:            ptl/val_auroc 0.74001
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         ptl/val_f1_score 0.66332
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             ptl/val_loss 0.84479
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:              ptl/val_mcc 0.11993
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:        ptl/val_precision 0.49624
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                     step 51
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:       time_since_restore 195.42106
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:         time_this_iter_s 195.42106
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:             time_total_s 195.42106
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:                timestamp 1699976169
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_66c47901_2_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0001_2023-11-15_01-35-12/wandb/offline-run-20231115_023303-66c47901
[2m[36m(_WandbLoggingActor pid=3507611)[0m wandb: Find logs at: ./wandb/offline-run-20231115_023303-66c47901/logs
[2m[36m(TorchTrainer pid=3509095)[0m Starting distributed worker processes: ['3509225 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3509225)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3509225)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3509225)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3509225)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3509225)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3509225)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3509225)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3509225)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3509225)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_0e364bfd_3_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0009_2023-11-15_02-32-53/lightning_logs
[2m[36m(RayTrainWorker pid=3509225)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3509225)[0m 
[2m[36m(RayTrainWorker pid=3509225)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3509225)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3509225)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3509225)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3509225)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3509225)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3509225)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3509225)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3509225)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3509225)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3509225)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3509225)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3509225)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3509225)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3509225)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3509225)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3509225)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3509225)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:39:41,525	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3509225)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_0e364bfd_3_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0009_2023-11-15_02-32-53/checkpoint_000000)
2023-11-15 02:39:44,278	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.752 s, which may be a performance bottleneck.
2023-11-15 02:39:44,279	WARNING util.py:315 -- The `process_trial_result` operation took 2.757 s, which may be a performance bottleneck.
2023-11-15 02:39:44,279	WARNING util.py:315 -- Processing trial results took 2.757 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:39:44,280	WARNING util.py:315 -- The `process_trial_result` operation took 2.758 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=3509225)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_0e364bfd_3_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0009_2023-11-15_02-32-53/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       ptl/train_accuracy 0.5049
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:           ptl/train_loss 0.84347
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         ptl/val_accuracy 0.50735
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             ptl/val_loss 0.69518
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:              ptl/val_mcc 0.00383
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                     step 102
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       time_since_restore 360.31475
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:         time_this_iter_s 167.06761
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:             time_total_s 360.31475
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:                timestamp 1699976551
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_0e364bfd_3_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0009_2023-11-15_02-32-53/wandb/offline-run-20231115_023635-0e364bfd
[2m[36m(_WandbLoggingActor pid=3509222)[0m wandb: Find logs at: ./wandb/offline-run-20231115_023635-0e364bfd/logs
[2m[36m(TorchTrainer pid=3510985)[0m Starting distributed worker processes: ['3511118 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3511118)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3511118)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3511118)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3511118)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3511118)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3511118)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3511118)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3511118)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3511118)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_00eced56_4_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0004_2023-11-15_02-36-28/lightning_logs
[2m[36m(RayTrainWorker pid=3511118)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3511118)[0m 
[2m[36m(RayTrainWorker pid=3511118)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3511118)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3511118)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3511118)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3511118)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3511118)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3511118)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3511118)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3511118)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3511118)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3511118)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3511118)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3511118)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3511118)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3511118)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3511118)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3511118)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3511118)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:45:58,088	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3511118)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_00eced56_4_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0004_2023-11-15_02-36-28/checkpoint_000000)
2023-11-15 02:46:01,057	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.968 s, which may be a performance bottleneck.
2023-11-15 02:46:01,058	WARNING util.py:315 -- The `process_trial_result` operation took 2.972 s, which may be a performance bottleneck.
2023-11-15 02:46:01,058	WARNING util.py:315 -- Processing trial results took 2.972 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:46:01,059	WARNING util.py:315 -- The `process_trial_result` operation took 2.972 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=3511118)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_00eced56_4_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0004_2023-11-15_02-36-28/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       ptl/train_accuracy 0.49755
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:           ptl/train_loss 0.73887
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         ptl/val_accuracy 0.49265
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         ptl/val_f1_score 0.65672
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             ptl/val_loss 0.69502
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:              ptl/val_mcc -0.00383
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:        ptl/val_precision 0.48889
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                     step 102
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       time_since_restore 358.47816
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:         time_this_iter_s 166.92753
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:             time_total_s 358.47816
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:                timestamp 1699976928
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_00eced56_4_batch_size=8,cell_type=pharyngeal_meso,layer_size=8,lr=0.0004_2023-11-15_02-36-28/wandb/offline-run-20231115_024254-00eced56
[2m[36m(_WandbLoggingActor pid=3511115)[0m wandb: Find logs at: ./wandb/offline-run-20231115_024254-00eced56/logs
[2m[36m(TorchTrainer pid=3512885)[0m Starting distributed worker processes: ['3513016 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3513016)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3513016)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3513016)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3513016)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3513016)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3513016)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3513016)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3513016)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3513016)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/lightning_logs
[2m[36m(RayTrainWorker pid=3513016)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3513016)[0m 
[2m[36m(RayTrainWorker pid=3513016)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3513016)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3513016)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3513016)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3513016)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3513016)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3513016)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3513016)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3513016)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3513016)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3513016)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3513016)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3513016)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3513016)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3513016)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3513016)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3513016)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3513016)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:52:14,460	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3513016)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/checkpoint_000000)
2023-11-15 02:52:17,123	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.663 s, which may be a performance bottleneck.
2023-11-15 02:52:17,124	WARNING util.py:315 -- The `process_trial_result` operation took 2.666 s, which may be a performance bottleneck.
2023-11-15 02:52:17,125	WARNING util.py:315 -- Processing trial results took 2.667 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:52:17,125	WARNING util.py:315 -- The `process_trial_result` operation took 2.667 s, which may be a performance bottleneck.
2023-11-15 02:55:04,711	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3513016)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/checkpoint_000001)
2023-11-15 02:57:54,930	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3513016)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3513016)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       ptl/train_accuracy █▁▇
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         ptl/val_accuracy ██▁▁
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             ptl/val_aupr ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:            ptl/val_auroc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         ptl/val_f1_score ▁▁██
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             ptl/val_loss ▂▁▇█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:              ptl/val_mcc ██▁▁
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:        ptl/val_precision ▁▁██
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:           ptl/val_recall ▁▁██
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       ptl/train_accuracy 0.51176
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:           ptl/train_loss 0.69368
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         ptl/val_accuracy 0.49265
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         ptl/val_f1_score 0.65672
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             ptl/val_loss 0.69787
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:              ptl/val_mcc -0.00383
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:        ptl/val_precision 0.48889
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                     step 204
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       time_since_restore 698.8868
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:         time_this_iter_s 170.01963
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:             time_total_s 698.8868
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:                timestamp 1699977645
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_39d5fdec_5_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0291_2023-11-15_02-42-46/wandb/offline-run-20231115_024910-39d5fdec
[2m[36m(_WandbLoggingActor pid=3513013)[0m wandb: Find logs at: ./wandb/offline-run-20231115_024910-39d5fdec/logs
[2m[36m(TrainTrainable pid=3515862)[0m Trainable.setup took 26.784 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3515862)[0m Starting distributed worker processes: ['3515994 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3515994)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3515994)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3515994)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3515994)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3515994)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3515994)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3515994)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3515994)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3515994)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_bec7995a_6_batch_size=4,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_02-49-03/lightning_logs
[2m[36m(RayTrainWorker pid=3515994)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3515994)[0m 
[2m[36m(RayTrainWorker pid=3515994)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3515994)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3515994)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3515994)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3515994)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3515994)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3515994)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3515994)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3515994)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3515994)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3515994)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3515994)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3515994)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3515994)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3515994)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3515994)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3515994)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3515994)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 03:05:48,119	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3515994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_bec7995a_6_batch_size=4,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_02-49-03/checkpoint_000000)
2023-11-15 03:05:56,925	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 8.805 s, which may be a performance bottleneck.
2023-11-15 03:05:56,926	WARNING util.py:315 -- The `process_trial_result` operation took 8.809 s, which may be a performance bottleneck.
2023-11-15 03:05:56,926	WARNING util.py:315 -- Processing trial results took 8.809 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:05:56,926	WARNING util.py:315 -- The `process_trial_result` operation took 8.809 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=3515994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_bec7995a_6_batch_size=4,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_02-49-03/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             ptl/val_aupr █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:            ptl/val_auroc █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:        ptl/val_precision █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       ptl/train_accuracy 0.54412
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:           ptl/train_loss 0.69441
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         ptl/val_accuracy 0.49265
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             ptl/val_aupr 0.49254
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:            ptl/val_auroc 0.50725
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         ptl/val_f1_score 0.65672
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             ptl/val_loss 0.70727
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:              ptl/val_mcc -0.00383
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:        ptl/val_precision 0.48889
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                     step 204
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       time_since_restore 406.91787
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:         time_this_iter_s 164.55209
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:             time_total_s 406.91787
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:                timestamp 1699978121
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_bec7995a_6_batch_size=4,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_02-49-03/wandb/offline-run-20231115_030217-bec7995a
[2m[36m(_WandbLoggingActor pid=3515989)[0m wandb: Find logs at: ./wandb/offline-run-20231115_030217-bec7995a/logs
[2m[36m(TorchTrainer pid=3517760)[0m Starting distributed worker processes: ['3517894 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3517894)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3517894)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3517894)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3517894)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3517894)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3517894)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3517894)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3517894)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3517894)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/lightning_logs
[2m[36m(RayTrainWorker pid=3517894)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3517894)[0m 
[2m[36m(RayTrainWorker pid=3517894)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3517894)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3517894)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3517894)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3517894)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3517894)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3517894)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3517894)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3517894)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3517894)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3517894)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3517894)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3517894)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3517894)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3517894)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3517894)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3517894)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3517894)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 03:12:12,106	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000000)
2023-11-15 03:12:15,255	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.149 s, which may be a performance bottleneck.
2023-11-15 03:12:15,257	WARNING util.py:315 -- The `process_trial_result` operation took 3.153 s, which may be a performance bottleneck.
2023-11-15 03:12:15,257	WARNING util.py:315 -- Processing trial results took 3.153 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:12:15,257	WARNING util.py:315 -- The `process_trial_result` operation took 3.153 s, which may be a performance bottleneck.
2023-11-15 03:15:02,581	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000001)
2023-11-15 03:17:53,046	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000002)
2023-11-15 03:20:43,607	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000003)
2023-11-15 03:23:34,198	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000004)
2023-11-15 03:26:24,918	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000005)
2023-11-15 03:29:15,487	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000006)
[2m[36m(RayTrainWorker pid=3517894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       ptl/train_accuracy ▅▇▁██▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             ptl/val_loss ▂▄▁▄▆▅█▇
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       ptl/train_accuracy 0.50686
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:           ptl/train_loss 0.6933
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         ptl/val_accuracy 0.49265
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         ptl/val_f1_score 0.65672
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             ptl/val_loss 0.69346
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:              ptl/val_mcc -0.00383
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:        ptl/val_precision 0.48889
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                     step 408
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       time_since_restore 1382.44989
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:         time_this_iter_s 170.31371
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:             time_total_s 1382.44989
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:                timestamp 1699979526
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_a2bc4458_7_batch_size=8,cell_type=pharyngeal_meso,layer_size=32,lr=0.0028_2023-11-15_03-01-45/wandb/offline-run-20231115_030907-a2bc4458
[2m[36m(_WandbLoggingActor pid=3517891)[0m wandb: Find logs at: ./wandb/offline-run-20231115_030907-a2bc4458/logs
[2m[36m(TrainTrainable pid=3522915)[0m Trainable.setup took 13.016 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3522915)[0m Starting distributed worker processes: ['3523045 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3523045)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3523045)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3523045)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3523045)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3523045)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3523045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3523045)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3523045)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3523045)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/lightning_logs
[2m[36m(RayTrainWorker pid=3523045)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3523045)[0m 
[2m[36m(RayTrainWorker pid=3523045)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3523045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3523045)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3523045)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3523045)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3523045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3523045)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3523045)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3523045)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3523045)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3523045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3523045)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3523045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3523045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3523045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3523045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3523045)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3523045)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 03:36:00,488	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3523045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/checkpoint_000000)
2023-11-15 03:36:03,284	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.795 s, which may be a performance bottleneck.
2023-11-15 03:36:03,285	WARNING util.py:315 -- The `process_trial_result` operation took 2.799 s, which may be a performance bottleneck.
2023-11-15 03:36:03,285	WARNING util.py:315 -- Processing trial results took 2.799 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:36:03,285	WARNING util.py:315 -- The `process_trial_result` operation took 2.800 s, which may be a performance bottleneck.
2023-11-15 03:38:50,482	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3523045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/checkpoint_000001)
2023-11-15 03:41:40,585	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3523045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3523045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       ptl/train_accuracy ▅█▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:           ptl/train_loss ▁▃█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         ptl/val_accuracy ▂█▁▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             ptl/val_aupr █▇█▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:            ptl/val_auroc ▇██▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         ptl/val_f1_score █▁██
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             ptl/val_loss ▅▁▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:              ptl/val_mcc ▁█▁▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:        ptl/val_precision ▁█▁▁
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:           ptl/val_recall ▇▁██
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       ptl/train_accuracy 0.53431
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:           ptl/train_loss 0.7505
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         ptl/val_accuracy 0.50735
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             ptl/val_aupr 0.55963
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:            ptl/val_auroc 0.60935
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         ptl/val_f1_score 0.66332
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             ptl/val_loss 0.70392
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:              ptl/val_mcc 0.11993
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:        ptl/val_precision 0.49624
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                     step 204
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       time_since_restore 713.98938
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:         time_this_iter_s 170.10102
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:             time_total_s 713.98938
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:                timestamp 1699980270
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_e76982ef_8_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0000_2023-11-15_03-08-59/wandb/offline-run-20231115_033249-e76982ef
[2m[36m(_WandbLoggingActor pid=3523041)[0m wandb: Find logs at: ./wandb/offline-run-20231115_033249-e76982ef/logs
[2m[36m(TorchTrainer pid=3525926)[0m Starting distributed worker processes: ['3526056 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3526056)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3526056)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3526056)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3526056)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3526056)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3526056)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3526056)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3526056)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3526056)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_53b2e06d_9_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0968_2023-11-15_03-32-33/lightning_logs
[2m[36m(RayTrainWorker pid=3526056)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3526056)[0m 
[2m[36m(RayTrainWorker pid=3526056)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3526056)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3526056)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3526056)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3526056)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3526056)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3526056)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3526056)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3526056)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3526056)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3526056)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3526056)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3526056)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3526056)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3526056)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3526056)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3526056)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3526056)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3526056)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_53b2e06d_9_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0968_2023-11-15_03-32-33/checkpoint_000000)
2023-11-15 03:48:04,992	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 7.546 s, which may be a performance bottleneck.
2023-11-15 03:48:04,994	WARNING util.py:315 -- The `process_trial_result` operation took 7.550 s, which may be a performance bottleneck.
2023-11-15 03:48:04,994	WARNING util.py:315 -- Processing trial results took 7.550 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:48:04,994	WARNING util.py:315 -- The `process_trial_result` operation took 7.551 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         ptl/val_accuracy 0.50735
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             ptl/val_loss 0.69751
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:              ptl/val_mcc 0.00383
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                     step 51
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:       time_since_restore 190.70373
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:         time_this_iter_s 190.70373
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:             time_total_s 190.70373
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:                timestamp 1699980477
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_53b2e06d_9_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0968_2023-11-15_03-32-33/wandb/offline-run-20231115_034453-53b2e06d
[2m[36m(_WandbLoggingActor pid=3526053)[0m wandb: Find logs at: ./wandb/offline-run-20231115_034453-53b2e06d/logs
[2m[36m(TorchTrainer pid=3527538)[0m Starting distributed worker processes: ['3527669 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=3527669)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3527669)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3527669)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3527669)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3527669)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3527669)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3527669)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3527669)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3527669)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_320dc96d_10_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0772_2023-11-15_03-44-46/lightning_logs
[2m[36m(RayTrainWorker pid=3527669)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3527669)[0m 
[2m[36m(RayTrainWorker pid=3527669)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3527669)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3527669)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3527669)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3527669)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3527669)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3527669)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3527669)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3527669)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3527669)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3527669)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3527669)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3527669)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3527669)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3527669)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3527669)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3527669)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3527669)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3527669)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_320dc96d_10_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0772_2023-11-15_03-44-46/checkpoint_000000)
2023-11-15 03:51:54,874	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.235 s, which may be a performance bottleneck.
2023-11-15 03:51:54,875	WARNING util.py:315 -- The `process_trial_result` operation took 5.238 s, which may be a performance bottleneck.
2023-11-15 03:51:54,876	WARNING util.py:315 -- Processing trial results took 5.238 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:51:54,876	WARNING util.py:315 -- The `process_trial_result` operation took 5.239 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         ptl/val_accuracy 0.50735
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             ptl/val_aupr 0.48889
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             ptl/val_loss 0.69537
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:              ptl/val_mcc 0.00383
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                     step 51
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:       time_since_restore 195.26159
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:         time_this_iter_s 195.26159
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:             time_total_s 195.26159
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:                timestamp 1699980709
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-34-42/TorchTrainer_320dc96d_10_batch_size=8,cell_type=pharyngeal_meso,layer_size=16,lr=0.0772_2023-11-15_03-44-46/wandb/offline-run-20231115_034843-320dc96d
[2m[36m(_WandbLoggingActor pid=3527666)[0m wandb: Find logs at: ./wandb/offline-run-20231115_034843-320dc96d/logs
