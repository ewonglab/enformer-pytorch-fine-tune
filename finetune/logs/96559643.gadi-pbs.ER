Global seed set to 42
2023-09-30 14:34:20,941	INFO worker.py:1642 -- Started a local Ray instance.
2023-09-30 14:34:56,758	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-09-30 14:34:56,815	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1797768)[0m Starting distributed worker processes: ['1797918 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1797918)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1797918)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1797918)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1797918)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1797918)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1797918)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1797918)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1797918)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1797918)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/lightning_logs
[2m[36m(RayTrainWorker pid=1797918)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1797918)[0m 
[2m[36m(RayTrainWorker pid=1797918)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1797918)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1797918)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1797918)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1797918)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1797918)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1797918)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1797918)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1797918)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1797918)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1797918)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1797918)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1797918)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1797918)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1797918)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1797918)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1797918)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1797918)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1797918)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1797918)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1797918)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1797918)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1797918)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1797918)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1797918)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1797918)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1797918)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1797918)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 14:36:57,946	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000000)
2023-09-30 14:37:01,149	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.203 s, which may be a performance bottleneck.
2023-09-30 14:37:01,150	WARNING util.py:315 -- The `process_trial_result` operation took 3.204 s, which may be a performance bottleneck.
2023-09-30 14:37:01,150	WARNING util.py:315 -- Processing trial results took 3.204 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:37:01,150	WARNING util.py:315 -- The `process_trial_result` operation took 3.205 s, which may be a performance bottleneck.
2023-09-30 14:38:15,886	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000001)
2023-09-30 14:39:33,003	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000002)
2023-09-30 14:40:50,448	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000003)
2023-09-30 14:42:07,626	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000004)
2023-09-30 14:43:25,068	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000005)
2023-09-30 14:44:42,159	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000006)
2023-09-30 14:45:58,778	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000007)
2023-09-30 14:47:15,637	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000008)
[2m[36m(RayTrainWorker pid=1797918)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/checkpoint_000009)
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                    epoch ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: iterations_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       ptl/train_accuracy █▅▃▂▃▁▂▁▂
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           ptl/train_loss █▅▃▂▃▁▂▁▂
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         ptl/val_accuracy ▁▆▄▁█▄▇█▇▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             ptl/val_aupr ▁▄▅▇▇▇▇███
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:            ptl/val_auroc ▁▄▅▆▇▇▇▇█▇
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         ptl/val_f1_score ▁█▅▁█▅██▇▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             ptl/val_loss █▁▂▃▁▂▁▁▁▅
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:              ptl/val_mcc ▁▅▅▃█▆█▇▇▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:        ptl/val_precision ▁▆▇█▇▇▇▆▇▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           ptl/val_recall ▁█▃▁▆▄▆▇▅▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                     step ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       time_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             time_total_s ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                timestamp ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           train_accuracy ▁█▁▁███▁█▁
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:               train_loss █▁▂▂▁▁▁▄▁▄
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       training_iteration ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                    epoch 9
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: iterations_since_restore 10
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       ptl/train_accuracy 0.74149
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           ptl/train_loss 0.74149
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             ptl/val_aupr 0.91798
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:            ptl/val_auroc 0.87255
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             ptl/val_loss 2.84853
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       time_since_restore 794.16092
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:         time_this_iter_s 76.4068
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:             time_total_s 794.16092
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:                timestamp 1696049312
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:               train_loss 3.02258
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb:       training_iteration 10
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_0cdf7a07_1_batch_size=4,layer_size=8,lr=0.0001_2023-09-30_14-34-56/wandb/offline-run-20230930_143521-0cdf7a07
[2m[36m(_WandbLoggingActor pid=1797913)[0m wandb: Find logs at: ./wandb/offline-run-20230930_143521-0cdf7a07/logs
[2m[36m(TorchTrainer pid=1814233)[0m Starting distributed worker processes: ['1814364 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1814364)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1814364)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1814364)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1814364)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1814364)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1814364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1814364)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1814364)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1814364)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_c7a807e1_2_batch_size=8,layer_size=16,lr=0.0063_2023-09-30_14-35-13/lightning_logs
[2m[36m(RayTrainWorker pid=1814364)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1814364)[0m 
[2m[36m(RayTrainWorker pid=1814364)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1814364)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1814364)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1814364)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1814364)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1814364)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1814364)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1814364)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1814364)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1814364)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1814364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1814364)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1814364)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1814364)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1814364)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1814364)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1814364)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1814364)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1814364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1814364)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1814364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1814364)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1814364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1814364)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1814364)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1814364)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1814364)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1814364)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1814364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_c7a807e1_2_batch_size=8,layer_size=16,lr=0.0063_2023-09-30_14-35-13/checkpoint_000000)
2023-09-30 14:50:27,250	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.657 s, which may be a performance bottleneck.
2023-09-30 14:50:27,252	WARNING util.py:315 -- The `process_trial_result` operation took 3.662 s, which may be a performance bottleneck.
2023-09-30 14:50:27,252	WARNING util.py:315 -- Processing trial results took 3.663 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:50:27,252	WARNING util.py:315 -- The `process_trial_result` operation took 3.663 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             ptl/val_aupr 0.69172
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:            ptl/val_auroc 0.64216
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             ptl/val_loss 19.85798
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                     step 22
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:       time_since_restore 95.92677
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:         time_this_iter_s 95.92677
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:             time_total_s 95.92677
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:                timestamp 1696049423
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:           train_accuracy 0.8
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:               train_loss 13.54053
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_c7a807e1_2_batch_size=8,layer_size=16,lr=0.0063_2023-09-30_14-35-13/wandb/offline-run-20230930_144854-c7a807e1
[2m[36m(_WandbLoggingActor pid=1814361)[0m wandb: Find logs at: ./wandb/offline-run-20230930_144854-c7a807e1/logs
[2m[36m(TorchTrainer pid=1815832)[0m Starting distributed worker processes: ['1815963 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1815963)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1815963)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1815963)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1815963)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1815963)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1815963)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1815963)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1815963)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1815963)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_e15844a7_3_batch_size=8,layer_size=8,lr=0.0060_2023-09-30_14-48-47/lightning_logs
[2m[36m(RayTrainWorker pid=1815963)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1815963)[0m 
[2m[36m(RayTrainWorker pid=1815963)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1815963)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1815963)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1815963)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1815963)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1815963)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1815963)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1815963)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1815963)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1815963)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1815963)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1815963)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1815963)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1815963)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1815963)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1815963)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1815963)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1815963)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1815963)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1815963)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1815963)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1815963)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1815963)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1815963)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1815963)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1815963)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1815963)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1815963)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 14:52:17,944	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1815963)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_e15844a7_3_batch_size=8,layer_size=8,lr=0.0060_2023-09-30_14-48-47/checkpoint_000000)
2023-09-30 14:52:20,502	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.557 s, which may be a performance bottleneck.
2023-09-30 14:52:20,503	WARNING util.py:315 -- The `process_trial_result` operation took 2.562 s, which may be a performance bottleneck.
2023-09-30 14:52:20,503	WARNING util.py:315 -- Processing trial results took 2.562 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:52:20,503	WARNING util.py:315 -- The `process_trial_result` operation took 2.562 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1815963)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_e15844a7_3_batch_size=8,layer_size=8,lr=0.0060_2023-09-30_14-48-47/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         ptl/val_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             ptl/val_aupr █▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         ptl/val_f1_score ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:              ptl/val_mcc ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           ptl/val_recall ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           train_accuracy █▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       ptl/train_accuracy 25.67438
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           ptl/train_loss 25.67438
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         ptl/val_accuracy 0.51562
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             ptl/val_aupr 0.71147
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:            ptl/val_auroc 0.66299
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         ptl/val_f1_score 0.4898
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             ptl/val_loss 1.96682
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:              ptl/val_mcc 0.25638
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:        ptl/val_precision 0.8
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           ptl/val_recall 0.35294
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       time_since_restore 168.11948
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:         time_this_iter_s 72.3919
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:             time_total_s 168.11948
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:                timestamp 1696049612
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:           train_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:               train_loss 0.41831
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_e15844a7_3_batch_size=8,layer_size=8,lr=0.0060_2023-09-30_14-48-47/wandb/offline-run-20230930_145049-e15844a7
[2m[36m(_WandbLoggingActor pid=1815960)[0m wandb: Find logs at: ./wandb/offline-run-20230930_145049-e15844a7/logs
[2m[36m(TorchTrainer pid=1817708)[0m Starting distributed worker processes: ['1817838 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1817838)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1817838)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1817838)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1817838)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1817838)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1817838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1817838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1817838)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1817838)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_780a196d_4_batch_size=8,layer_size=32,lr=0.0098_2023-09-30_14-50-42/lightning_logs
[2m[36m(RayTrainWorker pid=1817838)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1817838)[0m 
[2m[36m(RayTrainWorker pid=1817838)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1817838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1817838)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1817838)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1817838)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1817838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1817838)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1817838)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1817838)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1817838)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1817838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1817838)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1817838)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1817838)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1817838)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1817838)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1817838)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1817838)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1817838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1817838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1817838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1817838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1817838)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1817838)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1817838)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1817838)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 14:55:23,814	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1817838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_780a196d_4_batch_size=8,layer_size=32,lr=0.0098_2023-09-30_14-50-42/checkpoint_000000)
2023-09-30 14:55:26,401	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.586 s, which may be a performance bottleneck.
2023-09-30 14:55:26,402	WARNING util.py:315 -- The `process_trial_result` operation took 2.590 s, which may be a performance bottleneck.
2023-09-30 14:55:26,402	WARNING util.py:315 -- Processing trial results took 2.590 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:55:26,402	WARNING util.py:315 -- The `process_trial_result` operation took 2.590 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=1817838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_780a196d_4_batch_size=8,layer_size=32,lr=0.0098_2023-09-30_14-50-42/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             ptl/val_aupr █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:            ptl/val_auroc █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:        ptl/val_precision █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           train_accuracy █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       ptl/train_accuracy 308.72964
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           ptl/train_loss 308.72964
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             ptl/val_aupr 0.58917
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:            ptl/val_auroc 0.54412
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             ptl/val_loss 28.07079
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       time_since_restore 169.28071
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:         time_this_iter_s 73.18117
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:             time_total_s 169.28071
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:                timestamp 1696049799
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:           train_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:               train_loss 13.71873
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_780a196d_4_batch_size=8,layer_size=32,lr=0.0098_2023-09-30_14-50-42/wandb/offline-run-20230930_145354-780a196d
[2m[36m(_WandbLoggingActor pid=1817835)[0m wandb: Find logs at: ./wandb/offline-run-20230930_145354-780a196d/logs
[2m[36m(TorchTrainer pid=1820102)[0m Starting distributed worker processes: ['1820235 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1820235)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1820235)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1820235)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1820235)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1820235)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1820235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1820235)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1820235)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1820235)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/lightning_logs
[2m[36m(RayTrainWorker pid=1820235)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1820235)[0m 
[2m[36m(RayTrainWorker pid=1820235)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1820235)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1820235)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1820235)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1820235)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1820235)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1820235)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1820235)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1820235)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1820235)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1820235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1820235)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1820235)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1820235)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1820235)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1820235)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1820235)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1820235)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1820235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1820235)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1820235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1820235)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1820235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1820235)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1820235)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1820235)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1820235)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1820235)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 14:58:29,043	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000000)
2023-09-30 14:58:31,654	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.610 s, which may be a performance bottleneck.
2023-09-30 14:58:31,655	WARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.
2023-09-30 14:58:31,655	WARNING util.py:315 -- Processing trial results took 2.614 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:58:31,655	WARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.
2023-09-30 14:59:46,470	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000001)
2023-09-30 15:01:04,068	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000002)
2023-09-30 15:02:21,726	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000003)
2023-09-30 15:03:39,368	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000004)
2023-09-30 15:04:56,971	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000005)
2023-09-30 15:06:14,584	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000006)
[2m[36m(RayTrainWorker pid=1820235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       ptl/train_accuracy █▅▂▃▃▁▂
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           ptl/train_loss █▅▂▃▃▁▂
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         ptl/val_accuracy ▁▆▅▃▆█▆▆
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             ptl/val_aupr ▁▅▆▇▇███
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:            ptl/val_auroc ▁▅▆▇████
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         ptl/val_f1_score ▁█▆▄▇█▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             ptl/val_loss █▁▂▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:              ptl/val_mcc ▁▅▆▅▆█▆▅
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:        ptl/val_precision ▁▆█████▆
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           ptl/val_recall ▁█▄▃▅▇▅█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           train_accuracy ▁██▁███▁
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:               train_loss █▂▂▃▁▁▂▅
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       ptl/train_accuracy 0.59995
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           ptl/train_loss 0.59995
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         ptl/val_accuracy 0.7
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             ptl/val_aupr 0.90493
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:            ptl/val_auroc 0.85907
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         ptl/val_f1_score 0.78571
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             ptl/val_loss 0.54562
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:              ptl/val_mcc 0.37457
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:        ptl/val_precision 0.66
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           ptl/val_recall 0.97059
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                     step 352
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       time_since_restore 634.3439
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:         time_this_iter_s 77.46609
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:             time_total_s 634.3439
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:                timestamp 1696050452
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:               train_loss 2.28283
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a48d59dc_5_batch_size=4,layer_size=32,lr=0.0000_2023-09-30_14-53-47/wandb/offline-run-20230930_145700-a48d59dc
[2m[36m(_WandbLoggingActor pid=1820232)[0m wandb: Find logs at: ./wandb/offline-run-20230930_145700-a48d59dc/logs
[2m[36m(TorchTrainer pid=1825765)[0m Starting distributed worker processes: ['1825895 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1825895)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1825895)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1825895)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1825895)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1825895)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1825895)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1825895)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1825895)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1825895)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/lightning_logs
[2m[36m(RayTrainWorker pid=1825895)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1825895)[0m 
[2m[36m(RayTrainWorker pid=1825895)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1825895)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1825895)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1825895)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1825895)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1825895)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1825895)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1825895)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1825895)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1825895)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1825895)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1825895)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1825895)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1825895)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1825895)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1825895)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1825895)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1825895)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1825895)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1825895)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1825895)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1825895)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1825895)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1825895)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1825895)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1825895)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1825895)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1825895)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:09:22,117	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1825895)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/checkpoint_000000)
2023-09-30 15:09:24,649	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.531 s, which may be a performance bottleneck.
2023-09-30 15:09:24,650	WARNING util.py:315 -- The `process_trial_result` operation took 2.535 s, which may be a performance bottleneck.
2023-09-30 15:09:24,650	WARNING util.py:315 -- Processing trial results took 2.535 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:09:24,651	WARNING util.py:315 -- The `process_trial_result` operation took 2.535 s, which may be a performance bottleneck.
2023-09-30 15:10:38,558	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1825895)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/checkpoint_000001)
2023-09-30 15:11:55,217	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1825895)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/checkpoint_000002)
[2m[36m(RayTrainWorker pid=1825895)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       ptl/train_accuracy █▂▁
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           ptl/train_loss █▂▁
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         ptl/val_accuracy ▁█▇▃
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             ptl/val_aupr ▁▇▇█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:            ptl/val_auroc ▁▆▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         ptl/val_f1_score ▁██▄
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             ptl/val_loss █▁▁▂
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:              ptl/val_mcc ▁█▇▆
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:        ptl/val_precision ▁▆▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           ptl/val_recall ▁██▃
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           train_accuracy ▁██▁
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:               train_loss █▁▁▂
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       ptl/train_accuracy 1.07551
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           ptl/train_loss 1.07551
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         ptl/val_accuracy 0.48333
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             ptl/val_aupr 0.87041
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:            ptl/val_auroc 0.81127
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         ptl/val_f1_score 0.25641
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             ptl/val_loss 1.23991
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:              ptl/val_mcc 0.25806
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:        ptl/val_precision 1.0
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           ptl/val_recall 0.14706
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                     step 176
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       time_since_restore 321.76674
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:         time_this_iter_s 76.50473
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:             time_total_s 321.76674
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:                timestamp 1696050791
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:               train_loss 1.03609
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_2f57badb_6_batch_size=4,layer_size=8,lr=0.0003_2023-09-30_14-56-54/wandb/offline-run-20230930_150754-2f57badb
[2m[36m(_WandbLoggingActor pid=1825892)[0m wandb: Find logs at: ./wandb/offline-run-20230930_150754-2f57badb/logs
[2m[36m(TorchTrainer pid=1828979)[0m Starting distributed worker processes: ['1829115 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1829115)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1829115)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1829115)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1829115)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1829115)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1829115)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1829115)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1829115)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1829115)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_58a83762_7_batch_size=8,layer_size=32,lr=0.0003_2023-09-30_15-07-47/lightning_logs
[2m[36m(RayTrainWorker pid=1829115)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1829115)[0m 
[2m[36m(RayTrainWorker pid=1829115)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1829115)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1829115)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1829115)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1829115)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1829115)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1829115)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1829115)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1829115)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1829115)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1829115)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1829115)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1829115)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1829115)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1829115)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1829115)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1829115)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1829115)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1829115)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1829115)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1829115)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1829115)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1829115)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1829115)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1829115)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1829115)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1829115)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1829115)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:15:02,075	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1829115)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_58a83762_7_batch_size=8,layer_size=32,lr=0.0003_2023-09-30_15-07-47/checkpoint_000000)
2023-09-30 15:15:04,691	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.615 s, which may be a performance bottleneck.
2023-09-30 15:15:04,692	WARNING util.py:315 -- The `process_trial_result` operation took 2.618 s, which may be a performance bottleneck.
2023-09-30 15:15:04,692	WARNING util.py:315 -- Processing trial results took 2.618 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:15:04,692	WARNING util.py:315 -- The `process_trial_result` operation took 2.619 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1829115)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_58a83762_7_batch_size=8,layer_size=32,lr=0.0003_2023-09-30_15-07-47/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             ptl/val_aupr ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:              ptl/val_mcc ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           train_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       ptl/train_accuracy 10.07116
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           ptl/train_loss 10.07116
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         ptl/val_accuracy 0.45312
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             ptl/val_aupr 0.79199
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:            ptl/val_auroc 0.71324
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         ptl/val_f1_score 0.25641
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             ptl/val_loss 2.03187
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:              ptl/val_mcc 0.25806
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:        ptl/val_precision 1.0
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           ptl/val_recall 0.14706
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       time_since_restore 169.05532
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:         time_this_iter_s 73.19972
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:             time_total_s 169.05532
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:                timestamp 1696050977
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:               train_loss 0.20326
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_58a83762_7_batch_size=8,layer_size=32,lr=0.0003_2023-09-30_15-07-47/wandb/offline-run-20230930_151332-58a83762
[2m[36m(_WandbLoggingActor pid=1829112)[0m wandb: Find logs at: ./wandb/offline-run-20230930_151332-58a83762/logs
[2m[36m(TorchTrainer pid=1831124)[0m Starting distributed worker processes: ['1831253 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1831253)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1831253)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1831253)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1831253)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1831253)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1831253)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1831253)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1831253)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1831253)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/lightning_logs
[2m[36m(RayTrainWorker pid=1831253)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1831253)[0m 
[2m[36m(RayTrainWorker pid=1831253)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1831253)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1831253)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1831253)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1831253)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1831253)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1831253)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1831253)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1831253)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1831253)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1831253)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1831253)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1831253)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1831253)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1831253)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1831253)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1831253)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1831253)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1831253)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1831253)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1831253)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1831253)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1831253)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1831253)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1831253)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1831253)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:18:07,473	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000000)
2023-09-30 15:18:10,056	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.583 s, which may be a performance bottleneck.
2023-09-30 15:18:10,057	WARNING util.py:315 -- The `process_trial_result` operation took 2.586 s, which may be a performance bottleneck.
2023-09-30 15:18:10,058	WARNING util.py:315 -- Processing trial results took 2.587 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:18:10,058	WARNING util.py:315 -- The `process_trial_result` operation took 2.587 s, which may be a performance bottleneck.
2023-09-30 15:19:24,215	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000001)
2023-09-30 15:20:41,133	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000002)
2023-09-30 15:21:58,017	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000003)
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000004)
2023-09-30 15:23:15,034	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-09-30 15:24:32,328	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000005)
2023-09-30 15:25:49,376	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000006)
[2m[36m(RayTrainWorker pid=1831253)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       ptl/train_accuracy █▄▂▅▁▁▃
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           ptl/train_loss █▄▂▅▁▁▃
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         ptl/val_accuracy ▆▆▆▆▁█▄▆
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             ptl/val_aupr ▁▆▆▆▇█▇▇
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:            ptl/val_auroc ▁▅▆▇▇█▇▇
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         ptl/val_f1_score ▆███▁█▄█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             ptl/val_loss ▄▃▄▃█▁▆▇
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:              ptl/val_mcc ▆▁▁▁▃█▇▁
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:        ptl/val_precision ▄▁▁▁█▂▆▁
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           ptl/val_recall ▄███▁█▃█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           train_accuracy ███▁██▁▁
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:               train_loss ▂▂▁▆▁▂▅█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       ptl/train_accuracy 0.68781
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           ptl/train_loss 0.68781
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         ptl/val_accuracy 0.61667
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             ptl/val_aupr 0.87022
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:            ptl/val_auroc 0.8125
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         ptl/val_f1_score 0.74725
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             ptl/val_loss 0.80771
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:              ptl/val_mcc 0.15765
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:        ptl/val_precision 0.59649
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                     step 352
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       time_since_restore 629.77452
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:         time_this_iter_s 76.73438
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:             time_total_s 629.77452
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:                timestamp 1696051626
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:               train_loss 2.18977
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_a0e3cc3c_8_batch_size=4,layer_size=16,lr=0.0000_2023-09-30_15-13-26/wandb/offline-run-20230930_151639-a0e3cc3c
[2m[36m(_WandbLoggingActor pid=1831250)[0m wandb: Find logs at: ./wandb/offline-run-20230930_151639-a0e3cc3c/logs
[2m[36m(TorchTrainer pid=1836731)[0m Starting distributed worker processes: ['1836861 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1836861)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1836861)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1836861)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1836861)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1836861)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1836861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1836861)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1836861)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1836861)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_f0a66156_9_batch_size=4,layer_size=16,lr=0.0004_2023-09-30_15-16-32/lightning_logs
[2m[36m(RayTrainWorker pid=1836861)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1836861)[0m 
[2m[36m(RayTrainWorker pid=1836861)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1836861)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1836861)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1836861)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1836861)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1836861)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1836861)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1836861)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1836861)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1836861)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1836861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1836861)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1836861)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1836861)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1836861)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1836861)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1836861)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1836861)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1836861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1836861)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1836861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1836861)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1836861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1836861)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1836861)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1836861)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1836861)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1836861)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1836861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_f0a66156_9_batch_size=4,layer_size=16,lr=0.0004_2023-09-30_15-16-32/checkpoint_000000)
2023-09-30 15:29:17,189	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.913 s, which may be a performance bottleneck.
2023-09-30 15:29:17,191	WARNING util.py:315 -- The `process_trial_result` operation took 3.917 s, which may be a performance bottleneck.
2023-09-30 15:29:17,191	WARNING util.py:315 -- Processing trial results took 3.917 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:29:17,191	WARNING util.py:315 -- The `process_trial_result` operation took 3.917 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             ptl/val_aupr 0.62312
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:            ptl/val_auroc 0.56863
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             ptl/val_loss 13.3278
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:       time_since_restore 103.77412
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:         time_this_iter_s 103.77412
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:             time_total_s 103.77412
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:                timestamp 1696051753
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:               train_loss 9.86956
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_f0a66156_9_batch_size=4,layer_size=16,lr=0.0004_2023-09-30_15-16-32/wandb/offline-run-20230930_152740-f0a66156
[2m[36m(_WandbLoggingActor pid=1836856)[0m wandb: Find logs at: ./wandb/offline-run-20230930_152740-f0a66156/logs
[2m[36m(TorchTrainer pid=1838333)[0m Starting distributed worker processes: ['1838470 (10.6.9.10)']
[2m[36m(RayTrainWorker pid=1838470)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1838470)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1838470)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1838470)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1838470)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1838470)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1838470)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1838470)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1838470)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_658acfe6_10_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_15-27-29/lightning_logs
[2m[36m(RayTrainWorker pid=1838470)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1838470)[0m 
[2m[36m(RayTrainWorker pid=1838470)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1838470)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1838470)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1838470)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1838470)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1838470)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1838470)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1838470)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1838470)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1838470)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1838470)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1838470)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1838470)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1838470)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1838470)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1838470)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1838470)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1838470)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1838470)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1838470)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1838470)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1838470)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1838470)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1838470)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1838470)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1838470)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1838470)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1838470)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:31:21,818	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1838470)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_658acfe6_10_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_15-27-29/checkpoint_000000)
2023-09-30 15:31:26,192	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.373 s, which may be a performance bottleneck.
2023-09-30 15:31:26,193	WARNING util.py:315 -- The `process_trial_result` operation took 4.377 s, which may be a performance bottleneck.
2023-09-30 15:31:26,193	WARNING util.py:315 -- Processing trial results took 4.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:31:26,193	WARNING util.py:315 -- The `process_trial_result` operation took 4.377 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=1838470)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_658acfe6_10_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_15-27-29/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         ptl/val_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             ptl/val_aupr ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         ptl/val_f1_score ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:              ptl/val_mcc ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           ptl/val_recall ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           train_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       ptl/train_accuracy 2.11043
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           ptl/train_loss 2.11043
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         ptl/val_accuracy 0.63333
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             ptl/val_aupr 0.81884
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:            ptl/val_auroc 0.77328
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         ptl/val_f1_score 0.75
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             ptl/val_loss 0.84855
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:              ptl/val_mcc 0.18579
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:        ptl/val_precision 0.61111
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           ptl/val_recall 0.97059
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                     step 88
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       time_since_restore 175.75485
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:         time_this_iter_s 72.46085
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:             time_total_s 175.75485
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:                timestamp 1696051958
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:               train_loss 0.05957
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_14-34-56/TorchTrainer_658acfe6_10_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_15-27-29/wandb/offline-run-20230930_152949-658acfe6
[2m[36m(_WandbLoggingActor pid=1838465)[0m wandb: Find logs at: ./wandb/offline-run-20230930_152949-658acfe6/logs
