Global seed set to 42
2023-11-18 00:44:46,565	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-18 00:45:11,183	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-18 00:45:11,187	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-18 00:45:11,683	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=3312725)[0m Starting distributed worker processes: ['3313227 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3313227)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3313227)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3313227)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3313227)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3313227)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:46:12,525	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_29baf119
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3312725, ip=10.6.10.5, actor_id=328521cde40c973e633db77901000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3313227, ip=10.6.10.5, actor_id=7e2bd3a2872802d41837ec1b01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14596d358220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_29baf119_1_batch_size=8,cell_type=gut,layer_size=8,lr=0.0000_2023-11-18_00-45-11/wandb/offline-run-20231118_004550-29baf119
[2m[36m(_WandbLoggingActor pid=3313222)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004550-29baf119/logs
[2m[36m(TorchTrainer pid=3313618)[0m Starting distributed worker processes: ['3313748 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3313748)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3313748)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3313748)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3313748)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3313748)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:46:47,041	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_975e214f
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3313618, ip=10.6.10.5, actor_id=db305631e40bbb4c65774c8701000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3313748, ip=10.6.10.5, actor_id=5a066b6914f9c84820982fc501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14e099b9c130>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_975e214f_2_batch_size=8,cell_type=gut,layer_size=32,lr=0.0192_2023-11-18_00-45-42/wandb/offline-run-20231118_004637-975e214f
[2m[36m(_WandbLoggingActor pid=3313744)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004637-975e214f/logs
[2m[36m(TorchTrainer pid=3314140)[0m Starting distributed worker processes: ['3314271 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3314271)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3314271)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3314271)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3314271)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3314271)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:24,218	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_f1647328
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3314140, ip=10.6.10.5, actor_id=fec9db62a572e1ee2e73382a01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3314271, ip=10.6.10.5, actor_id=1bb1e90897b0b382404650cc01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14b6b408e220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_f1647328_3_batch_size=8,cell_type=gut,layer_size=32,lr=0.0431_2023-11-18_00-46-30/wandb/offline-run-20231118_004713-f1647328
[2m[36m(_WandbLoggingActor pid=3314266)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004713-f1647328/logs
[2m[36m(TorchTrainer pid=3314669)[0m Starting distributed worker processes: ['3314798 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3314798)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3314798)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3314798)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3314798)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3314798)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:58,575	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_b1ef09bc
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3314669, ip=10.6.10.5, actor_id=7e32116ca98db43954b14b1201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3314798, ip=10.6.10.5, actor_id=0dbfa17452fbad31671412c801000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x15323dedd1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_b1ef09bc_4_batch_size=8,cell_type=gut,layer_size=32,lr=0.0002_2023-11-18_00-47-05/wandb/offline-run-20231118_004749-b1ef09bc
[2m[36m(_WandbLoggingActor pid=3314794)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004749-b1ef09bc/logs
[2m[36m(TorchTrainer pid=3315190)[0m Starting distributed worker processes: ['3315320 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3315320)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3315320)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3315320)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3315320)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3315320)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:48:33,473	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_0cb9842b
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3315190, ip=10.6.10.5, actor_id=d09e344aaa03580100300bde01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3315320, ip=10.6.10.5, actor_id=6fb7476ca10762ff84878edb01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x151c4ec64220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_0cb9842b_5_batch_size=4,cell_type=gut,layer_size=32,lr=0.0001_2023-11-18_00-47-41/wandb/offline-run-20231118_004823-0cb9842b
[2m[36m(_WandbLoggingActor pid=3315316)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004823-0cb9842b/logs
[2m[36m(TorchTrainer pid=3315709)[0m Starting distributed worker processes: ['3315839 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3315839)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3315839)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3315839)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3315839)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3315839)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:49:11,410	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_fc2f2d57
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3315709, ip=10.6.10.5, actor_id=541794786496fee16c60266401000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3315839, ip=10.6.10.5, actor_id=01276522c9d4e2331448c34901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146bbe778d30>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_fc2f2d57_6_batch_size=8,cell_type=gut,layer_size=8,lr=0.0220_2023-11-18_00-48-15/wandb/offline-run-20231118_004900-fc2f2d57
[2m[36m(_WandbLoggingActor pid=3315835)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004900-fc2f2d57/logs
[2m[36m(TorchTrainer pid=3316237)[0m Starting distributed worker processes: ['3316366 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3316366)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3316366)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3316366)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3316366)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3316366)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:49:45,061	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_424fded2
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3316237, ip=10.6.10.5, actor_id=70364052df3e04b1c298440201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3316366, ip=10.6.10.5, actor_id=7962bcf2075a3fadbeb9988701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14aab57da1c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_424fded2_7_batch_size=4,cell_type=gut,layer_size=16,lr=0.0005_2023-11-18_00-48-52/wandb/offline-run-20231118_004935-424fded2
[2m[36m(_WandbLoggingActor pid=3316361)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004935-424fded2/logs
[2m[36m(TorchTrainer pid=3316757)[0m Starting distributed worker processes: ['3316888 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3316888)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3316888)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3316888)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3316888)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3316888)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:50:20,915	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_f0ce5f02
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3316757, ip=10.6.10.5, actor_id=883095173e1eb88cdd46b73501000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3316888, ip=10.6.10.5, actor_id=b43bc05f8cad15d18b7b5a5901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x149834f16160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_f0ce5f02_8_batch_size=8,cell_type=gut,layer_size=16,lr=0.0001_2023-11-18_00-49-28/wandb/offline-run-20231118_005010-f0ce5f02
[2m[36m(_WandbLoggingActor pid=3316884)[0m wandb: Find logs at: ./wandb/offline-run-20231118_005010-f0ce5f02/logs
[2m[36m(TorchTrainer pid=3317277)[0m Starting distributed worker processes: ['3317407 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3317407)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3317407)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3317407)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3317407)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3317407)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:50:56,740	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_fba8961a
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3317277, ip=10.6.10.5, actor_id=437577e4843bed4009c9f72b01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3317407, ip=10.6.10.5, actor_id=6d2e10ab3a9393b8c4fc6a2201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x151f16921250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_fba8961a_9_batch_size=8,cell_type=gut,layer_size=32,lr=0.0009_2023-11-18_00-50-02/wandb/offline-run-20231118_005045-fba8961a
[2m[36m(_WandbLoggingActor pid=3317401)[0m wandb: Find logs at: ./wandb/offline-run-20231118_005045-fba8961a/logs
[2m[36m(TorchTrainer pid=3317796)[0m Starting distributed worker processes: ['3317934 (10.6.10.5)']
[2m[36m(RayTrainWorker pid=3317934)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3317934)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3317934)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3317934)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3317934)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:51:30,097	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_045af77a
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=3317796, ip=10.6.10.5, actor_id=bb161aea5ba72ea89f5ce0bd01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=3317934, ip=10.6.10.5, actor_id=be5efe4ac48c75f55fc0049d01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14b875cdc220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-44-40/TorchTrainer_045af77a_10_batch_size=8,cell_type=gut,layer_size=8,lr=0.0000_2023-11-18_00-50-37/wandb/offline-run-20231118_005120-045af77a
[2m[36m(_WandbLoggingActor pid=3317929)[0m wandb: Find logs at: ./wandb/offline-run-20231118_005120-045af77a/logs
2023-11-18 00:51:35,909	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_29baf119, TorchTrainer_975e214f, TorchTrainer_f1647328, TorchTrainer_b1ef09bc, TorchTrainer_0cb9842b, TorchTrainer_fc2f2d57, TorchTrainer_424fded2, TorchTrainer_f0ce5f02, TorchTrainer_fba8961a, TorchTrainer_045af77a]
2023-11-18 00:51:35,955	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 368, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
