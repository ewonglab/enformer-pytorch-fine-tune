Global seed set to 42
2023-11-17 06:54:55,635	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-17 06:55:02,822	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-17 06:55:02,824	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-17 06:55:02,925	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=669454)[0m Starting distributed worker processes: ['670176 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=670176)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=670176)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=670176)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=670176)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=670176)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:55:35,321	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_f1969c6c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=669454, ip=10.6.8.5, actor_id=fc2c572e033594140802c6bf01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=670176, ip=10.6.8.5, actor_id=6b9053d75a962434e96bcf4301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146bb2520190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_f1969c6c_1_batch_size=8,cell_type=surface_ecto,layer_size=32,lr=0.0002_2023-11-17_06-55-02/wandb/offline-run-20231117_065525-f1969c6c
[2m[36m(_WandbLoggingActor pid=670171)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065525-f1969c6c/logs
[2m[36m(TorchTrainer pid=670568)[0m Starting distributed worker processes: ['670698 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=670698)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=670698)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=670698)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=670698)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=670698)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:56:07,997	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_49f49b15
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=670568, ip=10.6.8.5, actor_id=76d52d528dfdd338f88db97601000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=670698, ip=10.6.8.5, actor_id=ad16fad4c03346a6122d87e301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14d36fdac1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_49f49b15_2_batch_size=4,cell_type=surface_ecto,layer_size=16,lr=0.0000_2023-11-17_06-55-17/wandb/offline-run-20231117_065559-49f49b15
[2m[36m(_WandbLoggingActor pid=670692)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065559-49f49b15/logs
[2m[36m(TorchTrainer pid=671095)[0m Starting distributed worker processes: ['671227 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=671227)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=671227)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=671227)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=671227)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=671227)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:56:40,855	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_f6d47125
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=671095, ip=10.6.8.5, actor_id=524a8366828ea014249f696501000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=671227, ip=10.6.8.5, actor_id=6e2222c8bb9d3624e278e0a201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x148d79398130>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_f6d47125_3_batch_size=4,cell_type=surface_ecto,layer_size=8,lr=0.0001_2023-11-17_06-55-52/wandb/offline-run-20231117_065632-f6d47125
[2m[36m(_WandbLoggingActor pid=671222)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065632-f6d47125/logs
[2m[36m(TorchTrainer pid=671621)[0m Starting distributed worker processes: ['671751 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=671751)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=671751)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=671751)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=671751)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=671751)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:57:14,012	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_e1407454
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=671621, ip=10.6.8.5, actor_id=1fe17ae63ae54b27155bac2701000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=671751, ip=10.6.8.5, actor_id=77272156abe246af986b8d0e01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14cb6a461220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_e1407454_4_batch_size=4,cell_type=surface_ecto,layer_size=16,lr=0.0070_2023-11-17_06-56-25/wandb/offline-run-20231117_065705-e1407454
[2m[36m(_WandbLoggingActor pid=671745)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065705-e1407454/logs
[2m[36m(TorchTrainer pid=672147)[0m Starting distributed worker processes: ['672276 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=672276)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=672276)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=672276)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=672276)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=672276)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:57:46,535	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_18400314
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=672147, ip=10.6.8.5, actor_id=47b300f9e941d0ed2d10fdd801000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=672276, ip=10.6.8.5, actor_id=50cbcb95d787b5029bb0f77401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1532af1261f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_18400314_5_batch_size=4,cell_type=surface_ecto,layer_size=32,lr=0.0016_2023-11-17_06-56-58/wandb/offline-run-20231117_065738-18400314
[2m[36m(_WandbLoggingActor pid=672271)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065738-18400314/logs
[2m[36m(TorchTrainer pid=672669)[0m Starting distributed worker processes: ['672798 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=672798)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=672798)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=672798)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=672798)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=672798)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:58:18,159	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_5ae00b63
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=672669, ip=10.6.8.5, actor_id=905ba3707e82a63f5990fb9e01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=672798, ip=10.6.8.5, actor_id=7123e242884a6c0f2d73ff1c01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x15200569a1c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_5ae00b63_6_batch_size=4,cell_type=surface_ecto,layer_size=8,lr=0.0000_2023-11-17_06-57-31/wandb/offline-run-20231117_065809-5ae00b63
[2m[36m(_WandbLoggingActor pid=672793)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065809-5ae00b63/logs
[2m[36m(TorchTrainer pid=673199)[0m Starting distributed worker processes: ['673329 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=673329)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=673329)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=673329)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=673329)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=673329)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:58:49,928	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_94a8e1e8
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=673199, ip=10.6.8.5, actor_id=b23d9f4bbef2907a7eb6889101000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=673329, ip=10.6.8.5, actor_id=1cfffd70a502d5a62a81e1d501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x145eb5157190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_94a8e1e8_7_batch_size=8,cell_type=surface_ecto,layer_size=8,lr=0.0037_2023-11-17_06-58-02/wandb/offline-run-20231117_065841-94a8e1e8
[2m[36m(_WandbLoggingActor pid=673324)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065841-94a8e1e8/logs
[2m[36m(TorchTrainer pid=673722)[0m Starting distributed worker processes: ['673852 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=673852)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=673852)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=673852)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=673852)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=673852)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:59:22,295	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_bff30c3b
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=673722, ip=10.6.8.5, actor_id=811581b868254d8caf2f9f5401000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=673852, ip=10.6.8.5, actor_id=94b7cee7fa41a6853d1a1e1601000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1515a1298100>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_bff30c3b_8_batch_size=8,cell_type=surface_ecto,layer_size=8,lr=0.0006_2023-11-17_06-58-34/wandb/offline-run-20231117_065913-bff30c3b
[2m[36m(_WandbLoggingActor pid=673847)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065913-bff30c3b/logs
[2m[36m(TorchTrainer pid=674250)[0m Starting distributed worker processes: ['674375 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=674375)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=674375)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=674375)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=674375)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=674375)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:59:54,973	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_e57baa93
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=674250, ip=10.6.8.5, actor_id=c1f143375afc74967771eca001000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=674375, ip=10.6.8.5, actor_id=aba0f7291c78c2d016b5ec3501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14f02e5a0160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_e57baa93_9_batch_size=8,cell_type=surface_ecto,layer_size=32,lr=0.0053_2023-11-17_06-59-07/wandb/offline-run-20231117_065946-e57baa93
[2m[36m(_WandbLoggingActor pid=674370)[0m wandb: Find logs at: ./wandb/offline-run-20231117_065946-e57baa93/logs
[2m[36m(TorchTrainer pid=674768)[0m Starting distributed worker processes: ['674897 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=674897)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=674897)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=674897)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=674897)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=674897)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 07:00:26,831	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_4678d7c0
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=674768, ip=10.6.8.5, actor_id=52e4d6a80e9bb9b28ebb0dfe01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=674897, ip=10.6.8.5, actor_id=7f35eaed78d744d2be21bee901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x152ac79aa1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-54-50/TorchTrainer_4678d7c0_10_batch_size=4,cell_type=surface_ecto,layer_size=32,lr=0.0001_2023-11-17_06-59-39/wandb/offline-run-20231117_070018-4678d7c0
[2m[36m(_WandbLoggingActor pid=674892)[0m wandb: Find logs at: ./wandb/offline-run-20231117_070018-4678d7c0/logs
2023-11-17 07:00:32,644	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_f1969c6c, TorchTrainer_49f49b15, TorchTrainer_f6d47125, TorchTrainer_e1407454, TorchTrainer_18400314, TorchTrainer_5ae00b63, TorchTrainer_94a8e1e8, TorchTrainer_bff30c3b, TorchTrainer_e57baa93, TorchTrainer_4678d7c0]
2023-11-17 07:00:32,690	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 366, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
