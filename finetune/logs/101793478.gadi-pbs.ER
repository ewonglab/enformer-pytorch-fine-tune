Global seed set to 42
2023-11-19 10:39:54,418	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-19 10:40:06,361	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-19 10:40:06,365	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-19 10:40:06,748	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=3898684)[0m Starting distributed worker processes: ['3899186 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=3899186)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3899186)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3899186)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3899186)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3899186)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3899186)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3899186)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3899186)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3899186)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/lightning_logs
[2m[36m(RayTrainWorker pid=3899186)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3899186)[0m 
[2m[36m(RayTrainWorker pid=3899186)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3899186)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3899186)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3899186)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3899186)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3899186)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3899186)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3899186)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3899186)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3899186)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3899186)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3899186)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3899186)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3899186)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3899186)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3899186)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3899186)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3899186)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 10:42:42,424	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000000)
2023-11-19 10:42:45,148	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.724 s, which may be a performance bottleneck.
2023-11-19 10:42:45,149	WARNING util.py:315 -- The `process_trial_result` operation took 2.726 s, which may be a performance bottleneck.
2023-11-19 10:42:45,149	WARNING util.py:315 -- Processing trial results took 2.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 10:42:45,150	WARNING util.py:315 -- The `process_trial_result` operation took 2.726 s, which may be a performance bottleneck.
2023-11-19 10:44:17,864	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000001)
2023-11-19 10:45:54,540	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000002)
2023-11-19 10:47:30,358	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000003)
2023-11-19 10:49:06,333	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000004)
2023-11-19 10:50:40,677	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000005)
2023-11-19 10:52:14,631	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000006)
2023-11-19 10:53:48,659	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000007)
2023-11-19 10:55:22,386	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000008)
2023-11-19 10:56:55,950	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000009)
2023-11-19 10:58:29,596	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000010)
2023-11-19 11:00:03,560	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000011)
2023-11-19 11:01:37,337	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000012)
2023-11-19 11:03:11,244	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000013)
2023-11-19 11:04:44,888	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000014)
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000015)
2023-11-19 11:06:18,547	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-19 11:07:52,250	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000016)
2023-11-19 11:09:25,864	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000017)
2023-11-19 11:10:59,909	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000018)
[2m[36m(RayTrainWorker pid=3899186)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       ptl/train_accuracy ▆██▇▇▇▃▅█▇▅█▇█▁▇▇▇▇
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             ptl/val_loss ▁▁▃▃▆▅▇▆▄▄▇▄▃▅▇▅▄▇▇█
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         time_this_iter_s █▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                timestamp ▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       ptl/train_accuracy 0.50463
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:           ptl/train_loss 0.69356
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             ptl/val_loss 0.69236
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                     step 1080
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       time_since_restore 1922.95725
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:         time_this_iter_s 93.46201
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:             time_total_s 1922.95725
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:                timestamp 1700352753
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_79dfa3d9_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0070_2023-11-19_10-40-06/wandb/offline-run-20231119_104036-79dfa3d9
[2m[36m(_WandbLoggingActor pid=3899181)[0m wandb: Find logs at: ./wandb/offline-run-20231119_104036-79dfa3d9/logs
[2m[36m(TorchTrainer pid=3979338)[0m Starting distributed worker processes: ['3980443 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=3980443)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3980443)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3980443)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3980443)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3980443)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3980443)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3980443)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3980443)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3980443)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_24c704c3_2_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0027_2023-11-19_10-40-23/lightning_logs
[2m[36m(RayTrainWorker pid=3980443)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3980443)[0m 
[2m[36m(RayTrainWorker pid=3980443)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3980443)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3980443)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3980443)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3980443)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3980443)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3980443)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3980443)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3980443)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3980443)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3980443)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3980443)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3980443)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3980443)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3980443)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3980443)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3980443)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3980443)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3980443)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_24c704c3_2_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0027_2023-11-19_10-40-23/checkpoint_000000)
2023-11-19 11:14:45,930	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.686 s, which may be a performance bottleneck.
2023-11-19 11:14:45,932	WARNING util.py:315 -- The `process_trial_result` operation took 3.701 s, which may be a performance bottleneck.
2023-11-19 11:14:45,932	WARNING util.py:315 -- Processing trial results took 3.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 11:14:45,932	WARNING util.py:315 -- The `process_trial_result` operation took 3.702 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         ptl/val_accuracy 0.69841
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             ptl/val_aupr 0.91106
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:            ptl/val_auroc 0.84878
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         ptl/val_f1_score 0.65574
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             ptl/val_loss 0.92315
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:              ptl/val_mcc 0.53567
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:        ptl/val_precision 1.0
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:           ptl/val_recall 0.4878
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:       time_since_restore 113.25184
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:         time_this_iter_s 113.25184
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:             time_total_s 113.25184
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:                timestamp 1700352882
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_24c704c3_2_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0027_2023-11-19_10-40-23/wandb/offline-run-20231119_111256-24c704c3
[2m[36m(_WandbLoggingActor pid=3980438)[0m wandb: Find logs at: ./wandb/offline-run-20231119_111256-24c704c3/logs
[2m[36m(TrainTrainable pid=3989273)[0m Trainable.setup took 12.951 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3989273)[0m Starting distributed worker processes: ['3991069 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=3991069)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3991069)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3991069)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3991069)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3991069)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3991069)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3991069)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3991069)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3991069)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/lightning_logs
[2m[36m(RayTrainWorker pid=3991069)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3991069)[0m 
[2m[36m(RayTrainWorker pid=3991069)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3991069)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3991069)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3991069)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3991069)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3991069)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3991069)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3991069)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3991069)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3991069)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3991069)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3991069)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3991069)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3991069)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3991069)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3991069)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3991069)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3991069)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 11:17:13,133	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000000)
2023-11-19 11:17:16,282	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.148 s, which may be a performance bottleneck.
2023-11-19 11:17:16,283	WARNING util.py:315 -- The `process_trial_result` operation took 3.161 s, which may be a performance bottleneck.
2023-11-19 11:17:16,283	WARNING util.py:315 -- Processing trial results took 3.161 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 11:17:16,283	WARNING util.py:315 -- The `process_trial_result` operation took 3.161 s, which may be a performance bottleneck.
2023-11-19 11:18:44,535	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000001)
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000002)
2023-11-19 11:20:16,075	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-19 11:21:47,656	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000003)
2023-11-19 11:23:19,317	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000004)
2023-11-19 11:24:50,979	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000005)
2023-11-19 11:26:22,561	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000006)
2023-11-19 11:27:54,245	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000007)
2023-11-19 11:29:25,872	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000008)
2023-11-19 11:30:57,735	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000009)
2023-11-19 11:32:29,418	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000010)
2023-11-19 11:34:01,105	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000011)
2023-11-19 11:35:32,842	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000012)
2023-11-19 11:37:04,519	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000013)
2023-11-19 11:38:36,223	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000014)
2023-11-19 11:40:08,212	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000015)
2023-11-19 11:41:39,828	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000016)
2023-11-19 11:43:11,421	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000017)
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000018)
2023-11-19 11:44:42,982	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=3991069)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       ptl/train_accuracy ▁▄▄▆▅▆▆▆▇▆▇▆▆▇▇▇█▇█
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:           ptl/train_loss █▆▆▄▅▃▃▃▂▃▂▃▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         ptl/val_accuracy ▅▃▃▄▄▅▅▅▄▆▁▇▇▆▅▆▇██▅
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             ptl/val_aupr ▁▂▃▄▅▅▆▇▇▇█▇█▇▇▇██▇▇
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:            ptl/val_auroc ▁▁▃▄▄▅▆▇▇▇████▇█████
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         ptl/val_f1_score ▆▁▁▅▃▄▆▄▅▇▄▆▇▇▆▇▇██▆
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             ptl/val_loss ▄██▄▄▄▅▄▂▂█▃▂▁▂▃▁▁▁▆
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:              ptl/val_mcc ▅▆▆▃▅▆▄▆▄▆▁█▇▆▅▆▇▇▇▄
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:        ptl/val_precision ▄██▃▆▇▂▇▃▄▁▇▅▄▃▃▅▅▅▂
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:           ptl/val_recall ▆▁▁▆▃▃█▃▆▇█▃▅▆▇▇▆▆▆█
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       ptl/train_accuracy 0.9213
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:           ptl/train_loss 0.20399
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         ptl/val_accuracy 0.77579
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             ptl/val_aupr 0.94206
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:            ptl/val_auroc 0.91138
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         ptl/val_f1_score 0.82609
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             ptl/val_loss 0.5162
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:              ptl/val_mcc 0.54192
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:        ptl/val_precision 0.7451
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:           ptl/val_recall 0.92683
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                     step 540
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       time_since_restore 1855.72632
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:         time_this_iter_s 91.06585
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:             time_total_s 1855.72632
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:                timestamp 1700354774
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_ef763eca_3_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_11-12-48/wandb/offline-run-20231119_111525-ef763eca
[2m[36m(_WandbLoggingActor pid=3991062)[0m wandb: Find logs at: ./wandb/offline-run-20231119_111525-ef763eca/logs
[2m[36m(TorchTrainer pid=4028710)[0m Starting distributed worker processes: ['4028838 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4028838)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=4028838)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4028838)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4028838)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4028838)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4028838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4028838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4028838)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4028838)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_2bd64f0f_4_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0179_2023-11-19_11-15-11/lightning_logs
[2m[36m(RayTrainWorker pid=4028838)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4028838)[0m 
[2m[36m(RayTrainWorker pid=4028838)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4028838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4028838)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4028838)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=4028838)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4028838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4028838)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=4028838)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4028838)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=4028838)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4028838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4028838)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4028838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4028838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4028838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4028838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4028838)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4028838)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=4028838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_2bd64f0f_4_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0179_2023-11-19_11-15-11/checkpoint_000000)
2023-11-19 11:48:30,304	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.120 s, which may be a performance bottleneck.
2023-11-19 11:48:30,305	WARNING util.py:315 -- The `process_trial_result` operation took 3.124 s, which may be a performance bottleneck.
2023-11-19 11:48:30,305	WARNING util.py:315 -- Processing trial results took 3.124 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 11:48:30,305	WARNING util.py:315 -- The `process_trial_result` operation took 3.124 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         ptl/val_accuracy 0.52778
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             ptl/val_aupr 0.5538
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:            ptl/val_auroc 0.45122
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         ptl/val_f1_score 0.68519
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             ptl/val_loss 1.14998
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:              ptl/val_mcc -0.20901
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:        ptl/val_precision 0.55224
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:           ptl/val_recall 0.90244
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:       time_since_restore 113.51311
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:         time_this_iter_s 113.51311
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:             time_total_s 113.51311
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:                timestamp 1700354907
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_2bd64f0f_4_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0179_2023-11-19_11-15-11/wandb/offline-run-20231119_114641-2bd64f0f
[2m[36m(_WandbLoggingActor pid=4028835)[0m wandb: Find logs at: ./wandb/offline-run-20231119_114641-2bd64f0f/logs
[2m[36m(TorchTrainer pid=4030306)[0m Starting distributed worker processes: ['4030438 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4030438)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=4030438)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4030438)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4030438)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4030438)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4030438)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4030438)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4030438)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4030438)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f904c777_5_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0050_2023-11-19_11-46-33/lightning_logs
[2m[36m(RayTrainWorker pid=4030438)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4030438)[0m 
[2m[36m(RayTrainWorker pid=4030438)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4030438)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4030438)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4030438)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=4030438)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4030438)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4030438)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=4030438)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4030438)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=4030438)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4030438)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4030438)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4030438)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4030438)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4030438)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4030438)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4030438)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4030438)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=4030438)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f904c777_5_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0050_2023-11-19_11-46-33/checkpoint_000000)
2023-11-19 11:50:42,570	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.972 s, which may be a performance bottleneck.
2023-11-19 11:50:42,571	WARNING util.py:315 -- The `process_trial_result` operation took 2.974 s, which may be a performance bottleneck.
2023-11-19 11:50:42,571	WARNING util.py:315 -- Processing trial results took 2.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 11:50:42,571	WARNING util.py:315 -- The `process_trial_result` operation took 2.975 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         ptl/val_accuracy 0.75794
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             ptl/val_aupr 0.93677
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:            ptl/val_auroc 0.9
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         ptl/val_f1_score 0.76056
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             ptl/val_loss 2.11405
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:              ptl/val_mcc 0.55854
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:        ptl/val_precision 0.9
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:           ptl/val_recall 0.65854
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:       time_since_restore 113.23253
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:         time_this_iter_s 113.23253
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:             time_total_s 113.23253
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:                timestamp 1700355039
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f904c777_5_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0050_2023-11-19_11-46-33/wandb/offline-run-20231119_114854-f904c777
[2m[36m(_WandbLoggingActor pid=4030435)[0m wandb: Find logs at: ./wandb/offline-run-20231119_114854-f904c777/logs
[2m[36m(TorchTrainer pid=4031915)[0m Starting distributed worker processes: ['4032045 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4032045)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4032045)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4032045)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4032045)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4032045)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=4032045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4032045)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4032045)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4032045)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/lightning_logs
[2m[36m(RayTrainWorker pid=4032045)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4032045)[0m 
[2m[36m(RayTrainWorker pid=4032045)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4032045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4032045)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4032045)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=4032045)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4032045)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4032045)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=4032045)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4032045)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=4032045)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4032045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4032045)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4032045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4032045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4032045)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4032045)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4032045)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4032045)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 11:52:53,238	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000000)
2023-11-19 11:52:56,464	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.225 s, which may be a performance bottleneck.
2023-11-19 11:52:56,465	WARNING util.py:315 -- The `process_trial_result` operation took 3.229 s, which may be a performance bottleneck.
2023-11-19 11:52:56,465	WARNING util.py:315 -- Processing trial results took 3.229 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 11:52:56,465	WARNING util.py:315 -- The `process_trial_result` operation took 3.229 s, which may be a performance bottleneck.
2023-11-19 11:54:26,652	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000001)
2023-11-19 11:56:00,289	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000002)
2023-11-19 11:57:33,831	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000003)
2023-11-19 11:59:07,365	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000004)
2023-11-19 12:00:41,018	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000005)
2023-11-19 12:02:14,596	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000006)
2023-11-19 12:03:48,188	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000007)
2023-11-19 12:05:21,935	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000008)
2023-11-19 12:06:55,847	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000009)
2023-11-19 12:08:29,436	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000010)
2023-11-19 12:10:02,974	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000011)
2023-11-19 12:11:36,538	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000012)
2023-11-19 12:13:10,135	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000013)
2023-11-19 12:14:43,777	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000014)
2023-11-19 12:16:17,628	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000015)
2023-11-19 12:17:51,267	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000016)
2023-11-19 12:19:24,998	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000017)
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000018)
2023-11-19 12:20:58,848	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=4032045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       ptl/train_accuracy ▁▃▅▅▆▆▇▆▇▆▆▆▆▇█▇█▇█
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:           ptl/train_loss █▆▄▄▄▃▃▃▃▂▃▃▃▁▁▂▂▁▁
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         ptl/val_accuracy ▆▇▅▅▄▄▅▅▅▁▂▅▅▆▅█▆▆▇▅
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             ptl/val_aupr ▁▂▃▃▄▄▅▅▆▇▇▆█▇▇▇▇▇▇█
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:            ptl/val_auroc ▁▂▃▃▄▄▅▅▆▇▇▇█▇▇▇▇█▇█
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         ptl/val_f1_score ▅▆▄▄▃▁▄▂▄▂▃▁▄▆▅█▅▆▇▅
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             ptl/val_loss ▅▄▃▄▄▃▂▄▂█▇█▁▂▄▁▁▃▁▄
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:              ptl/val_mcc ▆▇▄▄▃▅▅▇▄▁▂▇▅▆▄█▆▆▇▄
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:        ptl/val_precision ▅▅▃▃▃▆▄▇▄▁▁█▄▄▃▅▅▄▅▃
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:           ptl/val_recall ▅▅▆▆▆▂▅▂▅██▁▅▆▇▆▅▆▆▇
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       ptl/train_accuracy 0.90741
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:           ptl/train_loss 0.24712
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         ptl/val_accuracy 0.77315
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             ptl/val_aupr 0.9444
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:            ptl/val_auroc 0.91138
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         ptl/val_f1_score 0.82222
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             ptl/val_loss 0.44943
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:              ptl/val_mcc 0.53669
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:        ptl/val_precision 0.7551
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:           ptl/val_recall 0.90244
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                     step 1080
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       time_since_restore 1885.93898
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:         time_this_iter_s 93.1859
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:             time_total_s 1885.93898
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:                timestamp 1700356952
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_f3168fa5_6_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0000_2023-11-19_11-48-46/wandb/offline-run-20231119_115106-f3168fa5
[2m[36m(_WandbLoggingActor pid=4032040)[0m wandb: Find logs at: ./wandb/offline-run-20231119_115106-f3168fa5/logs
[2m[36m(TorchTrainer pid=4043638)[0m Starting distributed worker processes: ['4043769 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4043769)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4043769)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4043769)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4043769)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4043769)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=4043769)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4043769)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4043769)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4043769)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_32845515_7_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0004_2023-11-19_11-50-59/lightning_logs
[2m[36m(RayTrainWorker pid=4043769)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4043769)[0m 
[2m[36m(RayTrainWorker pid=4043769)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4043769)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4043769)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4043769)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=4043769)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4043769)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4043769)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=4043769)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4043769)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=4043769)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4043769)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4043769)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4043769)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4043769)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4043769)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4043769)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4043769)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4043769)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 12:24:41,189	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4043769)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_32845515_7_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0004_2023-11-19_11-50-59/checkpoint_000000)
2023-11-19 12:24:44,171	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.982 s, which may be a performance bottleneck.
2023-11-19 12:24:44,173	WARNING util.py:315 -- The `process_trial_result` operation took 2.986 s, which may be a performance bottleneck.
2023-11-19 12:24:44,173	WARNING util.py:315 -- Processing trial results took 2.986 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 12:24:44,173	WARNING util.py:315 -- The `process_trial_result` operation took 2.986 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=4043769)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_32845515_7_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0004_2023-11-19_11-50-59/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       ptl/train_accuracy 0.5
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:           ptl/train_loss 0.94145
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             ptl/val_loss 0.67992
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                     step 108
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       time_since_restore 202.79229
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:         time_this_iter_s 89.98031
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:             time_total_s 202.79229
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:                timestamp 1700357174
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_32845515_7_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0004_2023-11-19_11-50-59/wandb/offline-run-20231119_122256-32845515
[2m[36m(_WandbLoggingActor pid=4043764)[0m wandb: Find logs at: ./wandb/offline-run-20231119_122256-32845515/logs
[2m[36m(TorchTrainer pid=4045508)[0m Starting distributed worker processes: ['4045638 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4045638)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=4045638)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4045638)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4045638)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4045638)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4045638)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4045638)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4045638)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4045638)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/lightning_logs
[2m[36m(RayTrainWorker pid=4045638)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4045638)[0m 
[2m[36m(RayTrainWorker pid=4045638)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4045638)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4045638)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4045638)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=4045638)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4045638)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4045638)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=4045638)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4045638)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=4045638)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4045638)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4045638)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4045638)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4045638)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4045638)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4045638)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4045638)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4045638)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 12:28:22,171	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000000)
2023-11-19 12:28:25,150	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.979 s, which may be a performance bottleneck.
2023-11-19 12:28:25,151	WARNING util.py:315 -- The `process_trial_result` operation took 2.982 s, which may be a performance bottleneck.
2023-11-19 12:28:25,152	WARNING util.py:315 -- Processing trial results took 2.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 12:28:25,152	WARNING util.py:315 -- The `process_trial_result` operation took 2.983 s, which may be a performance bottleneck.
2023-11-19 12:29:53,185	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000001)
2023-11-19 12:31:24,091	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000002)
2023-11-19 12:32:55,256	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000003)
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000004)
2023-11-19 12:34:26,387	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-19 12:35:57,554	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000005)
2023-11-19 12:37:28,661	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000006)
[2m[36m(RayTrainWorker pid=4045638)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       ptl/train_accuracy ▁▃▆▇▇▆█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:           ptl/train_loss █▃▂▂▁▂▁
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         ptl/val_accuracy ▄▄▄▄▄▂▁█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             ptl/val_aupr ▁▃▅▅▅▄█▆
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:            ptl/val_auroc ▁▃▅▅▆▆█▇
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         ptl/val_f1_score ▆▆▇▇▇▁▆█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             ptl/val_loss ▂▄▁▁▁██▃
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:              ptl/val_mcc ▄▄▄▄▄▅▁█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:        ptl/val_precision ▅▅▄▄▄█▁▇
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:           ptl/val_recall ▄▄▅▅▅▁█▄
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       ptl/train_accuracy 0.9213
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:           ptl/train_loss 0.24524
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         ptl/val_accuracy 0.82937
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             ptl/val_aupr 0.94303
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:            ptl/val_auroc 0.91138
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         ptl/val_f1_score 0.83784
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             ptl/val_loss 0.48974
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:              ptl/val_mcc 0.6828
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:        ptl/val_precision 0.93939
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:           ptl/val_recall 0.7561
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                     step 216
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       time_since_restore 746.5974
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:         time_this_iter_s 90.96985
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:             time_total_s 746.5974
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:                timestamp 1700357939
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_466d5e3c_8_batch_size=8,cell_type=exe_endo,layer_size=16,lr=0.0005_2023-11-19_12-22-48/wandb/offline-run-20231119_122636-466d5e3c
[2m[36m(_WandbLoggingActor pid=4045633)[0m wandb: Find logs at: ./wandb/offline-run-20231119_122636-466d5e3c/logs
[2m[36m(TorchTrainer pid=4050592)[0m Starting distributed worker processes: ['4050722 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4050722)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4050722)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4050722)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4050722)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4050722)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=4050722)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4050722)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4050722)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4050722)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_756312b1_9_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_12-26-29/lightning_logs
[2m[36m(RayTrainWorker pid=4050722)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4050722)[0m 
[2m[36m(RayTrainWorker pid=4050722)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4050722)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4050722)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4050722)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=4050722)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4050722)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4050722)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=4050722)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4050722)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=4050722)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4050722)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4050722)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4050722)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4050722)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4050722)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4050722)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4050722)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4050722)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=4050722)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_756312b1_9_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_12-26-29/checkpoint_000000)
2023-11-19 12:41:14,754	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.132 s, which may be a performance bottleneck.
2023-11-19 12:41:14,756	WARNING util.py:315 -- The `process_trial_result` operation took 3.136 s, which may be a performance bottleneck.
2023-11-19 12:41:14,756	WARNING util.py:315 -- Processing trial results took 3.136 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 12:41:14,756	WARNING util.py:315 -- The `process_trial_result` operation took 3.136 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             ptl/val_loss 0.69138
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:       time_since_restore 115.74836
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:         time_this_iter_s 115.74836
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:             time_total_s 115.74836
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:                timestamp 1700358071
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_756312b1_9_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0001_2023-11-19_12-26-29/wandb/offline-run-20231119_123925-756312b1
[2m[36m(_WandbLoggingActor pid=4050719)[0m wandb: Find logs at: ./wandb/offline-run-20231119_123925-756312b1/logs
[2m[36m(TorchTrainer pid=4052186)[0m Starting distributed worker processes: ['4052316 (10.6.29.2)']
[2m[36m(RayTrainWorker pid=4052316)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=4052316)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=4052316)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=4052316)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=4052316)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=4052316)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=4052316)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=4052316)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=4052316)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_1cb2c36a_10_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0001_2023-11-19_12-39-15/lightning_logs
[2m[36m(RayTrainWorker pid=4052316)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=4052316)[0m 
[2m[36m(RayTrainWorker pid=4052316)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=4052316)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4052316)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=4052316)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=4052316)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=4052316)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=4052316)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=4052316)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=4052316)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=4052316)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=4052316)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=4052316)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=4052316)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=4052316)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4052316)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=4052316)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=4052316)[0m finetune/fine_tune_tidy.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=4052316)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-19 12:43:24,059	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=4052316)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_1cb2c36a_10_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0001_2023-11-19_12-39-15/checkpoint_000000)
2023-11-19 12:43:27,186	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.127 s, which may be a performance bottleneck.
2023-11-19 12:43:27,188	WARNING util.py:315 -- The `process_trial_result` operation took 3.131 s, which may be a performance bottleneck.
2023-11-19 12:43:27,188	WARNING util.py:315 -- Processing trial results took 3.131 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-19 12:43:27,188	WARNING util.py:315 -- The `process_trial_result` operation took 3.132 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=4052316)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_1cb2c36a_10_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0001_2023-11-19_12-39-15/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       ptl/train_accuracy 0.51019
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:           ptl/train_loss 0.73526
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             ptl/val_loss 0.68202
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       time_since_restore 201.18099
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:         time_this_iter_s 87.72944
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:             time_total_s 201.18099
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:                timestamp 1700358294
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-19_10-39-48/TorchTrainer_1cb2c36a_10_batch_size=8,cell_type=exe_endo,layer_size=8,lr=0.0001_2023-11-19_12-39-15/wandb/offline-run-20231119_124138-1cb2c36a
[2m[36m(_WandbLoggingActor pid=4052313)[0m wandb: Find logs at: ./wandb/offline-run-20231119_124138-1cb2c36a/logs
