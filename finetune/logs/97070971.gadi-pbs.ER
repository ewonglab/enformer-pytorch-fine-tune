Global seed set to 42
2023-10-04 23:55:01,270	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-04 23:55:13,199	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-04 23:55:13,206	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 23:55:13,268	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1359592)[0m Starting distributed worker processes: ['1360861 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1360861)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1360861)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1360861)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1360861)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1360861)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1360861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1360861)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1360861)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1360861)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/lightning_logs
[2m[36m(RayTrainWorker pid=1360861)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1360861)[0m 
[2m[36m(RayTrainWorker pid=1360861)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1360861)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1360861)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1360861)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1360861)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1360861)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1360861)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1360861)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1360861)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1360861)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1360861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1360861)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1360861)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1360861)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1360861)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1360861)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1360861)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1360861)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1360861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1360861)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1360861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1360861)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1360861)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1360861)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1360861)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1360861)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1360861)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1360861)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-04 23:57:18,716	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000000)
2023-10-04 23:57:21,363	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.646 s, which may be a performance bottleneck.
2023-10-04 23:57:21,364	WARNING util.py:315 -- The `process_trial_result` operation took 2.648 s, which may be a performance bottleneck.
2023-10-04 23:57:21,364	WARNING util.py:315 -- Processing trial results took 2.649 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-04 23:57:21,364	WARNING util.py:315 -- The `process_trial_result` operation took 2.649 s, which may be a performance bottleneck.
2023-10-04 23:58:37,562	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000001)
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000002)
2023-10-04 23:59:56,189	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:01:15,183	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000003)
2023-10-05 00:02:33,360	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000004)
2023-10-05 00:03:51,550	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000005)
2023-10-05 00:05:10,070	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000006)
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000007)
2023-10-05 00:06:28,783	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:07:46,820	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000008)
2023-10-05 00:09:04,832	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000009)
2023-10-05 00:10:22,859	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000010)
2023-10-05 00:11:40,893	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000011)
2023-10-05 00:12:58,958	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000012)
2023-10-05 00:14:16,973	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000013)
2023-10-05 00:15:34,929	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000014)
2023-10-05 00:16:53,030	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000015)
2023-10-05 00:18:11,007	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000016)
2023-10-05 00:19:28,995	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000017)
2023-10-05 00:20:47,074	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1360861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         ptl/val_accuracy ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         ptl/val_f1_score ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             ptl/val_loss ▁▃▆▆▇█▇▇▆▇▇█▇████▇▆█
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:              ptl/val_mcc ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:        ptl/val_precision ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           ptl/val_recall ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         time_this_iter_s █▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           train_accuracy ██▁██▁▁██▁██████▁██▁
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:               train_loss ▁▄▆▄▃█▇▃▄▇▃▂▃▃▃▃█▃▄█
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       ptl/train_accuracy 0.69314
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           ptl/train_loss 0.69314
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             ptl/val_loss 0.71011
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                     step 880
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       time_since_restore 1590.51462
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:         time_this_iter_s 77.76164
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:             time_total_s 1590.51462
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:                timestamp 1696425725
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:               train_loss 0.77205
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_615dc22f_1_batch_size=4,layer_size=32,lr=0.0103_2023-10-04_23-55-13/wandb/offline-run-20231004_235533-615dc22f
[2m[36m(_WandbLoggingActor pid=1360851)[0m wandb: Find logs at: ./wandb/offline-run-20231004_235533-615dc22f/logs
[2m[36m(TorchTrainer pid=1377532)[0m Starting distributed worker processes: ['1377663 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1377663)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1377663)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1377663)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1377663)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1377663)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1377663)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1377663)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1377663)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1377663)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/lightning_logs
[2m[36m(RayTrainWorker pid=1377663)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1377663)[0m 
[2m[36m(RayTrainWorker pid=1377663)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1377663)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1377663)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1377663)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1377663)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1377663)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1377663)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1377663)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1377663)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1377663)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1377663)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1377663)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1377663)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1377663)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1377663)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1377663)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1377663)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1377663)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1377663)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1377663)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1377663)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1377663)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1377663)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1377663)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1377663)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1377663)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1377663)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1377663)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 00:23:59,094	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000000)
2023-10-05 00:24:01,829	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.734 s, which may be a performance bottleneck.
2023-10-05 00:24:01,830	WARNING util.py:315 -- The `process_trial_result` operation took 2.738 s, which may be a performance bottleneck.
2023-10-05 00:24:01,830	WARNING util.py:315 -- Processing trial results took 2.739 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:24:01,831	WARNING util.py:315 -- The `process_trial_result` operation took 2.739 s, which may be a performance bottleneck.
2023-10-05 00:25:15,102	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000001)
2023-10-05 00:26:31,074	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000002)
2023-10-05 00:27:47,321	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000003)
2023-10-05 00:29:03,362	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000004)
2023-10-05 00:30:19,466	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000005)
2023-10-05 00:31:35,525	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000006)
2023-10-05 00:32:51,546	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000007)
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000008)
2023-10-05 00:34:07,632	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:35:24,149	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000009)
2023-10-05 00:36:40,200	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000010)
2023-10-05 00:37:56,275	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000011)
2023-10-05 00:39:12,237	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000012)
2023-10-05 00:40:28,283	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000013)
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000014)
2023-10-05 00:41:44,283	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:43:00,288	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000015)
2023-10-05 00:44:16,277	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000016)
2023-10-05 00:45:32,323	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000017)
2023-10-05 00:46:48,369	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1377663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             ptl/val_loss ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           train_accuracy ▂▄▅▂▄▅█▄▂▅▁▅▄▂▄▄▄▅▂▄
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:               train_loss ▇▆▄▇▆▄▁▅▇▄█▄▅▆▅▅▅▄▆▅
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       ptl/train_accuracy 0.69701
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           ptl/train_loss 0.69701
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             ptl/val_loss 0.6834
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       time_since_restore 1535.81977
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:         time_this_iter_s 75.70956
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:             time_total_s 1535.81977
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:                timestamp 1696427284
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:               train_loss 0.70259
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_cf2e3dfe_2_batch_size=8,layer_size=16,lr=0.0003_2023-10-04_23-55-27/wandb/offline-run-20231005_002228-cf2e3dfe
[2m[36m(_WandbLoggingActor pid=1377660)[0m wandb: Find logs at: ./wandb/offline-run-20231005_002228-cf2e3dfe/logs
[2m[36m(TorchTrainer pid=1389064)[0m Starting distributed worker processes: ['1389195 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1389195)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1389195)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1389195)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1389195)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1389195)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1389195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1389195)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1389195)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1389195)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b630f466_3_batch_size=4,layer_size=8,lr=0.0006_2023-10-05_00-22-21/lightning_logs
[2m[36m(RayTrainWorker pid=1389195)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1389195)[0m 
[2m[36m(RayTrainWorker pid=1389195)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1389195)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1389195)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1389195)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1389195)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1389195)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1389195)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1389195)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1389195)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1389195)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1389195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1389195)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1389195)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1389195)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1389195)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1389195)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1389195)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1389195)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1389195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1389195)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1389195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1389195)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1389195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1389195)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1389195)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1389195)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1389195)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1389195)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1389195)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b630f466_3_batch_size=4,layer_size=8,lr=0.0006_2023-10-05_00-22-21/checkpoint_000000)
2023-10-05 00:50:02,486	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.974 s, which may be a performance bottleneck.
2023-10-05 00:50:02,488	WARNING util.py:315 -- The `process_trial_result` operation took 2.978 s, which may be a performance bottleneck.
2023-10-05 00:50:02,488	WARNING util.py:315 -- Processing trial results took 2.978 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:50:02,488	WARNING util.py:315 -- The `process_trial_result` operation took 2.978 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             ptl/val_loss 0.73161
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:       time_since_restore 96.88985
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:         time_this_iter_s 96.88985
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:             time_total_s 96.88985
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:                timestamp 1696427399
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:               train_loss 0.84534
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b630f466_3_batch_size=4,layer_size=8,lr=0.0006_2023-10-05_00-22-21/wandb/offline-run-20231005_004830-b630f466
[2m[36m(_WandbLoggingActor pid=1389192)[0m wandb: Find logs at: ./wandb/offline-run-20231005_004830-b630f466/logs
[2m[36m(TorchTrainer pid=1390662)[0m Starting distributed worker processes: ['1390793 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1390793)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1390793)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1390793)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1390793)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1390793)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1390793)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1390793)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1390793)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1390793)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_785cb757_4_batch_size=4,layer_size=32,lr=0.0089_2023-10-05_00-48-22/lightning_logs
[2m[36m(RayTrainWorker pid=1390793)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1390793)[0m 
[2m[36m(RayTrainWorker pid=1390793)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1390793)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1390793)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1390793)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1390793)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1390793)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1390793)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1390793)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1390793)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1390793)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1390793)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1390793)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1390793)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1390793)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1390793)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1390793)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1390793)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1390793)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1390793)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1390793)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1390793)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1390793)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1390793)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1390793)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1390793)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1390793)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1390793)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1390793)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1390793)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_785cb757_4_batch_size=4,layer_size=32,lr=0.0089_2023-10-05_00-48-22/checkpoint_000000)
2023-10-05 00:52:00,209	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.079 s, which may be a performance bottleneck.
2023-10-05 00:52:00,211	WARNING util.py:315 -- The `process_trial_result` operation took 3.082 s, which may be a performance bottleneck.
2023-10-05 00:52:00,211	WARNING util.py:315 -- Processing trial results took 3.083 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:52:00,211	WARNING util.py:315 -- The `process_trial_result` operation took 3.083 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         ptl/val_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             ptl/val_loss 0.68145
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:       time_since_restore 97.83777
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:         time_this_iter_s 97.83777
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:             time_total_s 97.83777
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:                timestamp 1696427517
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:               train_loss 0.62331
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_785cb757_4_batch_size=4,layer_size=32,lr=0.0089_2023-10-05_00-48-22/wandb/offline-run-20231005_005027-785cb757
[2m[36m(_WandbLoggingActor pid=1390790)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005027-785cb757/logs
[2m[36m(TorchTrainer pid=1392263)[0m Starting distributed worker processes: ['1392394 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1392394)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1392394)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1392394)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1392394)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1392394)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1392394)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1392394)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1392394)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1392394)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_d162cf02_5_batch_size=8,layer_size=32,lr=0.0119_2023-10-05_00-50-19/lightning_logs
[2m[36m(RayTrainWorker pid=1392394)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1392394)[0m 
[2m[36m(RayTrainWorker pid=1392394)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1392394)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1392394)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1392394)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1392394)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1392394)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1392394)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1392394)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1392394)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1392394)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1392394)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1392394)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1392394)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1392394)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1392394)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1392394)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1392394)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1392394)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1392394)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1392394)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1392394)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1392394)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1392394)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1392394)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1392394)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1392394)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1392394)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_d162cf02_5_batch_size=8,layer_size=32,lr=0.0119_2023-10-05_00-50-19/checkpoint_000000)
2023-10-05 00:53:57,620	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.948 s, which may be a performance bottleneck.
2023-10-05 00:53:57,621	WARNING util.py:315 -- The `process_trial_result` operation took 2.951 s, which may be a performance bottleneck.
2023-10-05 00:53:57,621	WARNING util.py:315 -- Processing trial results took 2.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:53:57,622	WARNING util.py:315 -- The `process_trial_result` operation took 2.951 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             ptl/val_loss 0.68387
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                     step 22
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:       time_since_restore 98.61821
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:         time_this_iter_s 98.61821
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:             time_total_s 98.61821
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:                timestamp 1696427634
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:           train_accuracy 0.2
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:               train_loss 0.71855
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_d162cf02_5_batch_size=8,layer_size=32,lr=0.0119_2023-10-05_00-50-19/wandb/offline-run-20231005_005223-d162cf02
[2m[36m(_WandbLoggingActor pid=1392391)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005223-d162cf02/logs
[2m[36m(TorchTrainer pid=1393862)[0m Starting distributed worker processes: ['1393993 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1393993)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1393993)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1393993)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1393993)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1393993)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1393993)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1393993)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1393993)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1393993)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_e48df672_6_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-52-16/lightning_logs
[2m[36m(RayTrainWorker pid=1393993)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1393993)[0m 
[2m[36m(RayTrainWorker pid=1393993)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1393993)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1393993)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1393993)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1393993)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1393993)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1393993)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1393993)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1393993)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1393993)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1393993)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1393993)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1393993)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1393993)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1393993)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1393993)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1393993)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1393993)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1393993)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1393993)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1393993)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1393993)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1393993)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1393993)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1393993)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1393993)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1393993)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1393993)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1393993)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_e48df672_6_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-52-16/checkpoint_000000)
2023-10-05 00:55:54,409	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.917 s, which may be a performance bottleneck.
2023-10-05 00:55:54,411	WARNING util.py:315 -- The `process_trial_result` operation took 2.921 s, which may be a performance bottleneck.
2023-10-05 00:55:54,411	WARNING util.py:315 -- Processing trial results took 2.921 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:55:54,411	WARNING util.py:315 -- The `process_trial_result` operation took 2.921 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             ptl/val_loss 0.69667
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                     step 22
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:       time_since_restore 97.41153
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:         time_this_iter_s 97.41153
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:             time_total_s 97.41153
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:                timestamp 1696427751
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:           train_accuracy 0.8
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:               train_loss 0.68502
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_e48df672_6_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_00-52-16/wandb/offline-run-20231005_005421-e48df672
[2m[36m(_WandbLoggingActor pid=1393990)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005421-e48df672/logs
[2m[36m(TorchTrainer pid=1395461)[0m Starting distributed worker processes: ['1395591 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1395591)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1395591)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1395591)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1395591)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1395591)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1395591)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1395591)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1395591)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1395591)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_758f23d3_7_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_00-54-14/lightning_logs
[2m[36m(RayTrainWorker pid=1395591)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1395591)[0m 
[2m[36m(RayTrainWorker pid=1395591)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1395591)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1395591)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1395591)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1395591)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1395591)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1395591)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1395591)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1395591)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1395591)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1395591)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1395591)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1395591)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1395591)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1395591)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1395591)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1395591)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1395591)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1395591)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1395591)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1395591)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1395591)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1395591)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1395591)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1395591)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1395591)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1395591)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1395591)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1395591)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_758f23d3_7_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_00-54-14/checkpoint_000000)
2023-10-05 00:57:51,790	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.889 s, which may be a performance bottleneck.
2023-10-05 00:57:51,791	WARNING util.py:315 -- The `process_trial_result` operation took 2.893 s, which may be a performance bottleneck.
2023-10-05 00:57:51,792	WARNING util.py:315 -- Processing trial results took 2.893 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:57:51,792	WARNING util.py:315 -- The `process_trial_result` operation took 2.893 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             ptl/val_loss 0.69494
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                     step 22
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:       time_since_restore 97.80726
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:         time_this_iter_s 97.80726
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:             time_total_s 97.80726
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:                timestamp 1696427868
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:           train_accuracy 0.8
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:               train_loss 0.68893
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_758f23d3_7_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_00-54-14/wandb/offline-run-20231005_005618-758f23d3
[2m[36m(_WandbLoggingActor pid=1395588)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005618-758f23d3/logs
2023-10-05 00:58:11,598	WARNING worker.py:2058 -- Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2355, in ray._raylet._auto_reconnect.wrapper
  File "python/ray/_raylet.pyx", line 2453, in ray._raylet.GcsClient.internal_kv_get
  File "python/ray/_raylet.pyx", line 455, in ray._raylet.check_status
ray.exceptions.RpcError: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.6.8.2:51465: connection attempt timed out before receiving SETTINGS frame

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2165, in connect
    node.check_version_info()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/node.py", line 382, in check_version_info
    cluster_metadata = ray_usage_lib.get_cluster_metadata(self.get_gcs_client())
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/usage/usage_lib.py", line 719, in get_cluster_metadata
    gcs_client.internal_kv_get(
  File "python/ray/_raylet.pyx", line 2357, in ray._raylet._auto_reconnect.wrapper
ModuleNotFoundError: No module named 'grpc'

[2m[36m(TorchTrainer pid=1397063)[0m Starting distributed worker processes: ['1397193 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1397193)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1397193)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1397193)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1397193)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1397193)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1397193)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1397193)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1397193)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1397193)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/lightning_logs
[2m[36m(RayTrainWorker pid=1397193)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1397193)[0m 
[2m[36m(RayTrainWorker pid=1397193)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1397193)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1397193)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1397193)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1397193)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1397193)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1397193)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1397193)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1397193)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1397193)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1397193)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1397193)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1397193)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1397193)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1397193)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1397193)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1397193)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1397193)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1397193)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1397193)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1397193)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1397193)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1397193)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1397193)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1397193)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1397193)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1397193)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1397193)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000000)
2023-10-05 00:59:46,996	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:59:49,929	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.932 s, which may be a performance bottleneck.
2023-10-05 00:59:49,931	WARNING util.py:315 -- The `process_trial_result` operation took 2.936 s, which may be a performance bottleneck.
2023-10-05 00:59:49,931	WARNING util.py:315 -- Processing trial results took 2.937 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:59:49,931	WARNING util.py:315 -- The `process_trial_result` operation took 2.937 s, which may be a performance bottleneck.
2023-10-05 01:01:03,385	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000001)
2023-10-05 01:02:20,139	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000002)
2023-10-05 01:03:36,741	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000003)
2023-10-05 01:04:53,327	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000004)
2023-10-05 01:06:09,988	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000005)
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000006)
2023-10-05 01:07:26,744	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 01:08:43,285	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000007)
2023-10-05 01:09:59,880	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000008)
2023-10-05 01:11:16,550	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000009)
2023-10-05 01:12:33,261	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000010)
2023-10-05 01:13:49,818	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000011)
2023-10-05 01:15:06,501	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000012)
2023-10-05 01:16:23,150	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000013)
2023-10-05 01:17:39,705	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000014)
2023-10-05 01:18:56,403	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000015)
2023-10-05 01:20:12,991	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000016)
2023-10-05 01:21:29,666	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000017)
2023-10-05 01:22:46,451	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1397193)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             ptl/val_loss ▁▁▂▂▂▃▃▄▄▄▄▅▅▆▆▆▇▇▇█
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           train_accuracy ▂▄▅▂▄▅█▄▂▅▁▅▄▂▄▄▄▅▂▄
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:               train_loss ▇▅▄▇▅▄▁▅▇▄█▄▅▇▅▅▅▄▇▅
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       ptl/train_accuracy 0.7003
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           ptl/train_loss 0.7003
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             ptl/val_loss 0.67766
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       time_since_restore 1547.38531
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:         time_this_iter_s 76.51445
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:             time_total_s 1547.38531
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:                timestamp 1696429443
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:               train_loss 0.71026
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_bf42f7ae_8_batch_size=8,layer_size=32,lr=0.0001_2023-10-05_00-56-11/wandb/offline-run-20231005_005816-bf42f7ae
[2m[36m(_WandbLoggingActor pid=1397190)[0m wandb: Find logs at: ./wandb/offline-run-20231005_005816-bf42f7ae/logs
[2m[36m(TorchTrainer pid=1409056)[0m Starting distributed worker processes: ['1409185 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1409185)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1409185)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1409185)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1409185)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1409185)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1409185)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1409185)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1409185)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1409185)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_f701be45_9_batch_size=4,layer_size=32,lr=0.0006_2023-10-05_00-58-08/lightning_logs
[2m[36m(RayTrainWorker pid=1409185)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1409185)[0m 
[2m[36m(RayTrainWorker pid=1409185)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1409185)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1409185)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1409185)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1409185)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1409185)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1409185)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1409185)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1409185)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1409185)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1409185)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1409185)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1409185)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1409185)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1409185)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1409185)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1409185)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1409185)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1409185)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1409185)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1409185)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1409185)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1409185)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1409185)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1409185)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1409185)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1409185)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1409185)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1409185)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_f701be45_9_batch_size=4,layer_size=32,lr=0.0006_2023-10-05_00-58-08/checkpoint_000000)
2023-10-05 01:25:54,261	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.293 s, which may be a performance bottleneck.
2023-10-05 01:25:54,262	WARNING util.py:315 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.
2023-10-05 01:25:54,262	WARNING util.py:315 -- Processing trial results took 2.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:25:54,263	WARNING util.py:315 -- The `process_trial_result` operation took 2.297 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             ptl/val_loss 0.70456
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:       time_since_restore 95.22555
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:         time_this_iter_s 95.22555
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:             time_total_s 95.22555
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:                timestamp 1696429551
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:               train_loss 0.74506
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_f701be45_9_batch_size=4,layer_size=32,lr=0.0006_2023-10-05_00-58-08/wandb/offline-run-20231005_012423-f701be45
[2m[36m(_WandbLoggingActor pid=1409182)[0m wandb: Find logs at: ./wandb/offline-run-20231005_012423-f701be45/logs
[2m[36m(TorchTrainer pid=1410393)[0m Starting distributed worker processes: ['1410522 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1410522)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1410522)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1410522)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1410522)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1410522)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1410522)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1410522)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1410522)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1410522)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b917c9b8_10_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-24-16/lightning_logs
[2m[36m(RayTrainWorker pid=1410522)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1410522)[0m 
[2m[36m(RayTrainWorker pid=1410522)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1410522)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1410522)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1410522)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1410522)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1410522)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1410522)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1410522)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1410522)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1410522)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1410522)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1410522)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1410522)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1410522)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1410522)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1410522)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1410522)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1410522)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1410522)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1410522)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1410522)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1410522)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1410522)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1410522)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1410522)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1410522)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1410522)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1410522)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1410522)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b917c9b8_10_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-24-16/checkpoint_000000)
2023-10-05 01:27:44,268	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.349 s, which may be a performance bottleneck.
2023-10-05 01:27:44,269	WARNING util.py:315 -- The `process_trial_result` operation took 2.351 s, which may be a performance bottleneck.
2023-10-05 01:27:44,269	WARNING util.py:315 -- Processing trial results took 2.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:27:44,269	WARNING util.py:315 -- The `process_trial_result` operation took 2.351 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             ptl/val_loss 0.71193
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:       time_since_restore 94.51102
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:         time_this_iter_s 94.51102
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:             time_total_s 94.51102
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:                timestamp 1696429661
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:               train_loss 0.77473
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-54-57/TorchTrainer_b917c9b8_10_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-24-16/wandb/offline-run-20231005_012613-b917c9b8
[2m[36m(_WandbLoggingActor pid=1410519)[0m wandb: Find logs at: ./wandb/offline-run-20231005_012613-b917c9b8/logs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
2023-10-05 01:28:31,457	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-05 01:28:36,671	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-05 01:28:36,676	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-05 01:28:36,738	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1415773)[0m Starting distributed worker processes: ['1416496 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1416496)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1416496)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1416496)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1416496)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1416496)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1416496)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1416496)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1416496)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/lightning_logs
[2m[36m(RayTrainWorker pid=1416496)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1416496)[0m 
[2m[36m(RayTrainWorker pid=1416496)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1416496)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1416496)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1416496)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1416496)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1416496)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1416496)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1416496)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1416496)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1416496)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1416496)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1416496)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1416496)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1416496)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1416496)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1416496)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1416496)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1416496)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:30:25,985	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000000)
2023-10-05 01:30:28,197	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.212 s, which may be a performance bottleneck.
2023-10-05 01:30:28,198	WARNING util.py:315 -- The `process_trial_result` operation took 2.214 s, which may be a performance bottleneck.
2023-10-05 01:30:28,199	WARNING util.py:315 -- Processing trial results took 2.214 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:30:28,199	WARNING util.py:315 -- The `process_trial_result` operation took 2.214 s, which may be a performance bottleneck.
2023-10-05 01:31:41,782	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000001)
2023-10-05 01:32:57,810	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000002)
2023-10-05 01:34:13,802	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000003)
2023-10-05 01:35:29,815	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000004)
2023-10-05 01:36:45,676	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000005)
2023-10-05 01:38:01,698	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000006)
2023-10-05 01:39:18,075	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000007)
2023-10-05 01:40:34,084	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000008)
2023-10-05 01:41:50,136	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000009)
2023-10-05 01:43:06,082	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000010)
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000011)
2023-10-05 01:44:22,093	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 01:45:38,163	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000012)
2023-10-05 01:46:54,094	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000013)
2023-10-05 01:48:10,039	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000014)
2023-10-05 01:49:26,244	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000015)
2023-10-05 01:50:42,250	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000016)
2023-10-05 01:51:58,215	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000017)
2023-10-05 01:53:14,352	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1416496)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             ptl/val_loss ▁▁▂▂▂▃▃▄▄▄▄▅▅▆▆▆▇▇▇█
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           train_accuracy ▂▄▅▂▄▅█▄▂▅▁▅▄▂▄▄▄▅▂▄
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:               train_loss ▇▅▄▇▅▄▁▅▇▄█▄▅▇▅▅▅▄▇▅
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       ptl/train_accuracy 0.6988
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           ptl/train_loss 0.6988
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             ptl/val_loss 0.67979
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       time_since_restore 1533.33905
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:         time_this_iter_s 75.91951
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:             time_total_s 1533.33905
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:                timestamp 1696431270
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:               train_loss 0.70716
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_b4ce00af_1_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-28-36/wandb/offline-run-20231005_012857-b4ce00af
[2m[36m(_WandbLoggingActor pid=1416491)[0m wandb: Find logs at: ./wandb/offline-run-20231005_012857-b4ce00af/logs
[2m[36m(TorchTrainer pid=1429941)[0m Starting distributed worker processes: ['1430071 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1430071)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1430071)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1430071)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1430071)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1430071)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1430071)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1430071)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1430071)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/lightning_logs
[2m[36m(RayTrainWorker pid=1430071)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1430071)[0m 
[2m[36m(RayTrainWorker pid=1430071)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1430071)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1430071)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1430071)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1430071)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1430071)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1430071)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1430071)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1430071)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1430071)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1430071)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1430071)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1430071)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1430071)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1430071)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1430071)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1430071)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1430071)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:56:19,979	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1430071)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/checkpoint_000000)
2023-10-05 01:56:22,504	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.525 s, which may be a performance bottleneck.
2023-10-05 01:56:22,506	WARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.
2023-10-05 01:56:22,506	WARNING util.py:315 -- Processing trial results took 2.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:56:22,506	WARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.
2023-10-05 01:57:35,318	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1430071)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/checkpoint_000001)
2023-10-05 01:58:50,862	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1430071)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/checkpoint_000002)
[2m[36m(RayTrainWorker pid=1430071)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       ptl/train_accuracy █▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         ptl/val_accuracy ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             ptl/val_aupr ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:            ptl/val_auroc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         ptl/val_f1_score ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             ptl/val_loss ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:              ptl/val_mcc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:        ptl/val_precision ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           ptl/val_recall ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           train_accuracy ▁▄█▁
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:               train_loss █▃▁▂
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       ptl/train_accuracy 0.70156
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           ptl/train_loss 0.70156
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             ptl/val_loss 0.68727
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                     step 88
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       time_since_restore 318.9712
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:         time_this_iter_s 75.5744
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:             time_total_s 318.9712
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:                timestamp 1696431606
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:           train_accuracy 0.2
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:               train_loss 0.70979
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ad58f705_2_batch_size=8,layer_size=8,lr=0.0072_2023-10-05_01-28-50/wandb/offline-run-20231005_015451-ad58f705
[2m[36m(_WandbLoggingActor pid=1430068)[0m wandb: Find logs at: ./wandb/offline-run-20231005_015451-ad58f705/logs
[2m[36m(TorchTrainer pid=1432873)[0m Starting distributed worker processes: ['1433007 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1433007)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1433007)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1433007)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1433007)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1433007)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1433007)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1433007)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1433007)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_9f00bc1b_3_batch_size=8,layer_size=32,lr=0.0061_2023-10-05_01-54-44/lightning_logs
[2m[36m(RayTrainWorker pid=1433007)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1433007)[0m 
[2m[36m(RayTrainWorker pid=1433007)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1433007)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1433007)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1433007)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1433007)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1433007)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1433007)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1433007)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1433007)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1433007)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1433007)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1433007)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1433007)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1433007)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1433007)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1433007)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1433007)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1433007)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1433007)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_9f00bc1b_3_batch_size=8,layer_size=32,lr=0.0061_2023-10-05_01-54-44/checkpoint_000000)
2023-10-05 02:01:57,797	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.253 s, which may be a performance bottleneck.
2023-10-05 02:01:57,798	WARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.
2023-10-05 02:01:57,799	WARNING util.py:315 -- Processing trial results took 2.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:01:57,799	WARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             ptl/val_loss 0.6872
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                     step 22
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:       time_since_restore 95.67305
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:         time_this_iter_s 95.67305
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:             time_total_s 95.67305
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:                timestamp 1696431715
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:           train_accuracy 0.2
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:               train_loss 0.70888
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_9f00bc1b_3_batch_size=8,layer_size=32,lr=0.0061_2023-10-05_01-54-44/wandb/offline-run-20231005_020026-9f00bc1b
[2m[36m(_WandbLoggingActor pid=1433004)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020026-9f00bc1b/logs
[2m[36m(TorchTrainer pid=1434206)[0m Starting distributed worker processes: ['1434335 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1434335)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1434335)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1434335)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1434335)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1434335)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1434335)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1434335)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1434335)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_62a9b59f_4_batch_size=4,layer_size=16,lr=0.0005_2023-10-05_02-00-19/lightning_logs
[2m[36m(RayTrainWorker pid=1434335)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1434335)[0m 
[2m[36m(RayTrainWorker pid=1434335)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1434335)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1434335)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1434335)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1434335)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1434335)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1434335)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1434335)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1434335)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1434335)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1434335)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1434335)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1434335)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1434335)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1434335)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1434335)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1434335)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1434335)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1434335)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_62a9b59f_4_batch_size=4,layer_size=16,lr=0.0005_2023-10-05_02-00-19/checkpoint_000000)
2023-10-05 02:03:48,142	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.364 s, which may be a performance bottleneck.
2023-10-05 02:03:48,143	WARNING util.py:315 -- The `process_trial_result` operation took 2.366 s, which may be a performance bottleneck.
2023-10-05 02:03:48,143	WARNING util.py:315 -- Processing trial results took 2.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:03:48,143	WARNING util.py:315 -- The `process_trial_result` operation took 2.367 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             ptl/val_loss 0.701
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:       time_since_restore 94.47365
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:         time_this_iter_s 94.47365
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:             time_total_s 94.47365
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:                timestamp 1696431825
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:               train_loss 0.7298
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_62a9b59f_4_batch_size=4,layer_size=16,lr=0.0005_2023-10-05_02-00-19/wandb/offline-run-20231005_020217-62a9b59f
[2m[36m(_WandbLoggingActor pid=1434332)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020217-62a9b59f/logs
[2m[36m(TorchTrainer pid=1435790)[0m Starting distributed worker processes: ['1435919 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1435919)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1435919)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1435919)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1435919)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1435919)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1435919)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1435919)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1435919)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ce742e79_5_batch_size=4,layer_size=8,lr=0.0078_2023-10-05_02-02-11/lightning_logs
[2m[36m(RayTrainWorker pid=1435919)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1435919)[0m 
[2m[36m(RayTrainWorker pid=1435919)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1435919)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1435919)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1435919)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1435919)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1435919)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1435919)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1435919)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1435919)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1435919)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1435919)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1435919)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1435919)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1435919)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1435919)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1435919)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1435919)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1435919)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1435919)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ce742e79_5_batch_size=4,layer_size=8,lr=0.0078_2023-10-05_02-02-11/checkpoint_000000)
2023-10-05 02:05:39,048	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.349 s, which may be a performance bottleneck.
2023-10-05 02:05:39,049	WARNING util.py:315 -- The `process_trial_result` operation took 2.352 s, which may be a performance bottleneck.
2023-10-05 02:05:39,049	WARNING util.py:315 -- Processing trial results took 2.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:05:39,050	WARNING util.py:315 -- The `process_trial_result` operation took 2.352 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         ptl/val_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             ptl/val_loss 0.68304
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:       time_since_restore 94.33891
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:         time_this_iter_s 94.33891
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:             time_total_s 94.33891
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:                timestamp 1696431936
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:               train_loss 0.63416
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ce742e79_5_batch_size=4,layer_size=8,lr=0.0078_2023-10-05_02-02-11/wandb/offline-run-20231005_020408-ce742e79
[2m[36m(_WandbLoggingActor pid=1435916)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020408-ce742e79/logs
[2m[36m(TorchTrainer pid=1437384)[0m Starting distributed worker processes: ['1437513 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1437513)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1437513)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1437513)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1437513)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1437513)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1437513)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1437513)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1437513)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_2d6eaccf_6_batch_size=4,layer_size=32,lr=0.0037_2023-10-05_02-04-02/lightning_logs
[2m[36m(RayTrainWorker pid=1437513)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1437513)[0m 
[2m[36m(RayTrainWorker pid=1437513)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1437513)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1437513)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1437513)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1437513)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1437513)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1437513)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1437513)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1437513)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1437513)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1437513)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1437513)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1437513)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1437513)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1437513)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1437513)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1437513)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1437513)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_2d6eaccf_6_batch_size=4,layer_size=32,lr=0.0037_2023-10-05_02-04-02/checkpoint_000000)
2023-10-05 02:07:29,714	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.323 s, which may be a performance bottleneck.
2023-10-05 02:07:29,716	WARNING util.py:315 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.
2023-10-05 02:07:29,716	WARNING util.py:315 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:07:29,717	WARNING util.py:315 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             ptl/val_loss 0.69386
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:       time_since_restore 95.09773
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:         time_this_iter_s 95.09773
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:             time_total_s 95.09773
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:                timestamp 1696432047
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:               train_loss 0.69644
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_2d6eaccf_6_batch_size=4,layer_size=32,lr=0.0037_2023-10-05_02-04-02/wandb/offline-run-20231005_020558-2d6eaccf
[2m[36m(_WandbLoggingActor pid=1437510)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020558-2d6eaccf/logs
2023-10-05 02:07:46,214	WARNING worker.py:2058 -- Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2355, in ray._raylet._auto_reconnect.wrapper
  File "python/ray/_raylet.pyx", line 2453, in ray._raylet.GcsClient.internal_kv_get
  File "python/ray/_raylet.pyx", line 455, in ray._raylet.check_status
ray.exceptions.RpcError: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.6.8.2:50501: connection attempt timed out before receiving SETTINGS frame

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2165, in connect
    node.check_version_info()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/node.py", line 382, in check_version_info
    cluster_metadata = ray_usage_lib.get_cluster_metadata(self.get_gcs_client())
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/usage/usage_lib.py", line 719, in get_cluster_metadata
    gcs_client.internal_kv_get(
  File "python/ray/_raylet.pyx", line 2357, in ray._raylet._auto_reconnect.wrapper
ModuleNotFoundError: No module named 'grpc'

[2m[36m(TorchTrainer pid=1438717)[0m Starting distributed worker processes: ['1438846 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1438846)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1438846)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1438846)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1438846)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1438846)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1438846)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1438846)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1438846)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ae058956_7_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_02-05-52/lightning_logs
[2m[36m(RayTrainWorker pid=1438846)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1438846)[0m 
[2m[36m(RayTrainWorker pid=1438846)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1438846)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1438846)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1438846)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1438846)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1438846)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1438846)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1438846)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1438846)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1438846)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1438846)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1438846)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1438846)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1438846)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1438846)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1438846)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1438846)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1438846)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:09:19,654	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1438846)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ae058956_7_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_02-05-52/checkpoint_000000)
2023-10-05 02:09:22,056	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.401 s, which may be a performance bottleneck.
2023-10-05 02:09:22,057	WARNING util.py:315 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.
2023-10-05 02:09:22,058	WARNING util.py:315 -- Processing trial results took 2.406 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:09:22,058	WARNING util.py:315 -- The `process_trial_result` operation took 2.406 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=1438846)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ae058956_7_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_02-05-52/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           train_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       ptl/train_accuracy 0.73064
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           ptl/train_loss 0.73064
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         ptl/val_accuracy 0.625
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             ptl/val_loss 0.68215
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       time_since_restore 169.856
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:         time_this_iter_s 73.9248
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:             time_total_s 169.856
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:                timestamp 1696432235
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:               train_loss 0.70408
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ae058956_7_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_02-05-52/wandb/offline-run-20231005_020750-ae058956
[2m[36m(_WandbLoggingActor pid=1438843)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020750-ae058956/logs
[2m[36m(TorchTrainer pid=1440584)[0m Starting distributed worker processes: ['1440713 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1440713)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1440713)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1440713)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1440713)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1440713)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1440713)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1440713)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1440713)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/lightning_logs
[2m[36m(RayTrainWorker pid=1440713)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1440713)[0m 
[2m[36m(RayTrainWorker pid=1440713)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1440713)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1440713)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1440713)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1440713)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1440713)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1440713)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1440713)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1440713)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1440713)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1440713)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1440713)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1440713)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1440713)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1440713)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1440713)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1440713)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1440713)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:12:24,255	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000000)
2023-10-05 02:12:27,660	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.404 s, which may be a performance bottleneck.
2023-10-05 02:12:27,661	WARNING util.py:315 -- The `process_trial_result` operation took 3.409 s, which may be a performance bottleneck.
2023-10-05 02:12:27,662	WARNING util.py:315 -- Processing trial results took 3.409 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:12:27,662	WARNING util.py:315 -- The `process_trial_result` operation took 3.409 s, which may be a performance bottleneck.
2023-10-05 02:13:42,408	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000001)
2023-10-05 02:15:00,372	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000002)
2023-10-05 02:16:18,420	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000003)
2023-10-05 02:17:36,440	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000004)
2023-10-05 02:18:54,449	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000005)
2023-10-05 02:20:12,685	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000006)
[2m[36m(RayTrainWorker pid=1440713)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             ptl/val_loss ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           train_accuracy ███▁▁██▁
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:               train_loss ▁▁▁██▂▂▇
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       ptl/train_accuracy 0.70107
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           ptl/train_loss 0.70107
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         ptl/val_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             ptl/val_loss 0.67917
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                     step 352
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       time_since_restore 636.83932
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:         time_this_iter_s 77.69433
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:             time_total_s 636.83932
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:                timestamp 1696432890
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:               train_loss 0.78747
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_023f1534_8_batch_size=4,layer_size=32,lr=0.0003_2023-10-05_02-07-43/wandb/offline-run-20231005_021055-023f1534
[2m[36m(_WandbLoggingActor pid=1440710)[0m wandb: Find logs at: ./wandb/offline-run-20231005_021055-023f1534/logs
[2m[36m(TorchTrainer pid=1445660)[0m Starting distributed worker processes: ['1445790 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1445790)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1445790)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1445790)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1445790)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1445790)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1445790)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1445790)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1445790)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ef962b68_9_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_02-10-49/lightning_logs
[2m[36m(RayTrainWorker pid=1445790)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1445790)[0m 
[2m[36m(RayTrainWorker pid=1445790)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1445790)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1445790)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1445790)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1445790)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1445790)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1445790)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1445790)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1445790)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1445790)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1445790)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1445790)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1445790)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1445790)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1445790)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1445790)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1445790)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1445790)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1445790)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ef962b68_9_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_02-10-49/checkpoint_000000)
2023-10-05 02:23:24,570	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.366 s, which may be a performance bottleneck.
2023-10-05 02:23:24,572	WARNING util.py:315 -- The `process_trial_result` operation took 2.369 s, which may be a performance bottleneck.
2023-10-05 02:23:24,572	WARNING util.py:315 -- Processing trial results took 2.370 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:23:24,573	WARNING util.py:315 -- The `process_trial_result` operation took 2.370 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             ptl/val_aupr 0.66708
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:            ptl/val_auroc 0.61029
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             ptl/val_loss 0.71071
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:       time_since_restore 94.99071
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:         time_this_iter_s 94.99071
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:             time_total_s 94.99071
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:                timestamp 1696433002
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:               train_loss 0.77012
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_ef962b68_9_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_02-10-49/wandb/offline-run-20231005_022153-ef962b68
[2m[36m(_WandbLoggingActor pid=1445787)[0m wandb: Find logs at: ./wandb/offline-run-20231005_022153-ef962b68/logs
[2m[36m(TorchTrainer pid=1446993)[0m Starting distributed worker processes: ['1447122 (10.6.8.2)']
[2m[36m(RayTrainWorker pid=1447122)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1447122)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1447122)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1447122)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1447122)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1447122)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1447122)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1447122)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_6a705e4f_10_batch_size=4,layer_size=8,lr=0.0003_2023-10-05_02-21-47/lightning_logs
[2m[36m(RayTrainWorker pid=1447122)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1447122)[0m 
[2m[36m(RayTrainWorker pid=1447122)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1447122)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1447122)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1447122)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1447122)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1447122)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1447122)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1447122)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1447122)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1447122)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1447122)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1447122)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1447122)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1447122)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1447122)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1447122)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1447122)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1447122)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1447122)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_6a705e4f_10_batch_size=4,layer_size=8,lr=0.0003_2023-10-05_02-21-47/checkpoint_000000)
2023-10-05 02:25:16,827	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.058 s, which may be a performance bottleneck.
2023-10-05 02:25:16,829	WARNING util.py:315 -- The `process_trial_result` operation took 4.062 s, which may be a performance bottleneck.
2023-10-05 02:25:16,829	WARNING util.py:315 -- Processing trial results took 4.062 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:25:16,829	WARNING util.py:315 -- The `process_trial_result` operation took 4.062 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         ptl/val_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         ptl/val_f1_score 0.73913
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             ptl/val_loss 0.69108
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:              ptl/val_mcc 0.04597
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:        ptl/val_precision 0.58621
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:       time_since_restore 94.3232
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:         time_this_iter_s 94.3232
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:             time_total_s 94.3232
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:                timestamp 1696433112
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:               train_loss 0.68258
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-28-26/TorchTrainer_6a705e4f_10_batch_size=4,layer_size=8,lr=0.0003_2023-10-05_02-21-47/wandb/offline-run-20231005_022344-6a705e4f
[2m[36m(_WandbLoggingActor pid=1447119)[0m wandb: Find logs at: ./wandb/offline-run-20231005_022344-6a705e4f/logs
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
finetune/fine_tune_tidy.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_loss.append(torch.tensor(loss))
finetune/fine_tune_tidy.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_probs.append(torch.tensor(class_1_probs))
finetune/fine_tune_tidy.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_target.append(torch.tensor(target.int()))
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_loss.append(torch.tensor(loss))
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_probs.append(torch.tensor(class_1_probs))
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.test_target.append(torch.tensor(target.int()))
