Global seed set to 42
2023-11-18 00:41:15,857	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-18 00:41:26,566	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-18 00:41:26,570	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-18 00:41:26,748	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1716697)[0m Starting distributed worker processes: ['1717415 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1717415)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1717415)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1717415)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1717415)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1717415)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:42:12,929	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_54979647
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1716697, ip=10.6.29.14, actor_id=f06b03a5a775e2dae39b837d01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1717415, ip=10.6.29.14, actor_id=f1cacb5a90282a77298e8e2e01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14d2a2b631f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_54979647_1_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0082_2023-11-18_00-41-26/wandb/offline-run-20231118_004153-54979647
[2m[36m(_WandbLoggingActor pid=1717410)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004153-54979647/logs
[2m[36m(TrainTrainable pid=1717807)[0m Trainable.setup took 11.468 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1717807)[0m Starting distributed worker processes: ['1717938 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1717938)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1717938)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1717938)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1717938)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1717938)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:43:12,818	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_95d986ef
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1717807, ip=10.6.29.14, actor_id=c90a0590e58baf0128d6514701000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1717938, ip=10.6.29.14, actor_id=94366edc0b70e34292434f7601000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14b5336e91c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_95d986ef_2_batch_size=4,cell_type=exe_endo,layer_size=8,lr=0.0003_2023-11-18_00-41-44/wandb/offline-run-20231118_004257-95d986ef
[2m[36m(_WandbLoggingActor pid=1717933)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004257-95d986ef/logs
[2m[36m(TorchTrainer pid=1718327)[0m Starting distributed worker processes: ['1718466 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1718466)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1718466)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1718466)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1718466)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1718466)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:43:55,654	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_80022ddf
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1718327, ip=10.6.29.14, actor_id=d6c440d5aeebd328a8170d8301000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1718466, ip=10.6.29.14, actor_id=3352f217b12abc26e5cc1efd01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x15001179a1c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_80022ddf_3_batch_size=4,cell_type=exe_endo,layer_size=8,lr=0.0008_2023-11-18_00-42-39/wandb/offline-run-20231118_004345-80022ddf
[2m[36m(_WandbLoggingActor pid=1718463)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004345-80022ddf/logs
[2m[36m(TrainTrainable pid=1718855)[0m Trainable.setup took 10.477 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1718855)[0m Starting distributed worker processes: ['1718986 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1718986)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1718986)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1718986)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1718986)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1718986)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:44:45,535	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_557a7911
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1718855, ip=10.6.29.14, actor_id=f4b8e049791f7b339246ab0a01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1718986, ip=10.6.29.14, actor_id=5c8e0f1f985043d56659d02201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14d30d69a220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_557a7911_4_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0004_2023-11-18_00-43-37/wandb/offline-run-20231118_004433-557a7911
[2m[36m(_WandbLoggingActor pid=1718981)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004433-557a7911/logs
[2m[36m(TrainTrainable pid=1719377)[0m Trainable.setup took 15.674 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1719377)[0m Starting distributed worker processes: ['1719516 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1719516)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1719516)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1719516)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1719516)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1719516)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:45:53,059	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_db738180
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1719377, ip=10.6.29.14, actor_id=b0e54105df143ceb36ddda1b01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1719516, ip=10.6.29.14, actor_id=017db7bbe29f01687aa9fd6501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x143b91d9c160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_db738180_5_batch_size=8,cell_type=exe_endo,layer_size=32,lr=0.0699_2023-11-18_00-44-19/wandb/offline-run-20231118_004542-db738180
[2m[36m(_WandbLoggingActor pid=1719511)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004542-db738180/logs
[2m[36m(TorchTrainer pid=1719908)[0m Starting distributed worker processes: ['1720039 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1720039)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1720039)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1720039)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1720039)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1720039)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:46:30,861	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_2d25d774
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1719908, ip=10.6.29.14, actor_id=0f7948552a957a3bca04584401000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1720039, ip=10.6.29.14, actor_id=9fc9678185e6705e85a5d41501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x153155d9c190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_2d25d774_6_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0000_2023-11-18_00-45-28/wandb/offline-run-20231118_004621-2d25d774
[2m[36m(_WandbLoggingActor pid=1720035)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004621-2d25d774/logs
[2m[36m(TorchTrainer pid=1720429)[0m Starting distributed worker processes: ['1720559 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1720559)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1720559)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1720559)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1720559)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1720559)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:07,084	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_450efbfa
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1720429, ip=10.6.29.14, actor_id=194719bad40f0f53d00e402201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1720559, ip=10.6.29.14, actor_id=abb16b5fea5f661ddf1a768c01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x144df7e6d220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_450efbfa_7_batch_size=4,cell_type=exe_endo,layer_size=16,lr=0.0203_2023-11-18_00-46-13/wandb/offline-run-20231118_004657-450efbfa
[2m[36m(_WandbLoggingActor pid=1720555)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004657-450efbfa/logs
[2m[36m(TorchTrainer pid=1720948)[0m Starting distributed worker processes: ['1721087 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1721087)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1721087)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1721087)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1721087)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1721087)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:44,411	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_c741da59
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1720948, ip=10.6.29.14, actor_id=fba40928f73afa3c0126cfc201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1721087, ip=10.6.29.14, actor_id=31bf9b16f4b716ddd7e25af901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146f7e4e01f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_c741da59_8_batch_size=4,cell_type=exe_endo,layer_size=8,lr=0.0036_2023-11-18_00-46-49/wandb/offline-run-20231118_004734-c741da59
[2m[36m(_WandbLoggingActor pid=1721084)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004734-c741da59/logs
[2m[36m(TorchTrainer pid=1721477)[0m Starting distributed worker processes: ['1721607 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1721607)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1721607)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1721607)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1721607)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1721607)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:48:18,871	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_d2bdee7c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1721477, ip=10.6.29.14, actor_id=e59535ac7f9721022dbdbac901000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1721607, ip=10.6.29.14, actor_id=4851318d98103ee79aa3122401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14d5968a2100>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_d2bdee7c_9_batch_size=4,cell_type=exe_endo,layer_size=8,lr=0.0055_2023-11-18_00-47-26/wandb/offline-run-20231118_004809-d2bdee7c
[2m[36m(_WandbLoggingActor pid=1721603)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004809-d2bdee7c/logs
[2m[36m(TorchTrainer pid=1721997)[0m Starting distributed worker processes: ['1722128 (10.6.29.14)']
[2m[36m(RayTrainWorker pid=1722128)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1722128)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1722128)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1722128)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1722128)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:48:56,771	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_1c742b35
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1721997, ip=10.6.29.14, actor_id=9f4d4f2f539863d724f44c6e01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1722128, ip=10.6.29.14, actor_id=7d35e05b009c6a30c9ee56a901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x153b0fd6c1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-41-11/TorchTrainer_1c742b35_10_batch_size=4,cell_type=exe_endo,layer_size=32,lr=0.0210_2023-11-18_00-48-01/wandb/offline-run-20231118_004846-1c742b35
[2m[36m(_WandbLoggingActor pid=1722123)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004846-1c742b35/logs
2023-11-18 00:49:02,742	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_54979647, TorchTrainer_95d986ef, TorchTrainer_80022ddf, TorchTrainer_557a7911, TorchTrainer_db738180, TorchTrainer_2d25d774, TorchTrainer_450efbfa, TorchTrainer_c741da59, TorchTrainer_d2bdee7c, TorchTrainer_1c742b35]
2023-11-18 00:49:02,792	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 368, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
