Global seed set to 42
2023-11-15 01:04:03,855	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 01:04:13,135	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 01:04:13,140	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 01:04:13,184	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=700644)[0m Starting distributed worker processes: ['701363 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=701363)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=701363)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=701363)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=701363)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=701363)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=701363)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=701363)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=701363)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=701363)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/lightning_logs
[2m[36m(RayTrainWorker pid=701363)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=701363)[0m 
[2m[36m(RayTrainWorker pid=701363)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=701363)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=701363)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=701363)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=701363)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=701363)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=701363)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=701363)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=701363)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=701363)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=701363)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=701363)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=701363)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=701363)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=701363)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=701363)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=701363)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=701363)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=701363)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 01:15:13,488	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000000)
2023-11-15 01:15:16,088	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.599 s, which may be a performance bottleneck.
2023-11-15 01:15:16,089	WARNING util.py:315 -- The `process_trial_result` operation took 2.601 s, which may be a performance bottleneck.
2023-11-15 01:15:16,089	WARNING util.py:315 -- Processing trial results took 2.601 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:15:16,090	WARNING util.py:315 -- The `process_trial_result` operation took 2.602 s, which may be a performance bottleneck.
2023-11-15 01:25:24,810	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000001)
2023-11-15 01:35:37,184	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000002)
2023-11-15 01:45:48,167	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000003)
2023-11-15 01:55:59,024	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000004)
2023-11-15 02:06:10,078	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000005)
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000006)
2023-11-15 02:16:21,715	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 02:26:32,922	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000007)
2023-11-15 02:36:43,918	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000008)
2023-11-15 02:46:54,918	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000009)
2023-11-15 02:57:05,801	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000010)
2023-11-15 03:07:16,262	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000011)
2023-11-15 03:17:27,153	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000012)
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000013)
2023-11-15 03:27:39,672	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 03:37:51,210	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000014)
2023-11-15 03:48:01,908	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000015)
2023-11-15 03:58:12,950	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000016)
2023-11-15 04:08:23,440	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000017)
2023-11-15 04:18:34,501	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000018)
[2m[36m(RayTrainWorker pid=701363)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       ptl/train_accuracy ▆▄▁▅▆▅▆█▆▇▇▄▄▂▇█▁▂▂
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:           ptl/train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         ptl/val_accuracy █▁▂▃▃▃▆▅▇▅▁▆▃▁▇▁▁▁▁▆
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             ptl/val_aupr █▁▁▂▁▁▃▂▃▂▁▂▁▂▄▁▁▁▁▃
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:            ptl/val_auroc █▁▂▂▂▂▄▃▄▃▁▃▂▃▅▁▁▁▁▄
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         ptl/val_f1_score █▁▇▇▇▇████▇█▇▁█▇▁▁▁█
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             ptl/val_loss █▂▂▂▂▂▁▂▁▁▂▂▂▂▁▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:              ptl/val_mcc █▁▄▄▄▄▇▆█▆▂▆▄▁█▁▁▁▁▇
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:        ptl/val_precision █▁▇▇▇▇▇▇█▇▇▇▇▁█▇▁▁▁█
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:           ptl/val_recall ▇▁███████████▁██▁▁▁█
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         time_this_iter_s █▁▂▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▂
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       ptl/train_accuracy 0.5107
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:           ptl/train_loss 0.69303
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         ptl/val_accuracy 0.63095
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             ptl/val_aupr 0.57295
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:            ptl/val_auroc 0.62818
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         ptl/val_f1_score 0.72072
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             ptl/val_loss 0.68436
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:              ptl/val_mcc 0.34823
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:        ptl/val_precision 0.57416
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:           ptl/val_recall 0.96774
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                     step 3740
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       time_since_restore 12250.30664
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:         time_this_iter_s 612.07993
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:             time_total_s 12250.30664
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:                timestamp 1699982926
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_7aae5a6c_1_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0043_2023-11-15_01-04-13/wandb/offline-run-20231115_010437-7aae5a6c
[2m[36m(_WandbLoggingActor pid=701358)[0m wandb: Find logs at: ./wandb/offline-run-20231115_010437-7aae5a6c/logs
[2m[36m(TrainTrainable pid=716064)[0m Trainable.setup took 13.623 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=716064)[0m Starting distributed worker processes: ['716196 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=716196)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=716196)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=716196)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=716196)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=716196)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=716196)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=716196)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=716196)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=716196)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_94c3ae51_2_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0044_2023-11-15_01-04-29/lightning_logs
[2m[36m(RayTrainWorker pid=716196)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=716196)[0m 
[2m[36m(RayTrainWorker pid=716196)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=716196)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=716196)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=716196)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=716196)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=716196)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=716196)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=716196)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=716196)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=716196)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=716196)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=716196)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=716196)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=716196)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=716196)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=716196)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=716196)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=716196)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=716196)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 04:40:06,627	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=716196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_94c3ae51_2_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0044_2023-11-15_01-04-29/checkpoint_000000)
2023-11-15 04:40:12,056	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.428 s, which may be a performance bottleneck.
2023-11-15 04:40:12,057	WARNING util.py:315 -- The `process_trial_result` operation took 5.432 s, which may be a performance bottleneck.
2023-11-15 04:40:12,057	WARNING util.py:315 -- Processing trial results took 5.432 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:40:12,057	WARNING util.py:315 -- The `process_trial_result` operation took 5.432 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=716196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_94c3ae51_2_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0044_2023-11-15_01-04-29/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       ptl/train_accuracy 0.51136
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:           ptl/train_loss 2.96361
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         ptl/val_accuracy 0.49603
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             ptl/val_loss 0.69363
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                     step 374
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       time_since_restore 1244.77399
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:         time_this_iter_s 604.69319
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:             time_total_s 1244.77399
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:                timestamp 1699984216
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_94c3ae51_2_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0044_2023-11-15_01-04-29/wandb/offline-run-20231115_042936-94c3ae51
[2m[36m(_WandbLoggingActor pid=716192)[0m wandb: Find logs at: ./wandb/offline-run-20231115_042936-94c3ae51/logs
[2m[36m(TorchTrainer pid=718046)[0m Starting distributed worker processes: ['718176 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=718176)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=718176)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=718176)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=718176)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=718176)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=718176)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=718176)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=718176)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=718176)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/lightning_logs
[2m[36m(RayTrainWorker pid=718176)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=718176)[0m 
[2m[36m(RayTrainWorker pid=718176)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=718176)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=718176)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=718176)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=718176)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=718176)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=718176)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=718176)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=718176)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=718176)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=718176)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=718176)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=718176)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=718176)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=718176)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=718176)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=718176)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=718176)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=718176)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 05:01:08,958	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000000)
2023-11-15 05:01:11,803	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.844 s, which may be a performance bottleneck.
2023-11-15 05:01:11,804	WARNING util.py:315 -- The `process_trial_result` operation took 2.848 s, which may be a performance bottleneck.
2023-11-15 05:01:11,804	WARNING util.py:315 -- Processing trial results took 2.849 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 05:01:11,805	WARNING util.py:315 -- The `process_trial_result` operation took 2.849 s, which may be a performance bottleneck.
2023-11-15 05:11:20,089	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000001)
2023-11-15 05:21:30,585	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000002)
2023-11-15 05:31:41,302	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000003)
2023-11-15 05:41:51,602	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000004)
2023-11-15 05:52:01,813	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000005)
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000006)
2023-11-15 06:02:13,457	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000007)
2023-11-15 06:12:24,024	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 06:22:34,665	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000008)
2023-11-15 06:32:45,117	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000009)
2023-11-15 06:42:55,627	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000010)
2023-11-15 06:53:06,672	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000011)
2023-11-15 07:03:17,043	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000012)
2023-11-15 07:13:31,494	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000013)
2023-11-15 07:23:42,406	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000014)
2023-11-15 07:33:52,796	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000015)
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000016)
2023-11-15 07:44:03,756	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 07:54:14,933	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000017)
2023-11-15 08:04:25,569	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000018)
[2m[36m(RayTrainWorker pid=718176)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       ptl/train_accuracy ▁▅▇▇▅▅█▄▇▅▅▇▂▄█▅▅█▅
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:           ptl/train_loss █▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             ptl/val_aupr █▁█▆▂▅▆▅▅▅▆▂▄▅▃▅▂▅▂▅
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:            ptl/val_auroc █▁█▇▃▆▇▆▇▆█▃▅▆▅▆▃▆▃▇
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             ptl/val_loss ██▅▅▆▄▃▄▃▃▂▄▃▂▃▂▃▂▃▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         time_this_iter_s █▁▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       ptl/train_accuracy 0.5107
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:           ptl/train_loss 0.62739
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         ptl/val_accuracy 0.49603
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             ptl/val_aupr 0.6524
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:            ptl/val_auroc 0.71877
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             ptl/val_loss 0.64555
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                     step 3740
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       time_since_restore 12236.33555
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:         time_this_iter_s 611.00964
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:             time_total_s 12236.33555
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:                timestamp 1699996476
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_2a792d93_3_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-29-26/wandb/offline-run-20231115_045041-2a792d93
[2m[36m(_WandbLoggingActor pid=718171)[0m wandb: Find logs at: ./wandb/offline-run-20231115_045041-2a792d93/logs
[2m[36m(TorchTrainer pid=730797)[0m Starting distributed worker processes: ['730929 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=730929)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=730929)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=730929)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=730929)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=730929)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=730929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=730929)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=730929)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=730929)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_8e96d2aa_4_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-50-33/lightning_logs
[2m[36m(RayTrainWorker pid=730929)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=730929)[0m 
[2m[36m(RayTrainWorker pid=730929)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=730929)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=730929)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=730929)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=730929)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=730929)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=730929)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=730929)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=730929)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=730929)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=730929)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=730929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=730929)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=730929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=730929)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=730929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=730929)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=730929)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=730929)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=730929)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_8e96d2aa_4_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-50-33/checkpoint_000000)
2023-11-15 08:25:55,892	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.747 s, which may be a performance bottleneck.
2023-11-15 08:25:55,894	WARNING util.py:315 -- The `process_trial_result` operation took 4.750 s, which may be a performance bottleneck.
2023-11-15 08:25:55,894	WARNING util.py:315 -- Processing trial results took 4.751 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 08:25:55,894	WARNING util.py:315 -- The `process_trial_result` operation took 4.751 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         ptl/val_accuracy 0.5
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             ptl/val_loss 0.70761
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                     step 374
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:       time_since_restore 647.33286
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:         time_this_iter_s 647.33286
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:             time_total_s 647.33286
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:                timestamp 1699997151
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_8e96d2aa_4_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_04-50-33/wandb/offline-run-20231115_081510-8e96d2aa
[2m[36m(_WandbLoggingActor pid=730926)[0m wandb: Find logs at: ./wandb/offline-run-20231115_081510-8e96d2aa/logs
[2m[36m(TorchTrainer pid=732459)[0m Starting distributed worker processes: ['732588 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=732588)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=732588)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=732588)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=732588)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=732588)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=732588)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=732588)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=732588)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=732588)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_0d72f13e_5_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0016_2023-11-15_08-15-03/lightning_logs
[2m[36m(RayTrainWorker pid=732588)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=732588)[0m 
[2m[36m(RayTrainWorker pid=732588)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=732588)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=732588)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=732588)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=732588)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=732588)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=732588)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=732588)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=732588)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=732588)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=732588)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=732588)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=732588)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=732588)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=732588)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=732588)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=732588)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=732588)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=732588)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 08:36:54,685	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=732588)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_0d72f13e_5_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0016_2023-11-15_08-15-03/checkpoint_000000)
2023-11-15 08:36:57,243	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.558 s, which may be a performance bottleneck.
2023-11-15 08:36:57,244	WARNING util.py:315 -- The `process_trial_result` operation took 2.562 s, which may be a performance bottleneck.
2023-11-15 08:36:57,245	WARNING util.py:315 -- Processing trial results took 2.562 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 08:36:57,245	WARNING util.py:315 -- The `process_trial_result` operation took 2.562 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=732588)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_0d72f13e_5_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0016_2023-11-15_08-15-03/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             ptl/val_aupr █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:            ptl/val_auroc █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:        ptl/val_precision █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       ptl/train_accuracy 0.62233
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:           ptl/train_loss 1.16659
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         ptl/val_accuracy 0.5
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             ptl/val_loss 0.69363
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                     step 748
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       time_since_restore 1265.15701
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:         time_this_iter_s 620.82516
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:             time_total_s 1265.15701
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:                timestamp 1699998438
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_0d72f13e_5_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0016_2023-11-15_08-15-03/wandb/offline-run-20231115_082617-0d72f13e
[2m[36m(_WandbLoggingActor pid=732584)[0m wandb: Find logs at: ./wandb/offline-run-20231115_082617-0d72f13e/logs
[2m[36m(TorchTrainer pid=734447)[0m Starting distributed worker processes: ['734574 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=734574)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=734574)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=734574)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=734574)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=734574)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=734574)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=734574)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=734574)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=734574)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_13025bfa_6_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0905_2023-11-15_08-26-10/lightning_logs
[2m[36m(RayTrainWorker pid=734574)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=734574)[0m 
[2m[36m(RayTrainWorker pid=734574)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=734574)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=734574)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=734574)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=734574)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=734574)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=734574)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=734574)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=734574)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=734574)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=734574)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=734574)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=734574)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=734574)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=734574)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=734574)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=734574)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=734574)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=734574)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=734574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_13025bfa_6_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0905_2023-11-15_08-26-10/checkpoint_000000)
2023-11-15 08:58:08,443	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.354 s, which may be a performance bottleneck.
2023-11-15 08:58:08,445	WARNING util.py:315 -- The `process_trial_result` operation took 3.358 s, which may be a performance bottleneck.
2023-11-15 08:58:08,445	WARNING util.py:315 -- Processing trial results took 3.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 08:58:08,445	WARNING util.py:315 -- The `process_trial_result` operation took 3.358 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         ptl/val_accuracy 0.49603
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             ptl/val_loss 0.70275
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                     step 187
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:       time_since_restore 633.62437
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:         time_this_iter_s 633.62437
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:             time_total_s 633.62437
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:                timestamp 1699999085
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_13025bfa_6_batch_size=8,cell_type=somitic_meso,layer_size=8,lr=0.0905_2023-11-15_08-26-10/wandb/offline-run-20231115_084738-13025bfa
[2m[36m(_WandbLoggingActor pid=734571)[0m wandb: Find logs at: ./wandb/offline-run-20231115_084738-13025bfa/logs
[2m[36m(TorchTrainer pid=736097)[0m Starting distributed worker processes: ['736235 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=736235)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=736235)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=736235)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=736235)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=736235)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=736235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=736235)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=736235)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=736235)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/lightning_logs
[2m[36m(RayTrainWorker pid=736235)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=736235)[0m 
[2m[36m(RayTrainWorker pid=736235)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=736235)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=736235)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=736235)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=736235)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=736235)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=736235)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=736235)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=736235)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=736235)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=736235)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=736235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=736235)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=736235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=736235)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=736235)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=736235)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=736235)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=736235)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 09:09:08,631	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000000)
2023-11-15 09:09:11,229	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.597 s, which may be a performance bottleneck.
2023-11-15 09:09:11,230	WARNING util.py:315 -- The `process_trial_result` operation took 2.601 s, which may be a performance bottleneck.
2023-11-15 09:09:11,231	WARNING util.py:315 -- Processing trial results took 2.601 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 09:09:11,231	WARNING util.py:315 -- The `process_trial_result` operation took 2.601 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000001)
2023-11-15 09:19:31,920	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 09:29:55,271	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000002)
2023-11-15 09:40:18,975	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000003)
2023-11-15 09:50:42,034	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000004)
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000005)
2023-11-15 10:01:05,864	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000006)
2023-11-15 10:11:36,228	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 10:21:59,865	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000007)
2023-11-15 10:32:24,098	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000008)
2023-11-15 10:42:47,959	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000009)
2023-11-15 10:53:11,592	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000010)
2023-11-15 11:03:35,096	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000011)
2023-11-15 11:14:00,414	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000012)
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000013)
2023-11-15 11:24:25,761	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 11:34:49,144	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000014)
[2m[36m(RayTrainWorker pid=736235)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/checkpoint_000015)
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                    epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       ptl/train_accuracy ▁▃▄▅▆▆▆▆▇▇▇▇█▇█
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:           ptl/train_loss █▆▅▅▄▃▃▃▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         ptl/val_accuracy ▁▅▇▇▅▆▄▇▄▃▄▆▇▆█▃
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             ptl/val_aupr ▁▃▄▅▆▆▇▇▇▇██████
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:            ptl/val_auroc ▁▃▄▅▆▆▇▇▇▇██████
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         ptl/val_f1_score ▁▄▃▅▆▆▄█▅▄▅▇█▇█▄
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             ptl/val_loss ▅▃▃▂▃▂▄▂▃▇▅▂▂▃▁█
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:              ptl/val_mcc ▁▅▆▇▆▆▄▇▅▄▅▇▇▆█▄
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:        ptl/val_precision ▁▄██▄▅▃▆▃▂▃▅▅▄█▂
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:           ptl/val_recall ▇▅▁▂▆▆▇▅▇█▇▇▆▇▄█
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                     step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         time_this_iter_s █▁▂▂▂▂▄▂▂▂▂▂▂▂▂▁
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                    epoch 15
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: iterations_since_restore 16
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       ptl/train_accuracy 0.76537
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:           ptl/train_loss 0.49677
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         ptl/val_accuracy 0.634
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             ptl/val_aupr 0.83267
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:            ptl/val_auroc 0.84171
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         ptl/val_f1_score 0.72564
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             ptl/val_loss 0.78529
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:              ptl/val_mcc 0.36652
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:        ptl/val_precision 0.57757
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:           ptl/val_recall 0.97581
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                     step 5984
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       time_since_restore 10000.90343
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:         time_this_iter_s 621.74241
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:             time_total_s 10000.90343
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:                timestamp 1700009111
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb:       training_iteration 16
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_82ea028b_7_batch_size=4,cell_type=somitic_meso,layer_size=16,lr=0.0000_2023-11-15_08-47-31/wandb/offline-run-20231115_085832-82ea028b
[2m[36m(_WandbLoggingActor pid=736232)[0m wandb: Find logs at: ./wandb/offline-run-20231115_085832-82ea028b/logs
[2m[36m(TrainTrainable pid=746327)[0m Trainable.setup took 54.518 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=746327)[0m Starting distributed worker processes: ['746476 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=746476)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=746476)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=746476)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=746476)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=746476)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=746476)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=746476)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=746476)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=746476)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_26027d1c_8_batch_size=4,cell_type=somitic_meso,layer_size=8,lr=0.0012_2023-11-15_08-58-25/lightning_logs
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=746476)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=746476)[0m 
[2m[36m(RayTrainWorker pid=746476)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=746476)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=746476)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=746476)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=746476)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=746476)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=746476)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=746476)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=746476)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=746476)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=746476)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=746476)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=746476)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=746476)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=746476)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=746476)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=746476)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=746476)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=746476)[0m   
[2m[36m(RayTrainWorker pid=746476)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_26027d1c_8_batch_size=4,cell_type=somitic_meso,layer_size=8,lr=0.0012_2023-11-15_08-58-25/checkpoint_000000)
2023-11-15 11:59:12,327	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 11:59:18,678	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 6.350 s, which may be a performance bottleneck.
2023-11-15 11:59:18,679	WARNING util.py:315 -- The `process_trial_result` operation took 6.354 s, which may be a performance bottleneck.
2023-11-15 11:59:18,680	WARNING util.py:315 -- Processing trial results took 6.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 11:59:18,680	WARNING util.py:315 -- The `process_trial_result` operation took 6.355 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=746476)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_26027d1c_8_batch_size=4,cell_type=somitic_meso,layer_size=8,lr=0.0012_2023-11-15_08-58-25/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             ptl/val_aupr █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:            ptl/val_auroc █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:        ptl/val_precision █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       ptl/train_accuracy 0.62901
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:           ptl/train_loss 1.18782
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         ptl/val_accuracy 0.5
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             ptl/val_loss 0.70127
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                     step 748
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       time_since_restore 1310.11195
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:         time_this_iter_s 615.28477
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:             time_total_s 1310.11195
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:                timestamp 1700010573
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_26027d1c_8_batch_size=4,cell_type=somitic_meso,layer_size=8,lr=0.0012_2023-11-15_08-58-25/wandb/offline-run-20231115_114811-26027d1c
[2m[36m(_WandbLoggingActor pid=746473)[0m wandb: Find logs at: ./wandb/offline-run-20231115_114811-26027d1c/logs
[2m[36m(TorchTrainer pid=748516)[0m Starting distributed worker processes: ['748654 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=748654)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=748654)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=748654)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=748654)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=748654)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=748654)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=748654)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=748654)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=748654)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_be386490_9_batch_size=8,cell_type=somitic_meso,layer_size=32,lr=0.0018_2023-11-15_11-47-37/lightning_logs
[2m[36m(RayTrainWorker pid=748654)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=748654)[0m 
[2m[36m(RayTrainWorker pid=748654)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=748654)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=748654)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=748654)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=748654)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=748654)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=748654)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=748654)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=748654)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=748654)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=748654)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=748654)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=748654)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=748654)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=748654)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=748654)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=748654)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=748654)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=748654)[0m   
[2m[36m(RayTrainWorker pid=748654)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_be386490_9_batch_size=8,cell_type=somitic_meso,layer_size=32,lr=0.0018_2023-11-15_11-47-37/checkpoint_000000)
2023-11-15 12:20:23,741	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.859 s, which may be a performance bottleneck.
2023-11-15 12:20:23,743	WARNING util.py:315 -- The `process_trial_result` operation took 2.862 s, which may be a performance bottleneck.
2023-11-15 12:20:23,743	WARNING util.py:315 -- Processing trial results took 2.863 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 12:20:23,744	WARNING util.py:315 -- The `process_trial_result` operation took 2.863 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         ptl/val_accuracy 0.49603
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             ptl/val_loss 0.7018
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                     step 187
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:       time_since_restore 632.01016
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:         time_this_iter_s 632.01016
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:             time_total_s 632.01016
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:                timestamp 1700011220
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_be386490_9_batch_size=8,cell_type=somitic_meso,layer_size=32,lr=0.0018_2023-11-15_11-47-37/wandb/offline-run-20231115_120955-be386490
[2m[36m(_WandbLoggingActor pid=748650)[0m wandb: Find logs at: ./wandb/offline-run-20231115_120955-be386490/logs
[2m[36m(TorchTrainer pid=750171)[0m Starting distributed worker processes: ['750301 (10.6.10.19)']
[2m[36m(RayTrainWorker pid=750301)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=750301)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=750301)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=750301)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=750301)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=750301)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=750301)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=750301)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=750301)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/lightning_logs
[2m[36m(RayTrainWorker pid=750301)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=750301)[0m 
[2m[36m(RayTrainWorker pid=750301)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=750301)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=750301)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=750301)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=750301)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=750301)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=750301)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=750301)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=750301)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=750301)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=750301)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=750301)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=750301)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=750301)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=750301)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=750301)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=750301)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=750301)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=750301)[0m   
2023-11-15 12:31:15,953	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=750301)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/checkpoint_000000)
2023-11-15 12:31:19,287	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.334 s, which may be a performance bottleneck.
2023-11-15 12:31:19,289	WARNING util.py:315 -- The `process_trial_result` operation took 3.338 s, which may be a performance bottleneck.
2023-11-15 12:31:19,289	WARNING util.py:315 -- Processing trial results took 3.338 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 12:31:19,289	WARNING util.py:315 -- The `process_trial_result` operation took 3.338 s, which may be a performance bottleneck.
2023-11-15 12:41:25,525	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=750301)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/checkpoint_000001)
2023-11-15 12:51:35,497	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=750301)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/checkpoint_000002)
[2m[36m(RayTrainWorker pid=750301)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       ptl/train_accuracy █▆▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         ptl/val_accuracy ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             ptl/val_aupr ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:            ptl/val_auroc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         ptl/val_f1_score ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             ptl/val_loss █▁██
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:              ptl/val_mcc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:        ptl/val_precision ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:           ptl/val_recall ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       ptl/train_accuracy 0.47883
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:           ptl/train_loss 0.69348
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         ptl/val_accuracy 0.49603
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             ptl/val_aupr 0.49799
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             ptl/val_loss 0.69772
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:              ptl/val_mcc 0.00036
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                     step 748
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       time_since_restore 2463.10607
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:         time_this_iter_s 609.46968
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:             time_total_s 2463.10607
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:                timestamp 1700013704
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-03-59/TorchTrainer_591b70c9_10_batch_size=8,cell_type=somitic_meso,layer_size=16,lr=0.0009_2023-11-15_12-09-48/wandb/offline-run-20231115_122046-591b70c9
[2m[36m(_WandbLoggingActor pid=750298)[0m wandb: Find logs at: ./wandb/offline-run-20231115_122046-591b70c9/logs
