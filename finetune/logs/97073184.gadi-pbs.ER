Global seed set to 42
2023-10-04 23:58:29,217	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-04 23:58:34,621	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-04 23:58:34,624	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 23:58:34,662	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=2766283)[0m Starting distributed worker processes: ['2767010 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2767010)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2767010)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2767010)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2767010)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2767010)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2767010)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2767010)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2767010)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2767010)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/lightning_logs
[2m[36m(RayTrainWorker pid=2767010)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2767010)[0m 
[2m[36m(RayTrainWorker pid=2767010)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2767010)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2767010)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2767010)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2767010)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2767010)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2767010)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2767010)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2767010)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2767010)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2767010)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2767010)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2767010)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2767010)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2767010)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2767010)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2767010)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2767010)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2767010)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2767010)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2767010)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2767010)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2767010)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2767010)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2767010)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2767010)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 00:04:46,012	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000000)
2023-10-05 00:04:48,767	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.754 s, which may be a performance bottleneck.
2023-10-05 00:04:48,768	WARNING util.py:315 -- The `process_trial_result` operation took 2.757 s, which may be a performance bottleneck.
2023-10-05 00:04:48,768	WARNING util.py:315 -- Processing trial results took 2.757 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:04:48,768	WARNING util.py:315 -- The `process_trial_result` operation took 2.757 s, which may be a performance bottleneck.
2023-10-05 00:10:05,684	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000001)
2023-10-05 00:15:24,869	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000002)
2023-10-05 00:20:43,703	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000003)
2023-10-05 00:26:02,084	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000004)
2023-10-05 00:31:20,522	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000005)
2023-10-05 00:36:39,247	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000006)
2023-10-05 00:41:57,988	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000007)
2023-10-05 00:47:16,824	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000008)
2023-10-05 00:52:35,639	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000009)
2023-10-05 00:57:54,465	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000010)
2023-10-05 01:03:12,742	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000011)
2023-10-05 01:08:32,533	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000012)
2023-10-05 01:13:50,970	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000013)
2023-10-05 01:19:09,510	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000014)
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000015)
2023-10-05 01:24:28,153	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 01:29:46,857	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000016)
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000017)
2023-10-05 01:35:06,642	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 01:40:25,395	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2767010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         ptl/val_accuracy ▁▁▁▁██▁█▁▁▁█▁▁█▁▁███
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         ptl/val_f1_score ▁▁▁▁██▁█▁▁▁█▁▁█▁▁███
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             ptl/val_loss ▄▂▂▂▁▂█▁▁▁▂▂▂▃▁▃▁▄▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:              ptl/val_mcc ▁▁▁▁██▁█▁▁▁█▁▁█▁▁███
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:        ptl/val_precision ▁▁▁▁██▁█▁▁▁█▁▁█▁▁███
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           ptl/val_recall ▁▁▁▁██▁█▁▁▁█▁▁█▁▁███
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         time_this_iter_s █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           train_accuracy ▁██▁▁▁██▁▁█████████▁
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:               train_loss █▃▄▆▆▆▁▄▅▆▃▃▃▃▄▃▄▂▄▆
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       ptl/train_accuracy 0.70295
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           ptl/train_loss 0.70295
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         ptl/val_accuracy 0.50781
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         ptl/val_f1_score 0.67188
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             ptl/val_loss 0.69348
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:              ptl/val_mcc 0.00147
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:        ptl/val_precision 0.50588
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                     step 3840
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       time_since_restore 6405.76141
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:         time_this_iter_s 317.80639
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:             time_total_s 6405.76141
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:                timestamp 1696430743
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:               train_loss 0.75759
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_bff27b89_1_batch_size=4,layer_size=32,lr=0.0538_2023-10-04_23-58-34/wandb/offline-run-20231004_235857-bff27b89
[2m[36m(_WandbLoggingActor pid=2767005)[0m wandb: Find logs at: ./wandb/offline-run-20231004_235857-bff27b89/logs
[2m[36m(TorchTrainer pid=2781548)[0m Starting distributed worker processes: ['2781682 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2781682)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2781682)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2781682)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2781682)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2781682)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2781682)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2781682)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2781682)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2781682)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/lightning_logs
[2m[36m(RayTrainWorker pid=2781682)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2781682)[0m 
[2m[36m(RayTrainWorker pid=2781682)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2781682)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2781682)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2781682)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2781682)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2781682)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2781682)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2781682)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2781682)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2781682)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2781682)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2781682)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2781682)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2781682)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2781682)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2781682)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2781682)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2781682)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2781682)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2781682)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2781682)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2781682)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2781682)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2781682)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2781682)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2781682)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2781682)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2781682)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:51:51,102	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000000)
2023-10-05 01:51:53,708	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.605 s, which may be a performance bottleneck.
2023-10-05 01:51:53,709	WARNING util.py:315 -- The `process_trial_result` operation took 2.610 s, which may be a performance bottleneck.
2023-10-05 01:51:53,709	WARNING util.py:315 -- Processing trial results took 2.610 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:51:53,709	WARNING util.py:315 -- The `process_trial_result` operation took 2.610 s, which may be a performance bottleneck.
2023-10-05 01:57:09,146	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000001)
2023-10-05 02:02:27,524	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000002)
2023-10-05 02:07:46,131	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000003)
2023-10-05 02:13:04,693	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000004)
2023-10-05 02:13:05,227	WARNING util.py:315 -- The `process_trial_save` operation took 0.502 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000005)
2023-10-05 02:18:23,202	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 02:23:41,519	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000006)
[2m[36m(RayTrainWorker pid=2781682)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/checkpoint_000007)
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                    epoch ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         ptl/val_accuracy ▁▁▁▁██▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         ptl/val_f1_score ▁▁▁▁██▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             ptl/val_loss ▃▄▂▃▁▁█▃
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:              ptl/val_mcc ▁▁▁▁██▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:        ptl/val_precision ▁▁▁▁██▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           ptl/val_recall ▁▁▁▁██▁▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                     step ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             time_total_s ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                timestamp ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           train_accuracy ▁██▁▁▁█▁
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:               train_loss ▇▂▄█▆▇▁█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       training_iteration ▁▂▃▄▅▆▇█
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                    epoch 7
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: iterations_since_restore 8
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       ptl/train_accuracy 0.69183
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           ptl/train_loss 0.69183
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         ptl/val_accuracy 0.49219
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             ptl/val_loss 0.69456
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:              ptl/val_mcc -0.00147
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                     step 1536
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       time_since_restore 2564.73512
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:         time_this_iter_s 319.01705
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:             time_total_s 2564.73512
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:                timestamp 1696433340
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:               train_loss 0.73918
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb:       training_iteration 8
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_4e3a59d1_2_batch_size=4,layer_size=32,lr=0.0112_2023-10-04_23-58-50/wandb/offline-run-20231005_014619-4e3a59d1
[2m[36m(_WandbLoggingActor pid=2781679)[0m wandb: Find logs at: ./wandb/offline-run-20231005_014619-4e3a59d1/logs
[2m[36m(TorchTrainer pid=2786968)[0m Starting distributed worker processes: ['2787105 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2787105)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2787105)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2787105)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2787105)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2787105)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2787105)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2787105)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2787105)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2787105)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/lightning_logs
[2m[36m(RayTrainWorker pid=2787105)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2787105)[0m 
[2m[36m(RayTrainWorker pid=2787105)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2787105)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2787105)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2787105)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2787105)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2787105)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2787105)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2787105)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2787105)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2787105)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2787105)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2787105)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2787105)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2787105)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2787105)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2787105)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2787105)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2787105)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2787105)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2787105)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2787105)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2787105)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2787105)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2787105)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2787105)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2787105)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:34:53,533	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000000)
2023-10-05 02:34:56,311	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.777 s, which may be a performance bottleneck.
2023-10-05 02:34:56,312	WARNING util.py:315 -- The `process_trial_result` operation took 2.782 s, which may be a performance bottleneck.
2023-10-05 02:34:56,313	WARNING util.py:315 -- Processing trial results took 2.782 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:34:56,313	WARNING util.py:315 -- The `process_trial_result` operation took 2.783 s, which may be a performance bottleneck.
2023-10-05 02:40:11,556	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000001)
2023-10-05 02:45:29,576	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000002)
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000003)
2023-10-05 02:50:47,781	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 02:56:06,272	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000004)
2023-10-05 03:01:24,891	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000005)
2023-10-05 03:06:43,156	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000006)
2023-10-05 03:12:01,497	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000007)
2023-10-05 03:17:20,156	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000008)
2023-10-05 03:22:38,990	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000009)
2023-10-05 03:27:57,758	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000010)
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000011)
2023-10-05 03:33:16,767	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 03:38:36,590	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000012)
2023-10-05 03:43:54,822	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000013)
2023-10-05 03:49:13,118	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000014)
2023-10-05 03:54:31,460	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000015)
2023-10-05 03:59:49,797	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000016)
2023-10-05 04:05:08,279	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000017)
2023-10-05 04:10:26,835	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2787105)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       ptl/train_accuracy █▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           ptl/train_loss █▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         ptl/val_accuracy ▆▅▇▆▇▇███▅▇▆▇▇▆█▁▇▇█
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             ptl/val_aupr ▁▂▃▄▅▅▆▆▇▇▇▇█▇██████
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:            ptl/val_auroc ▁▂▄▄▅▆▆▇▇▇▇▇████████
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         ptl/val_f1_score ▅▁▆▃▇▇▇▇█▅▇▆▇▇▆█▂▇▇█
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             ptl/val_loss ▂▂▂▂▂▂▂▂▁▃▂▃▂▂▃▁█▂▂▁
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:              ptl/val_mcc ▅▅▆▆▇▇▇▇█▆▇▆▇▇▆█▁▇▇█
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:        ptl/val_precision ▅█▆█▆▆▆▆▆▄▆▄▅▅▄▇▁▆▅▇
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           ptl/val_recall ▆▁▅▂▆▇▇▇▇█▇█▇▇█▆█▇█▇
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           train_accuracy ███▁█████▁██████████
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:               train_loss ▃▂▂▇▂▂▁▇▂█▄▁▂▁▂▁▂▃▁▁
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       ptl/train_accuracy 0.31849
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           ptl/train_loss 0.31849
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         ptl/val_accuracy 0.84766
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             ptl/val_aupr 0.91814
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:            ptl/val_auroc 0.92187
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         ptl/val_f1_score 0.85818
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             ptl/val_loss 0.41092
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:              ptl/val_mcc 0.69986
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:        ptl/val_precision 0.80822
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           ptl/val_recall 0.91473
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                     step 3840
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       time_since_restore 6381.09293
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:         time_this_iter_s 318.45505
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:             time_total_s 6381.09293
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:                timestamp 1696439745
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:               train_loss 0.04816
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_015a866e_3_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_01-46-12/wandb/offline-run-20231005_022924-015a866e
[2m[36m(_WandbLoggingActor pid=2787102)[0m wandb: Find logs at: ./wandb/offline-run-20231005_022924-015a866e/logs
[2m[36m(TorchTrainer pid=2799512)[0m Starting distributed worker processes: ['2799642 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2799642)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2799642)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2799642)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2799642)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2799642)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2799642)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2799642)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2799642)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2799642)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/lightning_logs
[2m[36m(RayTrainWorker pid=2799642)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2799642)[0m 
[2m[36m(RayTrainWorker pid=2799642)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2799642)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2799642)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2799642)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2799642)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2799642)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2799642)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2799642)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2799642)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2799642)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2799642)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2799642)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2799642)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2799642)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2799642)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2799642)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2799642)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2799642)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2799642)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2799642)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2799642)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2799642)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2799642)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2799642)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2799642)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2799642)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2799642)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2799642)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 04:21:49,128	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000000)
2023-10-05 04:21:51,993	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.864 s, which may be a performance bottleneck.
2023-10-05 04:21:51,994	WARNING util.py:315 -- The `process_trial_result` operation took 2.868 s, which may be a performance bottleneck.
2023-10-05 04:21:51,995	WARNING util.py:315 -- Processing trial results took 2.868 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 04:21:51,995	WARNING util.py:315 -- The `process_trial_result` operation took 2.868 s, which may be a performance bottleneck.
2023-10-05 04:26:59,991	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000001)
2023-10-05 04:32:11,043	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000002)
2023-10-05 04:37:22,090	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000003)
2023-10-05 04:42:33,221	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000004)
2023-10-05 04:47:44,385	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000005)
2023-10-05 04:52:55,436	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000006)
2023-10-05 04:58:06,536	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000007)
2023-10-05 05:03:17,914	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000008)
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000009)
2023-10-05 05:08:29,073	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 05:13:40,086	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000010)
2023-10-05 05:18:51,226	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000011)
2023-10-05 05:24:02,795	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000012)
2023-10-05 05:29:14,247	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000013)
2023-10-05 05:34:25,192	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000014)
2023-10-05 05:39:36,766	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000015)
2023-10-05 05:44:48,222	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000016)
2023-10-05 05:49:59,549	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000017)
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000018)
2023-10-05 05:55:10,939	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2799642)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       ptl/train_accuracy █▅▅▅▃▄▃▂▃▂▂▂▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           ptl/train_loss █▅▅▅▃▄▃▂▃▂▂▂▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         ptl/val_accuracy ▆▇▆▇▆▇█▇█▅▇▄█▇▄█▁▇▆▇
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             ptl/val_aupr ▁▃▄▄▆▆▆▇▇▇▇▇████████
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:            ptl/val_auroc ▁▃▄▅▆▆▇▇▇███████████
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         ptl/val_f1_score ▄▆▁▇▆▇▇▇▇▅▇▄█▇▄█▁▇▇▇
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             ptl/val_loss ▂▂▂▂▃▂▁▃▁▄▃▄▂▃▅▁█▂▃▃
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:              ptl/val_mcc ▅▇▆▇▆▇█▇█▆▇▅█▇▅█▁▇▇▇
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:        ptl/val_precision ▆▆█▆▅▅▇▅▇▃▅▃▆▅▃▇▁▅▄▅
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           ptl/val_recall ▃▅▁▅▇▇▅▇▅█▇█▆██▅█▇█▇
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           train_accuracy ▅████▅█▁██████▅███▅█
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:               train_loss ▆▄▁▂▃▅▁█▂▄▃▃▄▂▄▁▁▁▃▂
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       ptl/train_accuracy 0.30305
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           ptl/train_loss 0.30305
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         ptl/val_accuracy 0.79688
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             ptl/val_aupr 0.91699
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:            ptl/val_auroc 0.91977
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         ptl/val_f1_score 0.8255
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             ptl/val_loss 0.52635
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:              ptl/val_mcc 0.62225
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:        ptl/val_precision 0.72781
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           ptl/val_recall 0.95349
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                     step 1920
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       time_since_restore 6240.60369
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:         time_this_iter_s 311.10283
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:             time_total_s 6240.60369
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:                timestamp 1696446022
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:               train_loss 0.12483
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_94cd0b1d_4_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_02-29-17/wandb/offline-run-20231005_041622-94cd0b1d
[2m[36m(_WandbLoggingActor pid=2799639)[0m wandb: Find logs at: ./wandb/offline-run-20231005_041622-94cd0b1d/logs
[2m[36m(TrainTrainable pid=2811819)[0m Trainable.setup took 13.466 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=2811819)[0m Starting distributed worker processes: ['2811956 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2811956)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2811956)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2811956)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2811956)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2811956)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2811956)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2811956)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2811956)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2811956)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8851caad_5_batch_size=8,layer_size=32,lr=0.0006_2023-10-05_04-16-14/lightning_logs
[2m[36m(RayTrainWorker pid=2811956)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2811956)[0m 
[2m[36m(RayTrainWorker pid=2811956)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2811956)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2811956)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2811956)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2811956)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2811956)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2811956)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2811956)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2811956)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2811956)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2811956)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2811956)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2811956)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2811956)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2811956)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2811956)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2811956)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2811956)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2811956)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2811956)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2811956)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2811956)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2811956)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2811956)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2811956)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2811956)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2811956)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2811956)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 06:06:34,787	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2811956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8851caad_5_batch_size=8,layer_size=32,lr=0.0006_2023-10-05_04-16-14/checkpoint_000000)
2023-10-05 06:06:37,342	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.554 s, which may be a performance bottleneck.
2023-10-05 06:06:37,344	WARNING util.py:315 -- The `process_trial_result` operation took 2.558 s, which may be a performance bottleneck.
2023-10-05 06:06:37,344	WARNING util.py:315 -- Processing trial results took 2.559 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:06:37,344	WARNING util.py:315 -- The `process_trial_result` operation took 2.559 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=2811956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8851caad_5_batch_size=8,layer_size=32,lr=0.0006_2023-10-05_04-16-14/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             ptl/val_aupr ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           train_accuracy █▁
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:               train_loss ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       ptl/train_accuracy 0.85686
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           ptl/train_loss 0.85686
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         ptl/val_accuracy 0.70592
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             ptl/val_aupr 0.88981
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:            ptl/val_auroc 0.90144
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         ptl/val_f1_score 0.6114
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             ptl/val_loss 0.91985
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:              ptl/val_mcc 0.48164
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:        ptl/val_precision 0.92188
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           ptl/val_recall 0.45736
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       time_since_restore 643.96541
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:         time_this_iter_s 308.99181
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:             time_total_s 643.96541
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:                timestamp 1696446706
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:           train_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:               train_loss 1.72578
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8851caad_5_batch_size=8,layer_size=32,lr=0.0006_2023-10-05_04-16-14/wandb/offline-run-20231005_060106-8851caad
[2m[36m(_WandbLoggingActor pid=2811953)[0m wandb: Find logs at: ./wandb/offline-run-20231005_060106-8851caad/logs
[2m[36m(TorchTrainer pid=2813785)[0m Starting distributed worker processes: ['2813916 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2813916)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2813916)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2813916)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2813916)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2813916)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2813916)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2813916)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2813916)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2813916)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_5cd6fda8_6_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_06-00-59/lightning_logs
[2m[36m(RayTrainWorker pid=2813916)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2813916)[0m 
[2m[36m(RayTrainWorker pid=2813916)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2813916)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2813916)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2813916)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2813916)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2813916)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2813916)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2813916)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2813916)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2813916)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2813916)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2813916)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2813916)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813916)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813916)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813916)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2813916)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813916)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2813916)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2813916)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2813916)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2813916)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2813916)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2813916)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2813916)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813916)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813916)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813916)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813916)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_5cd6fda8_6_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_06-00-59/checkpoint_000000)
2023-10-05 06:17:37,278	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.718 s, which may be a performance bottleneck.
2023-10-05 06:17:37,280	WARNING util.py:315 -- The `process_trial_result` operation took 2.721 s, which may be a performance bottleneck.
2023-10-05 06:17:37,280	WARNING util.py:315 -- Processing trial results took 2.722 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:17:37,280	WARNING util.py:315 -- The `process_trial_result` operation took 2.722 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         ptl/val_accuracy 0.49219
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             ptl/val_loss 0.71623
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:              ptl/val_mcc -0.00147
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                     step 96
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:       time_since_restore 332.28779
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:         time_this_iter_s 332.28779
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:             time_total_s 332.28779
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:                timestamp 1696447054
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:               train_loss 0.75319
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_5cd6fda8_6_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_06-00-59/wandb/offline-run-20231005_061209-5cd6fda8
[2m[36m(_WandbLoggingActor pid=2813913)[0m wandb: Find logs at: ./wandb/offline-run-20231005_061209-5cd6fda8/logs
[2m[36m(TorchTrainer pid=2815429)[0m Starting distributed worker processes: ['2815559 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2815559)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2815559)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2815559)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2815559)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2815559)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2815559)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2815559)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2815559)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2815559)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_946f7947_7_batch_size=4,layer_size=8,lr=0.0070_2023-10-05_06-12-02/lightning_logs
[2m[36m(RayTrainWorker pid=2815559)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2815559)[0m 
[2m[36m(RayTrainWorker pid=2815559)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2815559)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2815559)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2815559)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2815559)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2815559)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2815559)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2815559)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2815559)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2815559)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2815559)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2815559)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2815559)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815559)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815559)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815559)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2815559)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815559)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2815559)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2815559)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2815559)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2815559)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2815559)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2815559)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2815559)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815559)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815559)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815559)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815559)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_946f7947_7_batch_size=4,layer_size=8,lr=0.0070_2023-10-05_06-12-02/checkpoint_000000)
2023-10-05 06:23:32,106	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.585 s, which may be a performance bottleneck.
2023-10-05 06:23:32,108	WARNING util.py:315 -- The `process_trial_result` operation took 2.589 s, which may be a performance bottleneck.
2023-10-05 06:23:32,108	WARNING util.py:315 -- Processing trial results took 2.589 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:23:32,108	WARNING util.py:315 -- The `process_trial_result` operation took 2.589 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         ptl/val_accuracy 0.49219
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             ptl/val_loss 0.6945
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:              ptl/val_mcc -0.00147
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:       time_since_restore 336.09019
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:         time_this_iter_s 336.09019
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:             time_total_s 336.09019
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:                timestamp 1696447409
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:               train_loss 0.73235
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_946f7947_7_batch_size=4,layer_size=8,lr=0.0070_2023-10-05_06-12-02/wandb/offline-run-20231005_061800-946f7947
[2m[36m(_WandbLoggingActor pid=2815556)[0m wandb: Find logs at: ./wandb/offline-run-20231005_061800-946f7947/logs
[2m[36m(TorchTrainer pid=2817074)[0m Starting distributed worker processes: ['2817207 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2817207)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2817207)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2817207)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2817207)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2817207)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2817207)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2817207)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2817207)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2817207)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_a3799591_8_batch_size=8,layer_size=8,lr=0.0173_2023-10-05_06-17-53/lightning_logs
[2m[36m(RayTrainWorker pid=2817207)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2817207)[0m 
[2m[36m(RayTrainWorker pid=2817207)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2817207)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2817207)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2817207)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2817207)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2817207)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2817207)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2817207)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2817207)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2817207)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2817207)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2817207)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2817207)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2817207)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2817207)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2817207)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2817207)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2817207)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2817207)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2817207)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2817207)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2817207)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2817207)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2817207)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2817207)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2817207)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2817207)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2817207)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 06:29:20,308	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2817207)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_a3799591_8_batch_size=8,layer_size=8,lr=0.0173_2023-10-05_06-17-53/checkpoint_000000)
2023-10-05 06:29:22,925	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.617 s, which may be a performance bottleneck.
2023-10-05 06:29:22,926	WARNING util.py:315 -- The `process_trial_result` operation took 2.619 s, which may be a performance bottleneck.
2023-10-05 06:29:22,926	WARNING util.py:315 -- Processing trial results took 2.620 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:29:22,926	WARNING util.py:315 -- The `process_trial_result` operation took 2.620 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=2817207)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_a3799591_8_batch_size=8,layer_size=8,lr=0.0173_2023-10-05_06-17-53/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           train_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:               train_loss ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       ptl/train_accuracy 2.43803
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           ptl/train_loss 2.43803
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         ptl/val_accuracy 0.49219
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             ptl/val_loss 0.69697
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:              ptl/val_mcc -0.00147
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       time_since_restore 640.23838
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:         time_this_iter_s 307.96336
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:             time_total_s 640.23838
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:                timestamp 1696448070
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:               train_loss 0.71018
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_a3799591_8_batch_size=8,layer_size=8,lr=0.0173_2023-10-05_06-17-53/wandb/offline-run-20231005_062355-a3799591
[2m[36m(_WandbLoggingActor pid=2817204)[0m wandb: Find logs at: ./wandb/offline-run-20231005_062355-a3799591/logs
[2m[36m(TorchTrainer pid=2819027)[0m Starting distributed worker processes: ['2819157 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2819157)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2819157)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2819157)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2819157)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2819157)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2819157)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2819157)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2819157)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2819157)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8e7d8a6d_9_batch_size=8,layer_size=8,lr=0.0002_2023-10-05_06-23-48/lightning_logs
[2m[36m(RayTrainWorker pid=2819157)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2819157)[0m 
[2m[36m(RayTrainWorker pid=2819157)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2819157)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2819157)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2819157)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2819157)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2819157)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2819157)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2819157)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2819157)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2819157)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2819157)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2819157)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2819157)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2819157)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2819157)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2819157)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2819157)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2819157)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2819157)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2819157)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2819157)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2819157)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2819157)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2819157)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2819157)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2819157)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2819157)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2819157)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2819157)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8e7d8a6d_9_batch_size=8,layer_size=8,lr=0.0002_2023-10-05_06-23-48/checkpoint_000000)
2023-10-05 06:40:20,650	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.857 s, which may be a performance bottleneck.
2023-10-05 06:40:20,652	WARNING util.py:315 -- The `process_trial_result` operation took 2.860 s, which may be a performance bottleneck.
2023-10-05 06:40:20,652	WARNING util.py:315 -- Processing trial results took 2.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:40:20,652	WARNING util.py:315 -- The `process_trial_result` operation took 2.861 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         ptl/val_accuracy 0.50781
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             ptl/val_aupr 0.50588
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         ptl/val_f1_score 0.67188
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             ptl/val_loss 0.70276
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:              ptl/val_mcc 0.00147
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:        ptl/val_precision 0.50588
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                     step 96
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:       time_since_restore 331.48252
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:         time_this_iter_s 331.48252
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:             time_total_s 331.48252
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:                timestamp 1696448417
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:           train_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:               train_loss 0.67408
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8e7d8a6d_9_batch_size=8,layer_size=8,lr=0.0002_2023-10-05_06-23-48/wandb/offline-run-20231005_063453-8e7d8a6d
[2m[36m(_WandbLoggingActor pid=2819154)[0m wandb: Find logs at: ./wandb/offline-run-20231005_063453-8e7d8a6d/logs
[2m[36m(TorchTrainer pid=2820672)[0m Starting distributed worker processes: ['2820802 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2820802)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2820802)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2820802)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2820802)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2820802)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2820802)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2820802)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2820802)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2820802)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/lightning_logs
[2m[36m(RayTrainWorker pid=2820802)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2820802)[0m 
[2m[36m(RayTrainWorker pid=2820802)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2820802)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2820802)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2820802)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2820802)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2820802)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2820802)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2820802)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2820802)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2820802)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2820802)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2820802)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2820802)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2820802)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2820802)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2820802)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2820802)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2820802)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2820802)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2820802)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2820802)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2820802)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2820802)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2820802)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2820802)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2820802)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2820802)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2820802)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 06:46:13,079	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000000)
2023-10-05 06:46:15,781	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.701 s, which may be a performance bottleneck.
2023-10-05 06:46:15,782	WARNING util.py:315 -- The `process_trial_result` operation took 2.704 s, which may be a performance bottleneck.
2023-10-05 06:46:15,782	WARNING util.py:315 -- Processing trial results took 2.704 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:46:15,782	WARNING util.py:315 -- The `process_trial_result` operation took 2.704 s, which may be a performance bottleneck.
2023-10-05 06:51:31,689	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000001)
2023-10-05 06:56:50,483	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000002)
2023-10-05 07:02:09,299	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000003)
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000004)
2023-10-05 07:07:28,014	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 07:12:46,831	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000005)
2023-10-05 07:18:05,442	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000006)
2023-10-05 07:23:23,492	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000007)
2023-10-05 07:28:41,989	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000008)
2023-10-05 07:34:00,567	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000009)
2023-10-05 07:39:19,362	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000010)
2023-10-05 07:44:37,854	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000011)
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000012)
2023-10-05 07:49:56,992	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 07:55:15,664	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000013)
2023-10-05 08:00:34,747	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000014)
[2m[36m(RayTrainWorker pid=2820802)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/checkpoint_000015)
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                    epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       ptl/train_accuracy █▆▅▅▄▃▃▃▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           ptl/train_loss █▆▅▅▄▃▃▃▂▂▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         ptl/val_accuracy ▁▃▄▅▆▇▅▇▆███▇██▇
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             ptl/val_aupr ▆▇▆▇▅▃▇▂▆██▃▆▁▃█
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:            ptl/val_auroc ▃▄▅▅▅▃▆▂▆▇▇▁▇▃▅█
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         ptl/val_f1_score ▁▃▄▅▆▇▅▇▆▇▇█▇██▇
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             ptl/val_loss █▆▆▅▄▄▄▄▃▂▂▄▂▂▁▁
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:              ptl/val_mcc ▁▂▄▄▅▆▄▇▆▇▇█▇██▇
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:        ptl/val_precision █▇▇▆▅▃▇▂▅▅▅▁▇▃▄▅
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           ptl/val_recall ▁▂▃▄▅▆▄▇▅▆▆█▅▇▇▆
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                     step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           train_accuracy ▁██▁█████▁████▁█
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:               train_loss ▅▃▃▅▂▂▂▄▂█▂▄▁▁▆▁
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                    epoch 15
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: iterations_since_restore 16
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       ptl/train_accuracy 0.52875
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           ptl/train_loss 0.52875
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         ptl/val_accuracy 0.79688
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             ptl/val_aupr 0.83847
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:            ptl/val_auroc 0.87191
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         ptl/val_f1_score 0.78689
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             ptl/val_loss 0.52429
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:              ptl/val_mcc 0.59622
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:        ptl/val_precision 0.83478
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           ptl/val_recall 0.74419
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                     step 3072
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       time_since_restore 5111.04408
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:         time_this_iter_s 318.59394
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:             time_total_s 5111.04408
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:                timestamp 1696453553
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:               train_loss 0.13979
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb:       training_iteration 16
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-58-25/TorchTrainer_8fec330d_10_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_06-34-46/wandb/offline-run-20231005_064043-8fec330d
[2m[36m(_WandbLoggingActor pid=2820799)[0m wandb: Find logs at: ./wandb/offline-run-20231005_064043-8fec330d/logs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
2023-10-05 08:07:20,325	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-05 08:07:25,815	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-05 08:07:25,824	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-05 08:07:25,883	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=2834898)[0m Starting distributed worker processes: ['2835404 (10.6.29.15)']
[2m[36m(RayTrainWorker pid=2835404)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2835404)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2835404)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2835404)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2835404)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2835399)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2835399)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2835399)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2835404)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2835404)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2835404)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/lightning_logs
[2m[36m(RayTrainWorker pid=2835404)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2835404)[0m 
[2m[36m(RayTrainWorker pid=2835404)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2835404)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2835404)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2835404)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2835404)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2835404)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2835404)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2835404)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2835404)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2835404)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2835404)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2835404)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2835404)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2835404)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2835404)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2835404)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2835404)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2835404)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 08:13:17,316	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000000)
2023-10-05 08:13:20,070	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.753 s, which may be a performance bottleneck.
2023-10-05 08:13:20,070	WARNING util.py:315 -- The `process_trial_result` operation took 2.755 s, which may be a performance bottleneck.
2023-10-05 08:13:20,070	WARNING util.py:315 -- Processing trial results took 2.755 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 08:13:20,070	WARNING util.py:315 -- The `process_trial_result` operation took 2.755 s, which may be a performance bottleneck.
2023-10-05 08:18:28,836	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000001)
2023-10-05 08:23:40,638	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000002)
2023-10-05 08:28:52,400	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000003)
2023-10-05 08:34:04,216	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000004)
2023-10-05 08:39:15,864	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000005)
2023-10-05 08:44:27,795	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000006)
2023-10-05 08:49:39,616	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000007)
2023-10-05 08:54:51,155	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000008)
2023-10-05 09:00:03,364	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2835404)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_08-07-15/TorchTrainer_e5d17226_1_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_08-07-25/checkpoint_000009)
2023-10-05 09:05:15,012	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 356, in <module>
    trainer_2.test(model, dataloaders=[test_dataloader_2])
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 742, in test
    return call._call_and_handle_interrupt(
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 785, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 932, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 469, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
