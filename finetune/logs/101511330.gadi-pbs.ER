Global seed set to 42
2023-11-17 06:14:44,399	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-17 06:14:50,606	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-17 06:14:50,609	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-17 06:14:50,693	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=629254)[0m Starting distributed worker processes: ['629977 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=629977)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=629977)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=629977)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=629977)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=629977)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:15:21,889	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_d645a863
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=629254, ip=10.6.8.5, actor_id=68448741d96787a41164a24e01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=629977, ip=10.6.8.5, actor_id=7cc41204f19b847d0041ad2001000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x148c1631f1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_d645a863_1_batch_size=8,cell_type=mid_hindbrain,layer_size=8,lr=0.0954_2023-11-17_06-14-50/wandb/offline-run-20231117_061512-d645a863
[2m[36m(_WandbLoggingActor pid=629972)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061512-d645a863/logs
[2m[36m(TorchTrainer pid=630369)[0m Starting distributed worker processes: ['630505 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=630505)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=630505)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=630505)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=630505)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=630505)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:15:54,575	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_b7115c23
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=630369, ip=10.6.8.5, actor_id=9279cdd06278516e44b5ae3501000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=630505, ip=10.6.8.5, actor_id=a06809de42988206753f9c0401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1504ac5911c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_b7115c23_2_batch_size=8,cell_type=mid_hindbrain,layer_size=8,lr=0.0127_2023-11-17_06-15-05/wandb/offline-run-20231117_061545-b7115c23
[2m[36m(_WandbLoggingActor pid=630502)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061545-b7115c23/logs
[2m[36m(TorchTrainer pid=630898)[0m Starting distributed worker processes: ['631028 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=631028)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=631028)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=631028)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=631028)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=631028)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:16:27,607	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_18f68803
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=630898, ip=10.6.8.5, actor_id=cf82d772f6063126c65e54ce01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=631028, ip=10.6.8.5, actor_id=851c3867d9fc6260753a03f401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146c9aae3190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_18f68803_3_batch_size=4,cell_type=mid_hindbrain,layer_size=8,lr=0.0016_2023-11-17_06-15-38/wandb/offline-run-20231117_061618-18f68803
[2m[36m(_WandbLoggingActor pid=631023)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061618-18f68803/logs
[2m[36m(TorchTrainer pid=631421)[0m Starting distributed worker processes: ['631551 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=631551)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=631551)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=631551)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=631551)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=631551)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:17:01,944	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_a20e860e
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=631421, ip=10.6.8.5, actor_id=ac52d9fa284a7f456775188a01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=631551, ip=10.6.8.5, actor_id=b497fdb5e2806c2b8ec72dc401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14ae2b669160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_a20e860e_4_batch_size=8,cell_type=mid_hindbrain,layer_size=16,lr=0.0059_2023-11-17_06-16-11/wandb/offline-run-20231117_061652-a20e860e
[2m[36m(_WandbLoggingActor pid=631546)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061652-a20e860e/logs
[2m[36m(TorchTrainer pid=631947)[0m Starting distributed worker processes: ['632076 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=632076)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=632076)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=632076)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=632076)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=632076)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:17:33,901	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_d6936796
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=631947, ip=10.6.8.5, actor_id=27ac46cfe9c6c2941e25fd6601000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=632076, ip=10.6.8.5, actor_id=342596ddd3a72e8fc210f55101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14f69b7a9190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_d6936796_5_batch_size=4,cell_type=mid_hindbrain,layer_size=16,lr=0.0042_2023-11-17_06-16-45/wandb/offline-run-20231117_061725-d6936796
[2m[36m(_WandbLoggingActor pid=632073)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061725-d6936796/logs
[2m[36m(TorchTrainer pid=632476)[0m Starting distributed worker processes: ['633268 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=633268)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=633268)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=633268)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=633268)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=633268)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:18:05,669	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_f74e8bed
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=632476, ip=10.6.8.5, actor_id=65089b58c0286a8532f3918501000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=633268, ip=10.6.8.5, actor_id=43ff83ce407cb0f68aece0a401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x151484c54190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_f74e8bed_6_batch_size=8,cell_type=mid_hindbrain,layer_size=16,lr=0.0005_2023-11-17_06-17-18/wandb/offline-run-20231117_061757-f74e8bed
[2m[36m(_WandbLoggingActor pid=633260)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061757-f74e8bed/logs
[2m[36m(TorchTrainer pid=633700)[0m Starting distributed worker processes: ['633830 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=633830)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=633830)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=633830)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=633830)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=633830)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:18:37,234	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_00f101ef
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=633700, ip=10.6.8.5, actor_id=8485f8d5c230ed96da4af0b601000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=633830, ip=10.6.8.5, actor_id=8638a8ac698460a6fcc25a4401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14a4540cf220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_00f101ef_7_batch_size=8,cell_type=mid_hindbrain,layer_size=8,lr=0.0519_2023-11-17_06-17-50/wandb/offline-run-20231117_061828-00f101ef
[2m[36m(_WandbLoggingActor pid=633827)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061828-00f101ef/logs
[2m[36m(TorchTrainer pid=634224)[0m Starting distributed worker processes: ['634353 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=634353)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=634353)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=634353)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=634353)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=634353)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:19:08,335	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_945de0bb
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=634224, ip=10.6.8.5, actor_id=354599c92ad5bbf012f8279c01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=634353, ip=10.6.8.5, actor_id=11786eb0a62cf8be1148255501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1450c8652190>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_945de0bb_8_batch_size=8,cell_type=mid_hindbrain,layer_size=8,lr=0.0449_2023-11-17_06-18-22/wandb/offline-run-20231117_061859-945de0bb
[2m[36m(_WandbLoggingActor pid=634350)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061859-945de0bb/logs
[2m[36m(TorchTrainer pid=634746)[0m Starting distributed worker processes: ['634875 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=634875)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=634875)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=634875)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=634875)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=634875)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:19:40,570	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_89e1d70c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=634746, ip=10.6.8.5, actor_id=529edb0b1e71d3b8c14db1a701000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=634875, ip=10.6.8.5, actor_id=373dd756a75270cbb3e4aa0401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14bebc2501c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_89e1d70c_9_batch_size=4,cell_type=mid_hindbrain,layer_size=32,lr=0.0010_2023-11-17_06-18-53/wandb/offline-run-20231117_061932-89e1d70c
[2m[36m(_WandbLoggingActor pid=634872)[0m wandb: Find logs at: ./wandb/offline-run-20231117_061932-89e1d70c/logs
[2m[36m(TorchTrainer pid=635276)[0m Starting distributed worker processes: ['635406 (10.6.8.5)']
[2m[36m(RayTrainWorker pid=635406)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=635406)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=635406)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=635406)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=635406)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:20:13,552	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_45c08e87
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=635276, ip=10.6.8.5, actor_id=b99d8ae9c30f3f31397472ea01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=635406, ip=10.6.8.5, actor_id=bea15f40865e5b36dd7a190801000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14806d5d81f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_06-14-40/TorchTrainer_45c08e87_10_batch_size=4,cell_type=mid_hindbrain,layer_size=16,lr=0.0038_2023-11-17_06-19-25/wandb/offline-run-20231117_062004-45c08e87
[2m[36m(_WandbLoggingActor pid=635403)[0m wandb: Find logs at: ./wandb/offline-run-20231117_062004-45c08e87/logs
2023-11-17 06:20:19,643	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_d645a863, TorchTrainer_b7115c23, TorchTrainer_18f68803, TorchTrainer_a20e860e, TorchTrainer_d6936796, TorchTrainer_f74e8bed, TorchTrainer_00f101ef, TorchTrainer_945de0bb, TorchTrainer_89e1d70c, TorchTrainer_45c08e87]
2023-11-17 06:20:19,691	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 366, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
