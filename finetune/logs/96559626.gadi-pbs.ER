Global seed set to 42
2023-09-30 13:59:22,125	INFO worker.py:1642 -- Started a local Ray instance.
2023-09-30 13:59:57,923	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-09-30 13:59:57,972	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=2391719)[0m Starting distributed worker processes: ['2393996 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=2393996)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2393996)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2393996)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2393996)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2393996)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2393996)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2393996)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2393996)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2393996)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/lightning_logs
[2m[36m(RayTrainWorker pid=2393996)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2393996)[0m 
[2m[36m(RayTrainWorker pid=2393996)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2393996)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2393996)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2393996)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2393996)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2393996)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2393996)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2393996)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2393996)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2393996)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2393996)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2393996)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2393996)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2393996)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2393996)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2393996)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2393996)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2393996)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2393996)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2393996)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2393996)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2393996)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2393996)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2393996)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2393996)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2393996)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2393996)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2393996)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 14:05:59,826	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000000)
2023-09-30 14:06:02,539	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.713 s, which may be a performance bottleneck.
2023-09-30 14:06:02,540	WARNING util.py:315 -- The `process_trial_result` operation took 2.715 s, which may be a performance bottleneck.
2023-09-30 14:06:02,540	WARNING util.py:315 -- Processing trial results took 2.715 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 14:06:02,540	WARNING util.py:315 -- The `process_trial_result` operation took 2.715 s, which may be a performance bottleneck.
2023-09-30 14:11:24,181	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000001)
2023-09-30 14:16:47,508	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000002)
2023-09-30 14:22:10,357	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000003)
2023-09-30 14:27:33,263	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000004)
2023-09-30 14:32:56,116	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000005)
2023-09-30 14:38:18,963	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000006)
2023-09-30 14:43:42,036	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000007)
2023-09-30 14:49:04,918	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000008)
[2m[36m(RayTrainWorker pid=2393996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/checkpoint_000009)
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                    epoch ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: iterations_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       ptl/train_accuracy █▄▄▄▂▂▃▂▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           ptl/train_loss █▄▄▄▂▂▃▂▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         ptl/val_accuracy ▇▄██▇▄▇▆▄▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             ptl/val_aupr ▁▃▅▅▇▇▇▇██
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:            ptl/val_auroc ▁▄▆▆▇▇████
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         ptl/val_f1_score ▇▁██▇▆▇▇▆▄
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             ptl/val_loss ▁▄▁▁▂▄▂▃▄█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:              ptl/val_mcc ▆▄██▇▅▇▇▅▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:        ptl/val_precision ▆█▅▇▄▃▄▄▃▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           ptl/val_recall ▅▁▇▅▇█████
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                     step ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       time_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         time_this_iter_s █▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             time_total_s ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                timestamp ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           train_accuracy ▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:               train_loss ▄▁▁▇▂▂▁█▄▄
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       training_iteration ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                    epoch 9
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: iterations_since_restore 10
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       ptl/train_accuracy 0.32514
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           ptl/train_loss 0.32514
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         ptl/val_accuracy 0.58203
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             ptl/val_aupr 0.91658
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:            ptl/val_auroc 0.91959
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         ptl/val_f1_score 0.70685
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             ptl/val_loss 1.21034
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:              ptl/val_mcc 0.2871
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:        ptl/val_precision 0.54661
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                     step 1920
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       time_since_restore 3252.91392
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:         time_this_iter_s 322.55942
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:             time_total_s 3252.91392
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:                timestamp 1696049667
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:               train_loss 0.29229
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb:       training_iteration 10
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_dfcc50e7_1_batch_size=4,layer_size=8,lr=0.0000_2023-09-30_13-59-57/wandb/offline-run-20230930_140016-dfcc50e7
[2m[36m(_WandbLoggingActor pid=2393991)[0m wandb: Find logs at: ./wandb/offline-run-20230930_140016-dfcc50e7/logs
[2m[36m(TorchTrainer pid=3000973)[0m Starting distributed worker processes: ['3001103 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3001103)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3001103)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3001103)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3001103)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3001103)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3001103)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3001103)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3001103)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3001103)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_e3a8c624_2_batch_size=8,layer_size=8,lr=0.0035_2023-09-30_14-00-10/lightning_logs
[2m[36m(RayTrainWorker pid=3001103)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3001103)[0m 
[2m[36m(RayTrainWorker pid=3001103)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3001103)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3001103)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3001103)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=3001103)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3001103)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3001103)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=3001103)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3001103)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=3001103)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3001103)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3001103)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3001103)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3001103)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3001103)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3001103)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3001103)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3001103)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3001103)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3001103)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3001103)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3001103)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3001103)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3001103)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3001103)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3001103)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3001103)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3001103)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3001103)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_e3a8c624_2_batch_size=8,layer_size=8,lr=0.0035_2023-09-30_14-00-10/checkpoint_000000)
2023-09-30 15:00:20,962	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.435 s, which may be a performance bottleneck.
2023-09-30 15:00:20,963	WARNING util.py:315 -- The `process_trial_result` operation took 2.438 s, which may be a performance bottleneck.
2023-09-30 15:00:20,964	WARNING util.py:315 -- Processing trial results took 2.439 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:00:20,964	WARNING util.py:315 -- The `process_trial_result` operation took 2.439 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         ptl/val_accuracy 0.76172
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             ptl/val_aupr 0.87093
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:            ptl/val_auroc 0.87985
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         ptl/val_f1_score 0.79868
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             ptl/val_loss 0.64318
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:              ptl/val_mcc 0.55558
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:        ptl/val_precision 0.6954
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:           ptl/val_recall 0.93798
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                     step 96
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:       time_since_restore 335.94273
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:         time_this_iter_s 335.94273
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:             time_total_s 335.94273
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:                timestamp 1696050018
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:               train_loss 0.27938
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_e3a8c624_2_batch_size=8,layer_size=8,lr=0.0035_2023-09-30_14-00-10/wandb/offline-run-20230930_145449-e3a8c624
[2m[36m(_WandbLoggingActor pid=3001100)[0m wandb: Find logs at: ./wandb/offline-run-20230930_145449-e3a8c624/logs
[2m[36m(TorchTrainer pid=3002595)[0m Starting distributed worker processes: ['3002725 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3002725)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3002725)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3002725)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3002725)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3002725)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3002725)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3002725)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3002725)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3002725)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/lightning_logs
[2m[36m(RayTrainWorker pid=3002725)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3002725)[0m 
[2m[36m(RayTrainWorker pid=3002725)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3002725)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3002725)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3002725)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=3002725)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3002725)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3002725)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=3002725)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3002725)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=3002725)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3002725)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3002725)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3002725)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3002725)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3002725)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3002725)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3002725)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3002725)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3002725)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3002725)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3002725)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3002725)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3002725)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3002725)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3002725)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3002725)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3002725)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3002725)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:06:16,829	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3002725)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/checkpoint_000000)
2023-09-30 15:06:19,512	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.682 s, which may be a performance bottleneck.
2023-09-30 15:06:19,514	WARNING util.py:315 -- The `process_trial_result` operation took 2.687 s, which may be a performance bottleneck.
2023-09-30 15:06:19,514	WARNING util.py:315 -- Processing trial results took 2.687 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:06:19,514	WARNING util.py:315 -- The `process_trial_result` operation took 2.688 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=3002725)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/checkpoint_000001)
2023-09-30 15:11:39,634	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-09-30 15:17:02,585	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3002725)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3002725)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       ptl/train_accuracy █▁▃
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           ptl/train_loss █▁▃
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         ptl/val_accuracy ▁▃█▃
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             ptl/val_aupr ▁▆█▇
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:            ptl/val_auroc ▁▆█▇
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         ptl/val_f1_score ▃▁█▅
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             ptl/val_loss ▄▁▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:              ptl/val_mcc ▁▁█▄
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:        ptl/val_precision ▂█▁▁
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           ptl/val_recall ▅▁█▇
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           train_accuracy ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:               train_loss █▁▁▆
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       ptl/train_accuracy 0.52136
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           ptl/train_loss 0.52136
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         ptl/val_accuracy 0.81641
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             ptl/val_aupr 0.89874
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:            ptl/val_auroc 0.90753
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         ptl/val_f1_score 0.83392
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             ptl/val_loss 0.58056
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:              ptl/val_mcc 0.64301
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:        ptl/val_precision 0.76623
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           ptl/val_recall 0.91473
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                     step 768
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       time_since_restore 1306.66289
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:         time_this_iter_s 322.60308
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:             time_total_s 1306.66289
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:                timestamp 1696051345
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:               train_loss 0.14039
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_752ed637_3_batch_size=4,layer_size=16,lr=0.0001_2023-09-30_14-54-42/wandb/offline-run-20230930_150042-752ed637
[2m[36m(_WandbLoggingActor pid=3002722)[0m wandb: Find logs at: ./wandb/offline-run-20230930_150042-752ed637/logs
[2m[36m(TorchTrainer pid=3005623)[0m Starting distributed worker processes: ['3005753 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3005753)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3005753)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3005753)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3005753)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3005753)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3005753)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3005753)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3005753)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3005753)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0ec5de02_4_batch_size=8,layer_size=16,lr=0.0000_2023-09-30_15-00-35/lightning_logs
[2m[36m(RayTrainWorker pid=3005753)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3005753)[0m 
[2m[36m(RayTrainWorker pid=3005753)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3005753)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3005753)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3005753)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=3005753)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3005753)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3005753)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=3005753)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3005753)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=3005753)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3005753)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3005753)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3005753)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3005753)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3005753)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3005753)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3005753)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3005753)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3005753)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3005753)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3005753)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3005753)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3005753)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3005753)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3005753)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3005753)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 15:28:37,751	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3005753)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0ec5de02_4_batch_size=8,layer_size=16,lr=0.0000_2023-09-30_15-00-35/checkpoint_000000)
2023-09-30 15:28:41,706	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.955 s, which may be a performance bottleneck.
2023-09-30 15:28:41,708	WARNING util.py:315 -- The `process_trial_result` operation took 3.959 s, which may be a performance bottleneck.
2023-09-30 15:28:41,708	WARNING util.py:315 -- Processing trial results took 3.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:28:41,709	WARNING util.py:315 -- The `process_trial_result` operation took 3.960 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=3005753)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0ec5de02_4_batch_size=8,layer_size=16,lr=0.0000_2023-09-30_15-00-35/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             ptl/val_aupr ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           train_accuracy █▁
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:               train_loss ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       ptl/train_accuracy 0.71392
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           ptl/train_loss 0.71392
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         ptl/val_accuracy 0.71373
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             ptl/val_aupr 0.88738
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:            ptl/val_auroc 0.89916
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         ptl/val_f1_score 0.62564
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             ptl/val_loss 0.81952
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:              ptl/val_mcc 0.49448
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:        ptl/val_precision 0.92424
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           ptl/val_recall 0.47287
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       time_since_restore 659.33648
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:         time_this_iter_s 311.98258
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:             time_total_s 659.33648
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:                timestamp 1696052033
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:           train_accuracy 0.6
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:               train_loss 1.14837
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0ec5de02_4_batch_size=8,layer_size=16,lr=0.0000_2023-09-30_15-00-35/wandb/offline-run-20230930_152302-0ec5de02
[2m[36m(_WandbLoggingActor pid=3005748)[0m wandb: Find logs at: ./wandb/offline-run-20230930_152302-0ec5de02/logs
[2m[36m(TorchTrainer pid=3007531)[0m Starting distributed worker processes: ['3007670 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3007670)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3007670)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3007670)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3007670)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3007670)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3007670)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3007670)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3007670)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3007670)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0c28ea25_5_batch_size=4,layer_size=8,lr=0.0068_2023-09-30_15-22-50/lightning_logs
[2m[36m(RayTrainWorker pid=3007670)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3007670)[0m 
[2m[36m(RayTrainWorker pid=3007670)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3007670)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3007670)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3007670)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=3007670)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3007670)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3007670)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=3007670)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3007670)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=3007670)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3007670)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3007670)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3007670)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3007670)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3007670)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3007670)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3007670)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3007670)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3007670)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3007670)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3007670)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3007670)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3007670)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3007670)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3007670)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3007670)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3007670)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0c28ea25_5_batch_size=4,layer_size=8,lr=0.0068_2023-09-30_15-22-50/checkpoint_000000)
2023-09-30 15:40:13,626	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.526 s, which may be a performance bottleneck.
2023-09-30 15:40:13,628	WARNING util.py:315 -- The `process_trial_result` operation took 4.529 s, which may be a performance bottleneck.
2023-09-30 15:40:13,628	WARNING util.py:315 -- Processing trial results took 4.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:40:13,628	WARNING util.py:315 -- The `process_trial_result` operation took 4.529 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         ptl/val_accuracy 0.76562
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             ptl/val_aupr 0.87241
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:            ptl/val_auroc 0.88021
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         ptl/val_f1_score 0.80263
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             ptl/val_loss 0.80185
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:              ptl/val_mcc 0.5658
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:        ptl/val_precision 0.69714
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:           ptl/val_recall 0.94574
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:       time_since_restore 351.12184
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:         time_this_iter_s 351.12184
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:             time_total_s 351.12184
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:                timestamp 1696052409
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:               train_loss 0.21507
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_0c28ea25_5_batch_size=4,layer_size=8,lr=0.0068_2023-09-30_15-22-50/wandb/offline-run-20230930_153430-0c28ea25
[2m[36m(_WandbLoggingActor pid=3007665)[0m wandb: Find logs at: ./wandb/offline-run-20230930_153430-0c28ea25/logs
[2m[36m(TorchTrainer pid=3009165)[0m Starting distributed worker processes: ['3009295 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3009295)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3009295)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3009295)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3009295)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3009295)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3009295)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3009295)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3009295)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3009295)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_f1f42f3b_6_batch_size=4,layer_size=8,lr=0.0044_2023-09-30_15-34-18/lightning_logs
[2m[36m(RayTrainWorker pid=3009295)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3009295)[0m 
[2m[36m(RayTrainWorker pid=3009295)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3009295)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3009295)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3009295)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=3009295)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3009295)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3009295)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=3009295)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3009295)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=3009295)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3009295)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3009295)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3009295)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3009295)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3009295)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3009295)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3009295)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3009295)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3009295)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3009295)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3009295)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3009295)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3009295)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3009295)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3009295)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3009295)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3009295)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3009295)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3009295)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_f1f42f3b_6_batch_size=4,layer_size=8,lr=0.0044_2023-09-30_15-34-18/checkpoint_000000)
2023-09-30 15:46:34,887	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.561 s, which may be a performance bottleneck.
2023-09-30 15:46:34,889	WARNING util.py:315 -- The `process_trial_result` operation took 2.564 s, which may be a performance bottleneck.
2023-09-30 15:46:34,889	WARNING util.py:315 -- Processing trial results took 2.565 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:46:34,889	WARNING util.py:315 -- The `process_trial_result` operation took 2.565 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         ptl/val_accuracy 0.78516
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             ptl/val_aupr 0.86573
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:            ptl/val_auroc 0.88095
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         ptl/val_f1_score 0.80969
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             ptl/val_loss 0.63798
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:              ptl/val_mcc 0.58499
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:        ptl/val_precision 0.73125
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:           ptl/val_recall 0.90698
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:       time_since_restore 353.5783
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:         time_this_iter_s 353.5783
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:             time_total_s 353.5783
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:                timestamp 1696052792
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:               train_loss 0.07463
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_f1f42f3b_6_batch_size=4,layer_size=8,lr=0.0044_2023-09-30_15-34-18/wandb/offline-run-20230930_154053-f1f42f3b
[2m[36m(_WandbLoggingActor pid=3009290)[0m wandb: Find logs at: ./wandb/offline-run-20230930_154053-f1f42f3b/logs
[2m[36m(TorchTrainer pid=3010791)[0m Starting distributed worker processes: ['3010920 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3010920)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3010920)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3010920)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3010920)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3010920)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3010920)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3010920)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3010920)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3010920)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_3e98bd5e_7_batch_size=4,layer_size=8,lr=0.0014_2023-09-30_15-40-38/lightning_logs
[2m[36m(RayTrainWorker pid=3010920)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3010920)[0m 
[2m[36m(RayTrainWorker pid=3010920)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3010920)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3010920)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3010920)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=3010920)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3010920)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3010920)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=3010920)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3010920)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=3010920)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3010920)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3010920)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3010920)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3010920)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3010920)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3010920)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3010920)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3010920)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3010920)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3010920)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3010920)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3010920)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3010920)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3010920)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3010920)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3010920)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3010920)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3010920)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3010920)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_3e98bd5e_7_batch_size=4,layer_size=8,lr=0.0014_2023-09-30_15-40-38/checkpoint_000000)
2023-09-30 15:52:32,745	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.513 s, which may be a performance bottleneck.
2023-09-30 15:52:32,747	WARNING util.py:315 -- The `process_trial_result` operation took 2.516 s, which may be a performance bottleneck.
2023-09-30 15:52:32,747	WARNING util.py:315 -- Processing trial results took 2.517 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:52:32,747	WARNING util.py:315 -- The `process_trial_result` operation took 2.517 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         ptl/val_accuracy 0.52734
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             ptl/val_aupr 0.87752
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:            ptl/val_auroc 0.90304
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         ptl/val_f1_score 0.68074
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             ptl/val_loss 3.09334
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:              ptl/val_mcc 0.1431
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:        ptl/val_precision 0.516
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:       time_since_restore 340.77272
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:         time_this_iter_s 340.77272
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:             time_total_s 340.77272
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:                timestamp 1696053150
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:               train_loss 0.0003
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_3e98bd5e_7_batch_size=4,layer_size=8,lr=0.0014_2023-09-30_15-40-38/wandb/offline-run-20230930_154656-3e98bd5e
[2m[36m(_WandbLoggingActor pid=3010917)[0m wandb: Find logs at: ./wandb/offline-run-20230930_154656-3e98bd5e/logs
[2m[36m(TorchTrainer pid=3012414)[0m Starting distributed worker processes: ['3012543 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3012543)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3012543)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3012543)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3012543)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3012543)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3012543)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3012543)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3012543)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3012543)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_86f7fcd4_8_batch_size=8,layer_size=8,lr=0.0097_2023-09-30_15-46-49/lightning_logs
[2m[36m(RayTrainWorker pid=3012543)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3012543)[0m 
[2m[36m(RayTrainWorker pid=3012543)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3012543)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3012543)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3012543)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=3012543)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3012543)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3012543)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=3012543)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3012543)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=3012543)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3012543)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3012543)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3012543)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3012543)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3012543)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3012543)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3012543)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3012543)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3012543)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3012543)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3012543)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3012543)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3012543)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3012543)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3012543)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3012543)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3012543)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3012543)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3012543)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_86f7fcd4_8_batch_size=8,layer_size=8,lr=0.0097_2023-09-30_15-46-49/checkpoint_000000)
2023-09-30 15:58:25,391	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.520 s, which may be a performance bottleneck.
2023-09-30 15:58:25,392	WARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.
2023-09-30 15:58:25,392	WARNING util.py:315 -- Processing trial results took 2.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 15:58:25,392	WARNING util.py:315 -- The `process_trial_result` operation took 2.523 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         ptl/val_accuracy 0.77679
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             ptl/val_aupr 0.81864
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:            ptl/val_auroc 0.84508
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         ptl/val_f1_score 0.78491
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             ptl/val_loss 1.33993
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:              ptl/val_mcc 0.55343
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:        ptl/val_precision 0.76471
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:           ptl/val_recall 0.8062
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                     step 96
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:       time_since_restore 336.4651
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:         time_this_iter_s 336.4651
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:             time_total_s 336.4651
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:                timestamp 1696053502
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:               train_loss 0.08863
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_86f7fcd4_8_batch_size=8,layer_size=8,lr=0.0097_2023-09-30_15-46-49/wandb/offline-run-20230930_155253-86f7fcd4
[2m[36m(_WandbLoggingActor pid=3012540)[0m wandb: Find logs at: ./wandb/offline-run-20230930_155253-86f7fcd4/logs
[2m[36m(TorchTrainer pid=3014037)[0m Starting distributed worker processes: ['3014166 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3014166)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3014166)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3014166)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3014166)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3014166)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3014166)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3014166)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3014166)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3014166)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_93777e5c_9_batch_size=4,layer_size=32,lr=0.0001_2023-09-30_15-52-46/lightning_logs
[2m[36m(RayTrainWorker pid=3014166)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3014166)[0m 
[2m[36m(RayTrainWorker pid=3014166)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3014166)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3014166)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3014166)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=3014166)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3014166)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3014166)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=3014166)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3014166)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=3014166)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3014166)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3014166)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3014166)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3014166)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3014166)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3014166)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3014166)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3014166)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3014166)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3014166)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3014166)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3014166)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3014166)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3014166)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3014166)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3014166)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3014166)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3014166)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3014166)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_93777e5c_9_batch_size=4,layer_size=32,lr=0.0001_2023-09-30_15-52-46/checkpoint_000000)
2023-09-30 16:04:23,559	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.409 s, which may be a performance bottleneck.
2023-09-30 16:04:23,561	WARNING util.py:315 -- The `process_trial_result` operation took 2.413 s, which may be a performance bottleneck.
2023-09-30 16:04:23,561	WARNING util.py:315 -- Processing trial results took 2.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 16:04:23,561	WARNING util.py:315 -- The `process_trial_result` operation took 2.413 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         ptl/val_accuracy 0.78516
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             ptl/val_aupr 0.87807
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:            ptl/val_auroc 0.89129
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         ptl/val_f1_score 0.81229
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             ptl/val_loss 0.69122
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:              ptl/val_mcc 0.58999
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:        ptl/val_precision 0.72561
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:           ptl/val_recall 0.92248
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                     step 192
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:       time_since_restore 341.45664
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:         time_this_iter_s 341.45664
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:             time_total_s 341.45664
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:                timestamp 1696053861
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:               train_loss 0.16473
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_93777e5c_9_batch_size=4,layer_size=32,lr=0.0001_2023-09-30_15-52-46/wandb/offline-run-20230930_155846-93777e5c
[2m[36m(_WandbLoggingActor pid=3014162)[0m wandb: Find logs at: ./wandb/offline-run-20230930_155846-93777e5c/logs
[2m[36m(TorchTrainer pid=3015575)[0m Starting distributed worker processes: ['3015715 (10.6.11.13)']
[2m[36m(RayTrainWorker pid=3015715)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3015715)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3015715)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3015715)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3015715)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3015715)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3015715)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3015715)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3015715)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/lightning_logs
[2m[36m(RayTrainWorker pid=3015715)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3015715)[0m 
[2m[36m(RayTrainWorker pid=3015715)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3015715)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3015715)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3015715)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=3015715)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3015715)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3015715)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=3015715)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3015715)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=3015715)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3015715)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3015715)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3015715)[0m finetune/fine_tune_tidy.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3015715)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3015715)[0m finetune/fine_tune_tidy.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3015715)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=3015715)[0m finetune/fine_tune_tidy.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3015715)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=3015715)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3015715)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3015715)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3015715)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3015715)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=3015715)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=3015715)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3015715)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=3015715)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3015715)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-09-30 16:10:14,124	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3015715)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/checkpoint_000000)
2023-09-30 16:10:16,386	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.262 s, which may be a performance bottleneck.
2023-09-30 16:10:16,387	WARNING util.py:315 -- The `process_trial_result` operation took 2.264 s, which may be a performance bottleneck.
2023-09-30 16:10:16,387	WARNING util.py:315 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-09-30 16:10:16,387	WARNING util.py:315 -- The `process_trial_result` operation took 2.264 s, which may be a performance bottleneck.
2023-09-30 16:15:30,191	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3015715)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/checkpoint_000001)
2023-09-30 16:20:46,365	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3015715)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3015715)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       ptl/train_accuracy █▁▁
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         ptl/val_accuracy ▁███
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             ptl/val_aupr ▁██▄
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:            ptl/val_auroc ▁▂█▇
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         ptl/val_f1_score ▁▆▇█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             ptl/val_loss ▄▂▁█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:              ptl/val_mcc ▁▇██
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:        ptl/val_precision ▁██▇
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           ptl/val_recall █▁▂▅
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           train_accuracy █▁██
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:               train_loss ▃█▃▁
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       ptl/train_accuracy 0.62826
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           ptl/train_loss 0.62826
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         ptl/val_accuracy 0.82366
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             ptl/val_aupr 0.87969
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:            ptl/val_auroc 0.89575
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         ptl/val_f1_score 0.83636
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             ptl/val_loss 0.7828
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:              ptl/val_mcc 0.6523
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:        ptl/val_precision 0.78767
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           ptl/val_recall 0.89147
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                     step 384
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       time_since_restore 1282.51447
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:         time_this_iter_s 316.02302
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:             time_total_s 1282.51447
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:                timestamp 1696055162
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:               train_loss 0.01898
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-09-30_13-59-57/TorchTrainer_c3a7ea30_10_batch_size=8,layer_size=16,lr=0.0065_2023-09-30_15-58-39/wandb/offline-run-20230930_160444-c3a7ea30
[2m[36m(_WandbLoggingActor pid=3015712)[0m wandb: Find logs at: ./wandb/offline-run-20230930_160444-c3a7ea30/logs
