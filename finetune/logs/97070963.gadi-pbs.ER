Global seed set to 42
2023-10-04 23:37:33,779	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-04 23:37:39,297	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-04 23:37:39,299	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 23:37:39,324	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=190391)[0m Starting distributed worker processes: ['191120 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=191120)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=191120)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=191120)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=191120)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=191120)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=191120)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=191120)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=191120)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=191120)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/lightning_logs
[2m[36m(RayTrainWorker pid=191120)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=191120)[0m 
[2m[36m(RayTrainWorker pid=191120)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=191120)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=191120)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=191120)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=191120)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=191120)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=191120)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=191120)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=191120)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=191120)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=191120)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=191120)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=191120)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=191120)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=191120)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=191120)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=191120)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=191120)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=191120)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=191120)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=191120)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=191120)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=191120)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=191120)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=191120)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=191120)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=191120)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-04 23:45:20,261	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000000)
2023-10-04 23:45:22,684	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.422 s, which may be a performance bottleneck.
2023-10-04 23:45:22,686	WARNING util.py:315 -- The `process_trial_result` operation took 2.425 s, which may be a performance bottleneck.
2023-10-04 23:45:22,686	WARNING util.py:315 -- Processing trial results took 2.426 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-04 23:45:22,686	WARNING util.py:315 -- The `process_trial_result` operation took 2.426 s, which may be a performance bottleneck.
2023-10-04 23:52:29,214	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000001)
2023-10-04 23:59:38,411	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000002)
2023-10-05 00:06:47,525	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000003)
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000004)
2023-10-05 00:13:56,808	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:21:05,939	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000005)
2023-10-05 00:28:15,297	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000006)
2023-10-05 00:35:24,163	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000007)
2023-10-05 00:42:33,364	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000008)
2023-10-05 00:49:42,696	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000009)
2023-10-05 00:56:51,455	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000010)
2023-10-05 01:04:00,786	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000011)
2023-10-05 01:11:10,187	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000012)
2023-10-05 01:18:19,144	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000013)
2023-10-05 01:25:27,943	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000014)
2023-10-05 01:32:37,032	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000015)
2023-10-05 01:39:45,785	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000016)
2023-10-05 01:46:54,734	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000017)
2023-10-05 01:54:04,182	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000018)
[2m[36m(RayTrainWorker pid=191120)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       ptl/train_accuracy ▁▅▇██▇█▇▇▇▇▇▆▇▇▆▇▆▆
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           ptl/train_loss ▁▅▇██▇█▇▇▇▇▇▆▇▇▆▇▆▆
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             ptl/val_aupr █▂▇▃▁▁▁▁▁▁▁▂▂▂▂▂▃▂▂▂
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:            ptl/val_auroc █▃▇▄▁▁▁▁▁▁▁▂▁▂▂▃▄▃▃▃
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             ptl/val_loss ▅▁▇███▇▇▇▇▇▆▆▆▆▆▆▅▅▅
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           train_accuracy ▃▃▃▁▆▃▃█▆▆▃▃█▆▃▆▃▆▆▃
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:               train_loss ▆▅▆█▃▆▆▁▃▃▆▆▁▃▆▃▆▃▃▆
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       ptl/train_accuracy 0.69341
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           ptl/train_loss 0.69341
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         ptl/val_accuracy 0.48563
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             ptl/val_aupr 0.45341
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:            ptl/val_auroc 0.37502
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             ptl/val_loss 0.69449
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                     step 5200
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       time_since_restore 8593.7696
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:         time_this_iter_s 429.27583
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:             time_total_s 8593.7696
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:                timestamp 1696431673
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:           train_accuracy 0.33333
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:               train_loss 0.70377
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_0a381e43_1_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-37-39/wandb/offline-run-20231004_233800-0a381e43
[2m[36m(_WandbLoggingActor pid=191115)[0m wandb: Find logs at: ./wandb/offline-run-20231004_233800-0a381e43/logs
[2m[36m(TorchTrainer pid=236707)[0m Starting distributed worker processes: ['237037 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=237037)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=237037)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=237037)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=237037)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=237037)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=237037)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=237037)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=237037)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=237037)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/lightning_logs
[2m[36m(RayTrainWorker pid=237037)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=237037)[0m 
[2m[36m(RayTrainWorker pid=237037)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=237037)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=237037)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=237037)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=237037)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=237037)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=237037)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=237037)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=237037)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=237037)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=237037)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=237037)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=237037)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=237037)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=237037)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=237037)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=237037)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=237037)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=237037)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=237037)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=237037)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=237037)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=237037)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=237037)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=237037)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=237037)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=237037)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:08:48,686	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000000)
2023-10-05 02:08:51,106	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.419 s, which may be a performance bottleneck.
2023-10-05 02:08:51,107	WARNING util.py:315 -- The `process_trial_result` operation took 2.424 s, which may be a performance bottleneck.
2023-10-05 02:08:51,108	WARNING util.py:315 -- Processing trial results took 2.424 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:08:51,108	WARNING util.py:315 -- The `process_trial_result` operation took 2.424 s, which may be a performance bottleneck.
2023-10-05 02:15:49,209	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000001)
2023-10-05 02:22:49,969	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000002)
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000003)
2023-10-05 02:29:50,750	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 02:36:51,231	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000004)
2023-10-05 02:43:52,018	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000005)
2023-10-05 02:50:53,146	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000006)
2023-10-05 02:57:53,861	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000007)
2023-10-05 03:04:54,749	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000008)
2023-10-05 03:11:55,203	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000009)
2023-10-05 03:18:55,699	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000010)
2023-10-05 03:25:56,339	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000011)
2023-10-05 03:32:57,061	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000012)
2023-10-05 03:39:57,765	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000013)
2023-10-05 03:46:59,054	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000014)
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=237037)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/checkpoint_000015)
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                    epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         ptl/val_accuracy █▁▁█████▁███▁▁█▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         ptl/val_f1_score █▁▁█████▁███▁▁█▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             ptl/val_loss ▁▁▄▂▃▁▁▁▅▁▁▁▆█▁▃
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:              ptl/val_mcc █▁▁█████▁███▁▁█▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:        ptl/val_precision █▁▁█████▁███▁▁█▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           ptl/val_recall █▁▁█████▁███▁▁█▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                     step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           train_accuracy ▁▃▁▆▅▅▃▃▆▃▁▅█▅▃▁
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:               train_loss ▇▆█▃▅▅▆▆▃▆█▅▁▅▆▇
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                    epoch 15
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: iterations_since_restore 16
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       ptl/train_accuracy 0.69621
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           ptl/train_loss 0.69621
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         ptl/val_accuracy 0.48011
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             ptl/val_loss 0.6974
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                     step 2080
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       time_since_restore 6745.31339
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:         time_this_iter_s 420.01596
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:             time_total_s 6745.31339
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:                timestamp 1696438439
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:           train_accuracy 0.28571
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:               train_loss 0.72471
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb:       training_iteration 16
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_8905cf25_2_batch_size=8,layer_size=32,lr=0.0281_2023-10-04_23-37-53/wandb/offline-run-20231005_020135-8905cf25
[2m[36m(_WandbLoggingActor pid=237034)[0m wandb: Find logs at: ./wandb/offline-run-20231005_020135-8905cf25/logs
[2m[36m(TrainTrainable pid=269453)[0m Trainable.setup took 13.866 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=269453)[0m Starting distributed worker processes: ['269584 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=269584)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=269584)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=269584)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=269584)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=269584)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=269584)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=269584)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=269584)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=269584)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_26f81496_3_batch_size=4,layer_size=8,lr=0.0308_2023-10-05_02-01-28/lightning_logs
[2m[36m(RayTrainWorker pid=269584)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=269584)[0m 
[2m[36m(RayTrainWorker pid=269584)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=269584)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=269584)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=269584)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=269584)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=269584)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=269584)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=269584)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=269584)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=269584)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=269584)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=269584)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=269584)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=269584)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=269584)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=269584)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=269584)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=269584)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=269584)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=269584)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=269584)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=269584)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=269584)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=269584)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=269584)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=269584)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=269584)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=269584)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=269584)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=269584)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_26f81496_3_batch_size=4,layer_size=8,lr=0.0308_2023-10-05_02-01-28/checkpoint_000000)
2023-10-05 04:02:10,997	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.007 s, which may be a performance bottleneck.
2023-10-05 04:02:10,999	WARNING util.py:315 -- The `process_trial_result` operation took 3.011 s, which may be a performance bottleneck.
2023-10-05 04:02:10,999	WARNING util.py:315 -- Processing trial results took 3.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 04:02:10,999	WARNING util.py:315 -- The `process_trial_result` operation took 3.012 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         ptl/val_accuracy 0.48563
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             ptl/val_loss 0.69663
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                     step 260
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:       time_since_restore 449.75509
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:         time_this_iter_s 449.75509
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:             time_total_s 449.75509
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:                timestamp 1696438927
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:           train_accuracy 0.33333
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:               train_loss 0.71167
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_26f81496_3_batch_size=4,layer_size=8,lr=0.0308_2023-10-05_02-01-28/wandb/offline-run-20231005_035446-26f81496
[2m[36m(_WandbLoggingActor pid=269580)[0m wandb: Find logs at: ./wandb/offline-run-20231005_035446-26f81496/logs
[2m[36m(TorchTrainer pid=271307)[0m Starting distributed worker processes: ['271437 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=271437)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=271437)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=271437)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=271437)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=271437)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=271437)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=271437)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=271437)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=271437)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_42dc7884_4_batch_size=4,layer_size=32,lr=0.0014_2023-10-05_03-54-38/lightning_logs
[2m[36m(RayTrainWorker pid=271437)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=271437)[0m 
[2m[36m(RayTrainWorker pid=271437)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=271437)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=271437)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=271437)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=271437)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=271437)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=271437)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=271437)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=271437)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=271437)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=271437)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=271437)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=271437)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=271437)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=271437)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=271437)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=271437)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=271437)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=271437)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=271437)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=271437)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=271437)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=271437)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=271437)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=271437)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=271437)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=271437)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=271437)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=271437)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=271437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_42dc7884_4_batch_size=4,layer_size=32,lr=0.0014_2023-10-05_03-54-38/checkpoint_000000)
2023-10-05 04:10:01,512	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.014 s, which may be a performance bottleneck.
2023-10-05 04:10:01,513	WARNING util.py:315 -- The `process_trial_result` operation took 3.017 s, which may be a performance bottleneck.
2023-10-05 04:10:01,514	WARNING util.py:315 -- Processing trial results took 3.017 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 04:10:01,514	WARNING util.py:315 -- The `process_trial_result` operation took 3.017 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         ptl/val_accuracy 0.48563
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             ptl/val_loss 0.69714
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                     step 260
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:       time_since_restore 449.9258
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:         time_this_iter_s 449.9258
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:             time_total_s 449.9258
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:                timestamp 1696439398
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:           train_accuracy 0.33333
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:               train_loss 0.7169
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_42dc7884_4_batch_size=4,layer_size=32,lr=0.0014_2023-10-05_03-54-38/wandb/offline-run-20231005_040236-42dc7884
[2m[36m(_WandbLoggingActor pid=271434)[0m wandb: Find logs at: ./wandb/offline-run-20231005_040236-42dc7884/logs
[2m[36m(TorchTrainer pid=272969)[0m Starting distributed worker processes: ['273099 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=273099)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=273099)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=273099)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=273099)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=273099)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=273099)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=273099)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=273099)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=273099)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/lightning_logs
[2m[36m(RayTrainWorker pid=273099)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=273099)[0m 
[2m[36m(RayTrainWorker pid=273099)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=273099)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=273099)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=273099)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=273099)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=273099)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=273099)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=273099)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=273099)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=273099)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=273099)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=273099)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=273099)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=273099)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=273099)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=273099)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=273099)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=273099)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=273099)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=273099)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=273099)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=273099)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=273099)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=273099)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=273099)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=273099)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=273099)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=273099)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=273099)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000000)
2023-10-05 04:17:40,349	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 04:17:43,829	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.480 s, which may be a performance bottleneck.
2023-10-05 04:17:43,830	WARNING util.py:315 -- The `process_trial_result` operation took 3.482 s, which may be a performance bottleneck.
2023-10-05 04:17:43,830	WARNING util.py:315 -- Processing trial results took 3.482 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 04:17:43,831	WARNING util.py:315 -- The `process_trial_result` operation took 3.483 s, which may be a performance bottleneck.
2023-10-05 04:24:39,925	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000001)
2023-10-05 04:31:39,596	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000002)
2023-10-05 04:38:39,163	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000003)
2023-10-05 04:45:38,983	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000004)
2023-10-05 04:52:38,651	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000005)
2023-10-05 04:59:38,442	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000006)
2023-10-05 05:06:37,940	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000007)
2023-10-05 05:13:37,946	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000008)
2023-10-05 05:20:39,132	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000009)
2023-10-05 05:27:38,894	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000010)
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000011)
2023-10-05 05:34:38,606	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 05:41:38,499	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000012)
2023-10-05 05:48:38,342	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000013)
2023-10-05 05:55:38,295	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000014)
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000015)
2023-10-05 06:02:38,045	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 06:09:37,620	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000016)
2023-10-05 06:16:37,288	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000017)
2023-10-05 06:23:37,196	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000018)
[2m[36m(RayTrainWorker pid=273099)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       ptl/train_accuracy █▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           ptl/train_loss █▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             ptl/val_aupr ▅▇█▁███████████▆████
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:            ptl/val_auroc ▅▇█▁███████████▆████
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             ptl/val_loss ▁▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▇▇▇█
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           train_accuracy ▃▆██▆▆▄▄▃▄▃▆▁▄▄██▆▆▆
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:               train_loss █▃▁▁▃▃▅▅▇▅▇▃█▅▅▂▂▄▄▄
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       ptl/train_accuracy 0.69342
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           ptl/train_loss 0.69342
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         ptl/val_accuracy 0.51989
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         ptl/val_f1_score 0.6781
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             ptl/val_loss 0.69263
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:              ptl/val_mcc 0.00279
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:        ptl/val_precision 0.51297
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                     step 2600
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       time_since_restore 8412.94541
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:         time_this_iter_s 420.26357
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:             time_total_s 8412.94541
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:                timestamp 1696447837
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:           train_accuracy 0.57143
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:               train_loss 0.69095
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_b4f25263_5_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_04-02-28/wandb/offline-run-20231005_041025-b4f25263
[2m[36m(_WandbLoggingActor pid=273096)[0m wandb: Find logs at: ./wandb/offline-run-20231005_041025-b4f25263/logs
[2m[36m(TorchTrainer pid=285602)[0m Starting distributed worker processes: ['285735 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=285735)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=285735)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=285735)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=285735)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=285735)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=285735)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=285735)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=285735)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=285735)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/lightning_logs
[2m[36m(RayTrainWorker pid=285735)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=285735)[0m 
[2m[36m(RayTrainWorker pid=285735)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=285735)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=285735)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=285735)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=285735)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=285735)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=285735)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=285735)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=285735)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=285735)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=285735)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=285735)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=285735)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=285735)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=285735)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=285735)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=285735)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=285735)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=285735)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=285735)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=285735)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=285735)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=285735)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=285735)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=285735)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=285735)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=285735)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=285735)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=285735)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 06:38:37,285	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000000)
2023-10-05 06:38:40,024	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.739 s, which may be a performance bottleneck.
2023-10-05 06:38:40,025	WARNING util.py:315 -- The `process_trial_result` operation took 2.741 s, which may be a performance bottleneck.
2023-10-05 06:38:40,025	WARNING util.py:315 -- Processing trial results took 2.742 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 06:38:40,025	WARNING util.py:315 -- The `process_trial_result` operation took 2.742 s, which may be a performance bottleneck.
2023-10-05 06:45:46,489	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000001)
2023-10-05 06:52:55,465	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000002)
2023-10-05 07:00:04,600	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000003)
2023-10-05 07:07:13,646	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000004)
2023-10-05 07:14:23,063	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000005)
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000006)
2023-10-05 07:21:32,130	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 07:28:41,537	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000007)
2023-10-05 07:35:50,734	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000008)
2023-10-05 07:43:01,168	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000009)
2023-10-05 07:50:10,981	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000010)
2023-10-05 07:57:20,072	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000011)
2023-10-05 08:04:29,460	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000012)
2023-10-05 08:11:38,868	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000013)
2023-10-05 08:18:48,350	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000014)
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000015)
2023-10-05 08:25:57,875	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 08:33:07,270	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000016)
2023-10-05 08:40:16,919	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000017)
2023-10-05 08:47:26,221	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000018)
[2m[36m(RayTrainWorker pid=285735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       ptl/train_accuracy █▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           ptl/train_loss █▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         ptl/val_accuracy ▁▅▇▄▇██▆▅█▆▆▇▇▇▇▇█▆█
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             ptl/val_aupr ▅▇▇▃▄▅█▇▄▇▁▁██▃█▇█▄▇
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:            ptl/val_auroc ▂▅▇▁▄▆█▆▄▆▁▂██▄▇▇█▄▇
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         ptl/val_f1_score ▂▅▆▄▆██▆▁▇▄▃▆▅▅▆▆▇▂▇
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             ptl/val_loss █▇▆▇▆▆▅▅▅▅▅▄▂▂▃▂▂▁▃▁
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:              ptl/val_mcc ▁▅▇▄▆██▆▆█▆▆██▇▇▇█▆█
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:        ptl/val_precision ▁▃▆▃▄▆▅▃▇▆▆▆▇█▆▇▇▆▇▆
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           ptl/val_recall █▇▄▇▆▅▇▇▁▅▃▂▃▂▄▄▄▅▂▄
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           train_accuracy ▁▁▁█▁██▁▁█▁█▁█▁█▁▁▁▁
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:               train_loss ███▆█▇▆█▅▅▆▅▆▁█▃▆▅▇▇
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       ptl/train_accuracy 0.55634
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           ptl/train_loss 0.55634
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         ptl/val_accuracy 0.81705
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             ptl/val_aupr 0.89952
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:            ptl/val_auroc 0.90075
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         ptl/val_f1_score 0.81194
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             ptl/val_loss 0.54751
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:              ptl/val_mcc 0.64248
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:        ptl/val_precision 0.86624
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           ptl/val_recall 0.76404
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                     step 5200
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       time_since_restore 8601.85539
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:         time_this_iter_s 429.8833
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:             time_total_s 8601.85539
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:                timestamp 1696456476
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:           train_accuracy 0.66667
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:               train_loss 0.65554
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_9ebce423_6_batch_size=4,layer_size=32,lr=0.0000_2023-10-05_04-10-17/wandb/offline-run-20231005_063114-9ebce423
[2m[36m(_WandbLoggingActor pid=285732)[0m wandb: Find logs at: ./wandb/offline-run-20231005_063114-9ebce423/logs
[2m[36m(TrainTrainable pid=298455)[0m Trainable.setup took 14.881 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=298455)[0m Starting distributed worker processes: ['298586 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=298586)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=298586)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=298586)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=298586)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=298586)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=298586)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=298586)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=298586)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=298586)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_46c75891_7_batch_size=4,layer_size=16,lr=0.0002_2023-10-05_06-31-07/lightning_logs
[2m[36m(RayTrainWorker pid=298586)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=298586)[0m 
[2m[36m(RayTrainWorker pid=298586)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=298586)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=298586)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=298586)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=298586)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=298586)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=298586)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=298586)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=298586)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=298586)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=298586)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=298586)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=298586)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=298586)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=298586)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=298586)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=298586)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=298586)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=298586)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=298586)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=298586)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=298586)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=298586)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=298586)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=298586)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=298586)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=298586)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=298586)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=298586)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=298586)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_46c75891_7_batch_size=4,layer_size=16,lr=0.0002_2023-10-05_06-31-07/checkpoint_000000)
2023-10-05 09:02:50,920	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.952 s, which may be a performance bottleneck.
2023-10-05 09:02:50,922	WARNING util.py:315 -- The `process_trial_result` operation took 2.956 s, which may be a performance bottleneck.
2023-10-05 09:02:50,922	WARNING util.py:315 -- Processing trial results took 2.956 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 09:02:50,922	WARNING util.py:315 -- The `process_trial_result` operation took 2.957 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         ptl/val_accuracy 0.48563
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             ptl/val_loss 0.70966
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                     step 260
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:       time_since_restore 450.21562
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:         time_this_iter_s 450.21562
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:             time_total_s 450.21562
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:                timestamp 1696456967
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:           train_accuracy 0.33333
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:               train_loss 0.75704
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_46c75891_7_batch_size=4,layer_size=16,lr=0.0002_2023-10-05_06-31-07/wandb/offline-run-20231005_085524-46c75891
[2m[36m(_WandbLoggingActor pid=298583)[0m wandb: Find logs at: ./wandb/offline-run-20231005_085524-46c75891/logs
[2m[36m(TorchTrainer pid=300122)[0m Starting distributed worker processes: ['300252 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=300252)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=300252)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=300252)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=300252)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=300252)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=300252)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=300252)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=300252)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=300252)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_a264d34c_8_batch_size=4,layer_size=16,lr=0.0031_2023-10-05_08-55-17/lightning_logs
[2m[36m(RayTrainWorker pid=300252)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=300252)[0m 
[2m[36m(RayTrainWorker pid=300252)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=300252)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=300252)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=300252)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=300252)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=300252)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=300252)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=300252)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=300252)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=300252)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=300252)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=300252)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=300252)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=300252)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=300252)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=300252)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=300252)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=300252)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=300252)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=300252)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=300252)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=300252)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=300252)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=300252)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=300252)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=300252)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=300252)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=300252)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=300252)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 09:10:33,990	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=300252)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_a264d34c_8_batch_size=4,layer_size=16,lr=0.0031_2023-10-05_08-55-17/checkpoint_000000)
2023-10-05 09:10:36,731	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.740 s, which may be a performance bottleneck.
2023-10-05 09:10:36,732	WARNING util.py:315 -- The `process_trial_result` operation took 2.743 s, which may be a performance bottleneck.
2023-10-05 09:10:36,732	WARNING util.py:315 -- Processing trial results took 2.743 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 09:10:36,732	WARNING util.py:315 -- The `process_trial_result` operation took 2.743 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=300252)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_a264d34c_8_batch_size=4,layer_size=16,lr=0.0031_2023-10-05_08-55-17/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           train_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       ptl/train_accuracy 1.52614
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           ptl/train_loss 1.52614
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         ptl/val_accuracy 0.48563
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             ptl/val_loss 0.69318
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                     step 520
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       time_since_restore 872.90762
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:         time_this_iter_s 425.78218
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:             time_total_s 872.90762
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:                timestamp 1696457862
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:           train_accuracy 0.33333
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:               train_loss 0.69375
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_a264d34c_8_batch_size=4,layer_size=16,lr=0.0031_2023-10-05_08-55-17/wandb/offline-run-20231005_090314-a264d34c
[2m[36m(_WandbLoggingActor pid=300249)[0m wandb: Find logs at: ./wandb/offline-run-20231005_090314-a264d34c/logs
[2m[36m(TorchTrainer pid=302108)[0m Starting distributed worker processes: ['302238 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=302238)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=302238)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=302238)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=302238)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=302238)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=302238)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=302238)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=302238)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=302238)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_34de893c_9_batch_size=8,layer_size=16,lr=0.0050_2023-10-05_09-03-06/lightning_logs
[2m[36m(RayTrainWorker pid=302238)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=302238)[0m 
[2m[36m(RayTrainWorker pid=302238)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=302238)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=302238)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=302238)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=302238)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=302238)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=302238)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=302238)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=302238)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=302238)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=302238)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=302238)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=302238)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=302238)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=302238)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=302238)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=302238)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=302238)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=302238)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=302238)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=302238)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=302238)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=302238)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=302238)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=302238)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=302238)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=302238)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=302238)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=302238)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=302238)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_34de893c_9_batch_size=8,layer_size=16,lr=0.0050_2023-10-05_09-03-06/checkpoint_000000)
2023-10-05 09:25:21,097	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.748 s, which may be a performance bottleneck.
2023-10-05 09:25:21,099	WARNING util.py:315 -- The `process_trial_result` operation took 2.752 s, which may be a performance bottleneck.
2023-10-05 09:25:21,099	WARNING util.py:315 -- Processing trial results took 2.752 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 09:25:21,099	WARNING util.py:315 -- The `process_trial_result` operation took 2.752 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         ptl/val_accuracy 0.48011
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             ptl/val_aupr 0.51297
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             ptl/val_loss 0.69433
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                     step 130
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:       time_since_restore 440.66448
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:         time_this_iter_s 440.66448
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:             time_total_s 440.66448
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:                timestamp 1696458318
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:           train_accuracy 0.71429
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:               train_loss 0.68382
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_34de893c_9_batch_size=8,layer_size=16,lr=0.0050_2023-10-05_09-03-06/wandb/offline-run-20231005_091805-34de893c
[2m[36m(_WandbLoggingActor pid=302235)[0m wandb: Find logs at: ./wandb/offline-run-20231005_091805-34de893c/logs
[2m[36m(TorchTrainer pid=303770)[0m Starting distributed worker processes: ['303901 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=303901)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=303901)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=303901)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=303901)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=303901)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=303901)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=303901)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=303901)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=303901)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_c0b5c2bc_10_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_09-17-57/lightning_logs
[2m[36m(RayTrainWorker pid=303901)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=303901)[0m 
[2m[36m(RayTrainWorker pid=303901)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=303901)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=303901)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=303901)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=303901)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=303901)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=303901)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=303901)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=303901)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=303901)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=303901)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=303901)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=303901)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=303901)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=303901)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=303901)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=303901)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=303901)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=303901)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=303901)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=303901)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=303901)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=303901)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=303901)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=303901)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=303901)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=303901)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=303901)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=303901)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=303901)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_c0b5c2bc_10_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_09-17-57/checkpoint_000000)
2023-10-05 09:33:00,226	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.592 s, which may be a performance bottleneck.
2023-10-05 09:33:00,227	WARNING util.py:315 -- The `process_trial_result` operation took 2.595 s, which may be a performance bottleneck.
2023-10-05 09:33:00,227	WARNING util.py:315 -- Processing trial results took 2.595 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 09:33:00,227	WARNING util.py:315 -- The `process_trial_result` operation took 2.595 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         ptl/val_accuracy 0.48011
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             ptl/val_aupr 0.4019
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:            ptl/val_auroc 0.32352
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             ptl/val_loss 0.69649
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:              ptl/val_mcc -0.00279
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                     step 130
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:       time_since_restore 440.71357
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:         time_this_iter_s 440.71357
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:             time_total_s 440.71357
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:                timestamp 1696458777
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:           train_accuracy 0.71429
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:               train_loss 0.67251
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-37-30/TorchTrainer_c0b5c2bc_10_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_09-17-57/wandb/offline-run-20231005_092544-c0b5c2bc
[2m[36m(_WandbLoggingActor pid=303898)[0m wandb: Find logs at: ./wandb/offline-run-20231005_092544-c0b5c2bc/logs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
2023-10-05 09:33:51,012	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-05 09:33:56,595	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-05 09:33:56,613	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-05 09:33:56,656	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=309202)[0m Starting distributed worker processes: ['309925 (10.6.9.16)']
[2m[36m(RayTrainWorker pid=309925)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=309925)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=309925)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=309925)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=309925)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=309920)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=309920)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=309920)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=309925)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=309925)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=309925)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/lightning_logs
[2m[36m(RayTrainWorker pid=309925)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=309925)[0m 
[2m[36m(RayTrainWorker pid=309925)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=309925)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=309925)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=309925)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=309925)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=309925)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=309925)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=309925)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=309925)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=309925)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=309925)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=309925)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=309925)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=309925)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=309925)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=309925)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=309925)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=309925)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=309925)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 09:41:32,802	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000000)
2023-10-05 09:41:35,476	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.673 s, which may be a performance bottleneck.
2023-10-05 09:41:35,477	WARNING util.py:315 -- The `process_trial_result` operation took 2.676 s, which may be a performance bottleneck.
2023-10-05 09:41:35,477	WARNING util.py:315 -- Processing trial results took 2.676 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 09:41:35,478	WARNING util.py:315 -- The `process_trial_result` operation took 2.676 s, which may be a performance bottleneck.
2023-10-05 09:48:33,081	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000001)
2023-10-05 09:55:33,577	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000002)
2023-10-05 10:02:34,067	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000003)
2023-10-05 10:09:34,941	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000004)
2023-10-05 10:16:35,791	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000005)
2023-10-05 10:23:36,293	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=309925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_09-33-46/TorchTrainer_8055d85a_1_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_09-33-56/checkpoint_000006)
2023-10-05 10:30:36,967	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 356, in <module>
    trainer_2.test(model, dataloaders=[test_dataloader_2])
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 742, in test
    return call._call_and_handle_interrupt(
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 785, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 932, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 469, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
