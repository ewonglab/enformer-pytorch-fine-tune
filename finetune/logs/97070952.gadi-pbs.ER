Global seed set to 42
2023-10-04 23:26:57,071	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-04 23:27:03,446	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-04 23:27:03,451	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 23:27:03,501	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=2761304)[0m Starting distributed worker processes: ['2762027 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2762027)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2762027)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2762027)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2762027)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2762027)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2762027)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2762027)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2762027)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2762027)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/lightning_logs
[2m[36m(RayTrainWorker pid=2762027)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2762027)[0m 
[2m[36m(RayTrainWorker pid=2762027)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2762027)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2762027)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2762027)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2762027)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2762027)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2762027)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2762027)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2762027)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2762027)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2762027)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2762027)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2762027)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2762027)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2762027)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2762027)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2762027)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2762027)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2762027)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2762027)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2762027)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2762027)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2762027)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2762027)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2762027)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2762027)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2762027)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2762027)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-04 23:29:22,294	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000000)
2023-10-04 23:29:25,034	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.739 s, which may be a performance bottleneck.
2023-10-04 23:29:25,035	WARNING util.py:315 -- The `process_trial_result` operation took 2.741 s, which may be a performance bottleneck.
2023-10-04 23:29:25,035	WARNING util.py:315 -- Processing trial results took 2.741 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-04 23:29:25,035	WARNING util.py:315 -- The `process_trial_result` operation took 2.741 s, which may be a performance bottleneck.
2023-10-04 23:30:54,751	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000001)
2023-10-04 23:32:28,453	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000002)
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000003)
2023-10-04 23:34:01,013	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-04 23:35:33,290	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000004)
2023-10-04 23:37:05,393	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000005)
2023-10-04 23:38:37,961	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000006)
2023-10-04 23:40:10,174	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000007)
2023-10-04 23:41:42,293	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000008)
2023-10-04 23:43:14,441	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000009)
2023-10-04 23:44:46,578	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000010)
2023-10-04 23:46:18,705	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000011)
2023-10-04 23:47:50,792	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000012)
2023-10-04 23:49:22,930	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000013)
2023-10-04 23:50:55,005	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000014)
2023-10-04 23:52:27,006	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000015)
2023-10-04 23:53:59,009	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000016)
2023-10-04 23:55:31,236	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000017)
2023-10-04 23:57:03,308	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2762027)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             ptl/val_loss ▁▂▃▄▅▅▆▆▆▇▇▆▆▅▇▇▇▇▇█
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         time_this_iter_s █▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           train_accuracy ▃▁▃▃▁▁█▆▃▃▃█▃▃▆▁▆▆██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:               train_loss ▅█▄▄▆▅▁▂▃▃▃▁▃▄▂▄▂▂▁▂
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       ptl/train_accuracy 0.69278
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           ptl/train_loss 0.69278
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             ptl/val_loss 0.69067
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                     step 540
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       time_since_restore 1869.43004
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:         time_this_iter_s 91.80132
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:             time_total_s 1869.43004
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:                timestamp 1696424315
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:           train_accuracy 0.8
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:               train_loss 0.68481
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_c687abd6_1_batch_size=8,layer_size=16,lr=0.0033_2023-10-04_23-27-03/wandb/offline-run-20231004_232726-c687abd6
[2m[36m(_WandbLoggingActor pid=2762022)[0m wandb: Find logs at: ./wandb/offline-run-20231004_232726-c687abd6/logs
[2m[36m(TorchTrainer pid=2775600)[0m Starting distributed worker processes: ['2775732 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2775732)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2775732)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2775732)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2775732)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2775732)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2775732)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2775732)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2775732)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2775732)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_cafda0b4_2_batch_size=4,layer_size=32,lr=0.0007_2023-10-04_23-27-19/lightning_logs
[2m[36m(RayTrainWorker pid=2775732)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2775732)[0m 
[2m[36m(RayTrainWorker pid=2775732)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2775732)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2775732)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2775732)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2775732)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2775732)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2775732)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2775732)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2775732)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2775732)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2775732)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2775732)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2775732)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2775732)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2775732)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2775732)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2775732)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2775732)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2775732)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2775732)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2775732)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2775732)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2775732)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2775732)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2775732)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2775732)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2775732)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2775732)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2775732)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_cafda0b4_2_batch_size=4,layer_size=32,lr=0.0007_2023-10-04_23-27-19/checkpoint_000000)
2023-10-05 00:00:46,285	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.731 s, which may be a performance bottleneck.
2023-10-05 00:00:46,287	WARNING util.py:315 -- The `process_trial_result` operation took 2.736 s, which may be a performance bottleneck.
2023-10-05 00:00:46,287	WARNING util.py:315 -- Processing trial results took 2.737 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:00:46,287	WARNING util.py:315 -- The `process_trial_result` operation took 2.737 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         ptl/val_accuracy 0.41667
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             ptl/val_loss 0.71352
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:              ptl/val_mcc -0.03722
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:       time_since_restore 112.47823
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:         time_this_iter_s 112.47823
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:             time_total_s 112.47823
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:                timestamp 1696424443
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:               train_loss 0.79309
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_cafda0b4_2_batch_size=4,layer_size=32,lr=0.0007_2023-10-04_23-27-19/wandb/offline-run-20231004_235858-cafda0b4
[2m[36m(_WandbLoggingActor pid=2775729)[0m wandb: Find logs at: ./wandb/offline-run-20231004_235858-cafda0b4/logs
[2m[36m(TorchTrainer pid=2777382)[0m Starting distributed worker processes: ['2777521 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2777521)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2777521)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2777521)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2777521)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2777521)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2777521)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2777521)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2777521)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2777521)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8c713f76_3_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-58-51/lightning_logs
[2m[36m(RayTrainWorker pid=2777521)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2777521)[0m 
[2m[36m(RayTrainWorker pid=2777521)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2777521)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2777521)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2777521)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2777521)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2777521)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2777521)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2777521)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2777521)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2777521)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2777521)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2777521)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2777521)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2777521)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2777521)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2777521)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2777521)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2777521)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2777521)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2777521)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2777521)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2777521)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2777521)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2777521)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2777521)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2777521)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2777521)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2777521)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2777521)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8c713f76_3_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-58-51/checkpoint_000000)
2023-10-05 00:02:57,430	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.713 s, which may be a performance bottleneck.
2023-10-05 00:02:57,432	WARNING util.py:315 -- The `process_trial_result` operation took 2.718 s, which may be a performance bottleneck.
2023-10-05 00:02:57,432	WARNING util.py:315 -- Processing trial results took 2.718 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:02:57,432	WARNING util.py:315 -- The `process_trial_result` operation took 2.718 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         ptl/val_accuracy 0.41667
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             ptl/val_aupr 0.57158
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:            ptl/val_auroc 0.4878
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             ptl/val_loss 0.7435
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:              ptl/val_mcc -0.03722
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:       time_since_restore 112.66363
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:         time_this_iter_s 112.66363
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:             time_total_s 112.66363
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:                timestamp 1696424574
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:               train_loss 0.90361
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8c713f76_3_batch_size=4,layer_size=16,lr=0.0000_2023-10-04_23-58-51/wandb/offline-run-20231005_000109-8c713f76
[2m[36m(_WandbLoggingActor pid=2777518)[0m wandb: Find logs at: ./wandb/offline-run-20231005_000109-8c713f76/logs
[2m[36m(TorchTrainer pid=2778992)[0m Starting distributed worker processes: ['2779131 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2779131)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2779131)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2779131)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2779131)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2779131)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2779131)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2779131)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2779131)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2779131)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/lightning_logs
[2m[36m(RayTrainWorker pid=2779131)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2779131)[0m 
[2m[36m(RayTrainWorker pid=2779131)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2779131)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2779131)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2779131)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2779131)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2779131)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2779131)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2779131)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2779131)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2779131)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2779131)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2779131)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2779131)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2779131)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2779131)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2779131)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2779131)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2779131)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2779131)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2779131)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2779131)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2779131)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2779131)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2779131)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2779131)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2779131)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2779131)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2779131)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 00:05:08,454	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000000)
2023-10-05 00:05:11,174	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.719 s, which may be a performance bottleneck.
2023-10-05 00:05:11,175	WARNING util.py:315 -- The `process_trial_result` operation took 2.723 s, which may be a performance bottleneck.
2023-10-05 00:05:11,176	WARNING util.py:315 -- Processing trial results took 2.724 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:05:11,176	WARNING util.py:315 -- The `process_trial_result` operation took 2.724 s, which may be a performance bottleneck.
2023-10-05 00:06:40,856	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000001)
2023-10-05 00:08:13,502	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000002)
2023-10-05 00:09:46,388	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000003)
2023-10-05 00:11:18,983	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000004)
2023-10-05 00:12:51,606	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000005)
2023-10-05 00:14:24,378	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000006)
2023-10-05 00:15:57,148	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000007)
2023-10-05 00:17:30,018	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000008)
2023-10-05 00:19:02,786	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000009)
2023-10-05 00:20:35,349	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000010)
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000011)
2023-10-05 00:22:08,197	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:23:41,111	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000012)
2023-10-05 00:25:13,689	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000013)
2023-10-05 00:26:46,570	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000014)
2023-10-05 00:28:19,425	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000015)
2023-10-05 00:29:51,991	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000016)
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000017)
2023-10-05 00:31:24,609	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 00:32:57,331	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2779131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       ptl/train_accuracy █▆▄▄▄▃▃▃▂▂▂▂▂▃▃▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           ptl/train_loss █▆▄▄▄▃▃▃▂▂▂▂▂▃▃▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         ptl/val_accuracy ▁▆▇▂▂▆▇██▄▄█▇▆█▅▆█▆▃
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             ptl/val_aupr █▇▆▇▇▇▇▇▇▇▇█▃▁▇▇█▇▇█
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:            ptl/val_auroc █▇▆▇▇▇▇▇▇▇▇█▃▁▇▇█▇▇█
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         ptl/val_f1_score ▁▂▆▁▁▆▆▇▇▃▃▇▄▁▇▅▅█▅▃
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             ptl/val_loss █▆▃▃▄▂▂▂▂▂▂▁▂▃▁▂▂▁▂▅
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:              ptl/val_mcc ▁▇▇▃▃▇███▄▅█▇██▆▆█▆▄
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:        ptl/val_precision ▁▆▅▂▂▄▆▅▅▂▃▅▆█▅▃▄▅▄▂
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           ptl/val_recall █▂▅▆▇▆▄▆▅▆▆▅▃▁▆▆▆▆▆█
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           train_accuracy ▃▆▅▅▁██▆█▅█▆▆▆███▆██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:               train_loss ▇▅▆▆█▅▁▄▄▅▂▃▅▅▃▅▂▃▁▂
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       ptl/train_accuracy 0.48123
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           ptl/train_loss 0.48123
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         ptl/val_accuracy 0.65079
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             ptl/val_aupr 0.92676
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:            ptl/val_auroc 0.87805
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         ptl/val_f1_score 0.7619
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             ptl/val_loss 0.56483
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:              ptl/val_mcc 0.29098
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:        ptl/val_precision 0.625
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           ptl/val_recall 0.97561
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                     step 540
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       time_since_restore 1870.49075
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:         time_this_iter_s 92.78162
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:             time_total_s 1870.49075
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:                timestamp 1696426470
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:               train_loss 0.38504
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_d07b2a4a_4_batch_size=8,layer_size=32,lr=0.0000_2023-10-05_00-01-02/wandb/offline-run-20231005_000321-d07b2a4a
[2m[36m(_WandbLoggingActor pid=2779128)[0m wandb: Find logs at: ./wandb/offline-run-20231005_000321-d07b2a4a/logs
[2m[36m(TorchTrainer pid=2790520)[0m Starting distributed worker processes: ['2790650 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2790650)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2790650)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2790650)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2790650)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2790650)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2790650)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2790650)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2790650)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2790650)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/lightning_logs
[2m[36m(RayTrainWorker pid=2790650)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2790650)[0m 
[2m[36m(RayTrainWorker pid=2790650)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2790650)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2790650)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2790650)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2790650)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2790650)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2790650)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2790650)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2790650)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2790650)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2790650)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2790650)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2790650)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2790650)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2790650)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2790650)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2790650)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2790650)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2790650)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2790650)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2790650)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2790650)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2790650)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2790650)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2790650)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2790650)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2790650)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2790650)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 00:36:39,159	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000000)
2023-10-05 00:36:41,986	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.826 s, which may be a performance bottleneck.
2023-10-05 00:36:41,987	WARNING util.py:315 -- The `process_trial_result` operation took 2.830 s, which may be a performance bottleneck.
2023-10-05 00:36:41,987	WARNING util.py:315 -- Processing trial results took 2.831 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 00:36:41,987	WARNING util.py:315 -- The `process_trial_result` operation took 2.831 s, which may be a performance bottleneck.
2023-10-05 00:38:11,008	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000001)
2023-10-05 00:39:43,088	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000002)
2023-10-05 00:41:15,233	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000003)
2023-10-05 00:42:47,350	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000004)
2023-10-05 00:44:19,398	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000005)
2023-10-05 00:45:51,476	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000006)
2023-10-05 00:47:23,780	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000007)
2023-10-05 00:48:55,831	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000008)
2023-10-05 00:50:28,008	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000009)
2023-10-05 00:52:00,084	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000010)
2023-10-05 00:53:32,127	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000011)
2023-10-05 00:55:04,312	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000012)
2023-10-05 00:56:36,478	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000013)
2023-10-05 00:58:08,573	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000014)
2023-10-05 00:59:40,660	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000015)
2023-10-05 01:01:12,749	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000016)
2023-10-05 01:02:44,925	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000017)
2023-10-05 01:04:17,070	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2790650)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       ptl/train_accuracy █▆▅▄▄▃▃▃▃▂▂▂▂▂▃▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           ptl/train_loss █▆▅▄▄▃▃▃▃▂▂▂▂▂▃▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         ptl/val_accuracy ▁█▇▂▃▅█▆▇▃▃▇▇▆▆▅▆█▆▃
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             ptl/val_aupr █▇▇▇▇▇▇▇█▇▇▇▇▁▇▇█▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:            ptl/val_auroc █▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇█▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         ptl/val_f1_score ▁▇▆▁▃▄█▅▇▂▃▇▆▂▅▄▅█▆▄
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             ptl/val_loss █▆▅▄▄▃▃▂▂▃▃▁▂▃▂▂▂▁▁▄
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:              ptl/val_mcc ▁█▇▂▄▅█▆▇▄▄▇▇█▆▅▆█▆▄
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:        ptl/val_precision ▁▆▅▂▂▃▆▄▆▂▂▅▆█▃▃▄▅▄▂
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           ptl/val_recall █▅▅▆█▅▅▅▅▆▆▅▄▁▆▆▆▅▆█
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           train_accuracy ▃▅▅▅▁██▆█▅█▆▆▆▆▆█▆██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:               train_loss ▇▆▆▆█▅▁▄▄▆▃▃▅▅▂▅▂▃▁▂
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       ptl/train_accuracy 0.49971
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           ptl/train_loss 0.49971
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         ptl/val_accuracy 0.65278
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             ptl/val_aupr 0.92199
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:            ptl/val_auroc 0.86992
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         ptl/val_f1_score 0.76636
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             ptl/val_loss 0.55524
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:              ptl/val_mcc 0.32177
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:        ptl/val_precision 0.62121
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                     step 540
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       time_since_restore 1856.0798
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:         time_this_iter_s 91.95768
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:             time_total_s 1856.0798
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:                timestamp 1696428349
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:               train_loss 0.41636
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_f73cb009_5_batch_size=8,layer_size=16,lr=0.0000_2023-10-05_00-03-13/wandb/offline-run-20231005_003453-f73cb009
[2m[36m(_WandbLoggingActor pid=2790647)[0m wandb: Find logs at: ./wandb/offline-run-20231005_003453-f73cb009/logs
[2m[36m(TorchTrainer pid=2802039)[0m Starting distributed worker processes: ['2802169 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2802169)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2802169)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2802169)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2802169)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2802169)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2802169)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2802169)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2802169)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2802169)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/lightning_logs
[2m[36m(RayTrainWorker pid=2802169)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2802169)[0m 
[2m[36m(RayTrainWorker pid=2802169)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2802169)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2802169)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2802169)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2802169)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2802169)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2802169)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2802169)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2802169)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2802169)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2802169)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2802169)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2802169)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2802169)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2802169)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2802169)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2802169)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2802169)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2802169)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2802169)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2802169)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2802169)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2802169)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2802169)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2802169)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2802169)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2802169)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2802169)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:07:58,997	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000000)
2023-10-05 01:08:01,687	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.689 s, which may be a performance bottleneck.
2023-10-05 01:08:01,688	WARNING util.py:315 -- The `process_trial_result` operation took 2.693 s, which may be a performance bottleneck.
2023-10-05 01:08:01,689	WARNING util.py:315 -- Processing trial results took 2.693 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:08:01,689	WARNING util.py:315 -- The `process_trial_result` operation took 2.693 s, which may be a performance bottleneck.
2023-10-05 01:09:30,631	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000001)
2023-10-05 01:11:02,301	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000002)
2023-10-05 01:12:34,010	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000003)
2023-10-05 01:14:05,873	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000004)
2023-10-05 01:15:37,676	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000005)
2023-10-05 01:17:09,424	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000006)
2023-10-05 01:18:41,206	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000007)
2023-10-05 01:20:12,926	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000008)
2023-10-05 01:21:44,604	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000009)
2023-10-05 01:23:16,322	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000010)
2023-10-05 01:24:48,114	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000011)
2023-10-05 01:26:19,953	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000012)
2023-10-05 01:27:51,757	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000013)
2023-10-05 01:29:23,582	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000014)
2023-10-05 01:30:55,404	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000015)
2023-10-05 01:32:27,294	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000016)
2023-10-05 01:33:59,076	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000017)
2023-10-05 01:35:30,844	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2802169)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       ptl/train_accuracy █▆▅▃▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           ptl/train_loss █▆▅▃▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         ptl/val_accuracy ▁▆█▅▅█▆█▇▆▆▇▆▇▇▇██▆▆
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             ptl/val_aupr ▅▅▅▅▄▅▄▆▆▆▇▁▆▁▆▆▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:            ptl/val_auroc ▅▄▄▄▃▄▄▆▅▆▆▁▆▂▅▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         ptl/val_f1_score ▁▂█▄▅█▃▇▆▆▆▄▂▄▇▇▇█▆▆
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             ptl/val_loss █▅▃▃▃▂▂▁▁▂▃▂▂▂▂▂▁▁▃▃
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:              ptl/val_mcc ▁▇█▅▆█▇█▇▆▆▇▇█▇▇██▆▆
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:        ptl/val_precision ▁▇▇▃▄▆▇▇▇▄▄▇██▅▅▆▇▄▄
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           ptl/val_recall █▁▄▆▅▅▁▄▃▆▆▂▁▁▅▅▄▄▆▆
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           train_accuracy ▁▆▃▆▃██▆▆▃█▃▆▆███▆█▆
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:               train_loss █▆▆▆▇▄▂▅▄▅▂▆▅▅▃▄▂▃▁▄
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       ptl/train_accuracy 0.4076
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           ptl/train_loss 0.4076
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         ptl/val_accuracy 0.75992
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             ptl/val_aupr 0.9324
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:            ptl/val_auroc 0.88943
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         ptl/val_f1_score 0.80899
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             ptl/val_loss 0.46962
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:              ptl/val_mcc 0.50459
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:        ptl/val_precision 0.75
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           ptl/val_recall 0.87805
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                     step 540
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       time_since_restore 1850.9437
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:         time_this_iter_s 91.9652
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:             time_total_s 1850.9437
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:                timestamp 1696430222
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:           train_accuracy 0.8
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:               train_loss 0.39756
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_1dc27a67_6_batch_size=8,layer_size=8,lr=0.0001_2023-10-05_00-34-46/wandb/offline-run-20231005_010612-1dc27a67
[2m[36m(_WandbLoggingActor pid=2802166)[0m wandb: Find logs at: ./wandb/offline-run-20231005_010612-1dc27a67/logs
[2m[36m(TorchTrainer pid=2813566)[0m Starting distributed worker processes: ['2813696 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2813696)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2813696)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2813696)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2813696)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2813696)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2813696)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2813696)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2813696)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2813696)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_4af63e97_7_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_01-06-05/lightning_logs
[2m[36m(RayTrainWorker pid=2813696)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2813696)[0m 
[2m[36m(RayTrainWorker pid=2813696)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2813696)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2813696)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2813696)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2813696)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2813696)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2813696)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2813696)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2813696)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2813696)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2813696)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2813696)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2813696)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813696)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813696)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813696)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2813696)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813696)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2813696)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2813696)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2813696)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2813696)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2813696)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2813696)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2813696)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813696)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813696)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2813696)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2813696)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_4af63e97_7_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_01-06-05/checkpoint_000000)
2023-10-05 01:39:15,058	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.802 s, which may be a performance bottleneck.
2023-10-05 01:39:15,058	WARNING util.py:315 -- The `process_trial_result` operation took 2.805 s, which may be a performance bottleneck.
2023-10-05 01:39:15,058	WARNING util.py:315 -- Processing trial results took 2.805 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:39:15,058	WARNING util.py:315 -- The `process_trial_result` operation took 2.805 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             ptl/val_loss 0.68106
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:       time_since_restore 113.73389
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:         time_this_iter_s 113.73389
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:             time_total_s 113.73389
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:                timestamp 1696430352
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:               train_loss 0.72014
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_4af63e97_7_batch_size=8,layer_size=32,lr=0.0005_2023-10-05_01-06-05/wandb/offline-run-20231005_013725-4af63e97
[2m[36m(_WandbLoggingActor pid=2813693)[0m wandb: Find logs at: ./wandb/offline-run-20231005_013725-4af63e97/logs
[2m[36m(TorchTrainer pid=2815157)[0m Starting distributed worker processes: ['2815287 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2815287)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2815287)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2815287)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2815287)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2815287)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2815287)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2815287)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2815287)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2815287)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_5b08d578_8_batch_size=4,layer_size=32,lr=0.0233_2023-10-05_01-37-18/lightning_logs
[2m[36m(RayTrainWorker pid=2815287)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2815287)[0m 
[2m[36m(RayTrainWorker pid=2815287)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2815287)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2815287)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2815287)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2815287)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2815287)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2815287)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2815287)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2815287)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2815287)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2815287)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2815287)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2815287)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815287)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815287)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815287)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2815287)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815287)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2815287)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2815287)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2815287)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2815287)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2815287)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2815287)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2815287)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815287)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815287)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2815287)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2815287)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_5b08d578_8_batch_size=4,layer_size=32,lr=0.0233_2023-10-05_01-37-18/checkpoint_000000)
2023-10-05 01:41:26,733	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.803 s, which may be a performance bottleneck.
2023-10-05 01:41:26,735	WARNING util.py:315 -- The `process_trial_result` operation took 2.806 s, which may be a performance bottleneck.
2023-10-05 01:41:26,735	WARNING util.py:315 -- Processing trial results took 2.807 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:41:26,735	WARNING util.py:315 -- The `process_trial_result` operation took 2.807 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             ptl/val_loss 0.682
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:       time_since_restore 113.17764
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:         time_this_iter_s 113.17764
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:             time_total_s 113.17764
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:                timestamp 1696430483
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:               train_loss 0.60633
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_5b08d578_8_batch_size=4,layer_size=32,lr=0.0233_2023-10-05_01-37-18/wandb/offline-run-20231005_013937-5b08d578
[2m[36m(_WandbLoggingActor pid=2815284)[0m wandb: Find logs at: ./wandb/offline-run-20231005_013937-5b08d578/logs
[2m[36m(TorchTrainer pid=2816759)[0m Starting distributed worker processes: ['2816890 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2816890)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2816890)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2816890)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2816890)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2816890)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2816890)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2816890)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2816890)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2816890)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dbef1d8_9_batch_size=8,layer_size=32,lr=0.0003_2023-10-05_01-39-30/lightning_logs
[2m[36m(RayTrainWorker pid=2816890)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2816890)[0m 
[2m[36m(RayTrainWorker pid=2816890)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2816890)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2816890)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2816890)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=2816890)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2816890)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2816890)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=2816890)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2816890)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=2816890)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2816890)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2816890)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2816890)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2816890)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2816890)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2816890)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2816890)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2816890)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2816890)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2816890)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2816890)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2816890)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2816890)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2816890)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2816890)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2816890)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2816890)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2816890)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2816890)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dbef1d8_9_batch_size=8,layer_size=32,lr=0.0003_2023-10-05_01-39-30/checkpoint_000000)
2023-10-05 01:43:39,793	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.698 s, which may be a performance bottleneck.
2023-10-05 01:43:39,795	WARNING util.py:315 -- The `process_trial_result` operation took 2.702 s, which may be a performance bottleneck.
2023-10-05 01:43:39,795	WARNING util.py:315 -- Processing trial results took 2.702 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:43:39,795	WARNING util.py:315 -- The `process_trial_result` operation took 2.702 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             ptl/val_loss 0.69208
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:       time_since_restore 113.99628
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:         time_this_iter_s 113.99628
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:             time_total_s 113.99628
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:                timestamp 1696430617
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:               train_loss 0.69448
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dbef1d8_9_batch_size=8,layer_size=32,lr=0.0003_2023-10-05_01-39-30/wandb/offline-run-20231005_014150-8dbef1d8
[2m[36m(_WandbLoggingActor pid=2816887)[0m wandb: Find logs at: ./wandb/offline-run-20231005_014150-8dbef1d8/logs
[2m[36m(TorchTrainer pid=2818359)[0m Starting distributed worker processes: ['2818489 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2818489)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2818489)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2818489)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2818489)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2818489)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2818489)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2818489)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2818489)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2818489)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dfb9fda_10_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-41-43/lightning_logs
[2m[36m(RayTrainWorker pid=2818489)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2818489)[0m 
[2m[36m(RayTrainWorker pid=2818489)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2818489)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2818489)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2818489)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2818489)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2818489)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2818489)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2818489)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2818489)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2818489)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2818489)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2818489)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2818489)[0m finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2818489)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2818489)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2818489)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2818489)[0m finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2818489)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2818489)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=2818489)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2818489)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=2818489)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=2818489)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2818489)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2818489)[0m finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2818489)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2818489)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2818489)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2818489)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dfb9fda_10_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-41-43/checkpoint_000000)
2023-10-05 01:45:53,075	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.833 s, which may be a performance bottleneck.
2023-10-05 01:45:53,076	WARNING util.py:315 -- The `process_trial_result` operation took 2.836 s, which may be a performance bottleneck.
2023-10-05 01:45:53,077	WARNING util.py:315 -- Processing trial results took 2.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:45:53,077	WARNING util.py:315 -- The `process_trial_result` operation took 2.837 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             ptl/val_loss 0.68549
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                     step 27
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:       time_since_restore 114.06871
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:         time_this_iter_s 114.06871
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:             time_total_s 114.06871
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:                timestamp 1696430750
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:           train_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:               train_loss 0.70567
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-04_23-26-53/TorchTrainer_8dfb9fda_10_batch_size=8,layer_size=16,lr=0.0001_2023-10-05_01-41-43/wandb/offline-run-20231005_014404-8dfb9fda
[2m[36m(_WandbLoggingActor pid=2818485)[0m wandb: Find logs at: ./wandb/offline-run-20231005_014404-8dfb9fda/logs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
2023-10-05 01:46:35,606	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-05 01:46:41,404	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-10-05 01:46:41,409	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-05 01:46:41,446	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=2823746)[0m Starting distributed worker processes: ['2824473 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2824473)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2824473)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2824473)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2824473)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2824473)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2824473)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2824473)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2824473)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/lightning_logs
[2m[36m(RayTrainWorker pid=2824473)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2824473)[0m 
[2m[36m(RayTrainWorker pid=2824473)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2824473)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2824473)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2824473)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2824473)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2824473)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2824473)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2824473)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2824473)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2824473)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2824473)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2824473)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2824473)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2824473)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2824473)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2824473)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2824473)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2824473)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 01:48:50,017	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000000)
2023-10-05 01:48:53,077	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.060 s, which may be a performance bottleneck.
2023-10-05 01:48:53,079	WARNING util.py:315 -- The `process_trial_result` operation took 3.063 s, which may be a performance bottleneck.
2023-10-05 01:48:53,079	WARNING util.py:315 -- Processing trial results took 3.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 01:48:53,079	WARNING util.py:315 -- The `process_trial_result` operation took 3.063 s, which may be a performance bottleneck.
2023-10-05 01:50:23,447	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000001)
2023-10-05 01:51:57,049	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000002)
2023-10-05 01:53:30,665	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000003)
2023-10-05 01:55:04,407	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000004)
2023-10-05 01:56:38,328	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000005)
2023-10-05 01:58:11,879	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000006)
2023-10-05 01:59:45,344	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000007)
2023-10-05 02:01:18,953	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000008)
2023-10-05 02:02:52,585	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000009)
2023-10-05 02:04:26,176	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000010)
2023-10-05 02:05:59,724	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000011)
2023-10-05 02:07:33,495	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000012)
2023-10-05 02:09:07,073	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000013)
2023-10-05 02:10:40,606	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000014)
2023-10-05 02:12:14,163	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000015)
2023-10-05 02:13:47,754	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000016)
2023-10-05 02:15:21,374	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000017)
2023-10-05 02:16:55,057	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000018)
[2m[36m(RayTrainWorker pid=2824473)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       ptl/train_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             ptl/val_loss ▁▁▁▂▂▂▃▄▄▄▅▅▅▅▆▆▆▇▇█
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           train_accuracy ███▁▁▁▁██▁▁█▁██▁▁▁▁█
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:               train_loss ▁▁▁████▁▁██▁█▁▁████▁
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       ptl/train_accuracy 0.69374
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           ptl/train_loss 0.69374
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             ptl/val_loss 0.6868
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                     step 1080
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       time_since_restore 1884.27381
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:         time_this_iter_s 93.32134
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:             time_total_s 1884.27381
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:                timestamp 1696432708
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:               train_loss 0.65031
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_749c9a6f_1_batch_size=4,layer_size=8,lr=0.0001_2023-10-05_01-46-41/wandb/offline-run-20231005_014704-749c9a6f
[2m[36m(_WandbLoggingActor pid=2824468)[0m wandb: Find logs at: ./wandb/offline-run-20231005_014704-749c9a6f/logs
[2m[36m(TorchTrainer pid=2837933)[0m Starting distributed worker processes: ['2838063 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2838063)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2838063)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2838063)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2838063)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2838063)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2838063)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2838063)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2838063)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_275f79be_2_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-46-57/lightning_logs
[2m[36m(RayTrainWorker pid=2838063)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2838063)[0m 
[2m[36m(RayTrainWorker pid=2838063)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2838063)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2838063)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2838063)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=2838063)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2838063)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2838063)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=2838063)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2838063)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=2838063)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2838063)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2838063)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2838063)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2838063)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2838063)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2838063)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2838063)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2838063)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2838063)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_275f79be_2_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-46-57/checkpoint_000000)
2023-10-05 02:20:40,110	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.784 s, which may be a performance bottleneck.
2023-10-05 02:20:40,111	WARNING util.py:315 -- The `process_trial_result` operation took 2.787 s, which may be a performance bottleneck.
2023-10-05 02:20:40,112	WARNING util.py:315 -- Processing trial results took 2.788 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:20:40,112	WARNING util.py:315 -- The `process_trial_result` operation took 2.788 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         ptl/val_accuracy 0.41667
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             ptl/val_loss 0.7193
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:              ptl/val_mcc -0.03722
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                     step 54
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:       time_since_restore 112.87476
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:         time_this_iter_s 112.87476
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:             time_total_s 112.87476
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:                timestamp 1696432837
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:           train_accuracy 0.0
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:               train_loss 0.81632
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_275f79be_2_batch_size=4,layer_size=16,lr=0.0001_2023-10-05_01-46-57/wandb/offline-run-20231005_021851-275f79be
[2m[36m(_WandbLoggingActor pid=2838060)[0m wandb: Find logs at: ./wandb/offline-run-20231005_021851-275f79be/logs
[2m[36m(TorchTrainer pid=2839530)[0m Starting distributed worker processes: ['2839660 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2839660)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2839660)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2839660)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2839660)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2839660)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2839660)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2839660)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2839660)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_b961729f_3_batch_size=4,layer_size=8,lr=0.0164_2023-10-05_02-18-44/lightning_logs
[2m[36m(RayTrainWorker pid=2839660)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2839660)[0m 
[2m[36m(RayTrainWorker pid=2839660)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2839660)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2839660)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2839660)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2839660)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2839660)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2839660)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2839660)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2839660)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2839660)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2839660)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2839660)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2839660)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2839660)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2839660)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2839660)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2839660)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2839660)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:22:49,074	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2839660)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_b961729f_3_batch_size=4,layer_size=8,lr=0.0164_2023-10-05_02-18-44/checkpoint_000000)
2023-10-05 02:22:51,946	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.871 s, which may be a performance bottleneck.
2023-10-05 02:22:51,948	WARNING util.py:315 -- The `process_trial_result` operation took 2.875 s, which may be a performance bottleneck.
2023-10-05 02:22:51,948	WARNING util.py:315 -- Processing trial results took 2.875 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:22:51,948	WARNING util.py:315 -- The `process_trial_result` operation took 2.876 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=2839660)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_b961729f_3_batch_size=4,layer_size=8,lr=0.0164_2023-10-05_02-18-44/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             ptl/val_loss █▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           train_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       ptl/train_accuracy 16.22694
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           ptl/train_loss 16.22694
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         ptl/val_accuracy 0.58333
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             ptl/val_aupr 0.57746
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         ptl/val_f1_score 0.73214
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             ptl/val_loss 0.68623
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:              ptl/val_mcc 0.03722
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:        ptl/val_precision 0.57746
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                     step 108
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       time_since_restore 203.26807
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:         time_this_iter_s 90.58757
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:             time_total_s 203.26807
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:                timestamp 1696433062
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:               train_loss 0.64362
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_b961729f_3_batch_size=4,layer_size=8,lr=0.0164_2023-10-05_02-18-44/wandb/offline-run-20231005_022104-b961729f
[2m[36m(_WandbLoggingActor pid=2839657)[0m wandb: Find logs at: ./wandb/offline-run-20231005_022104-b961729f/logs
[2m[36m(TorchTrainer pid=2841396)[0m Starting distributed worker processes: ['2841526 (10.6.30.5)']
[2m[36m(RayTrainWorker pid=2841526)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=2841526)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=2841526)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=2841526)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=2841526)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=2841523)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=2841523)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=2841523)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=2841526)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=2841526)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=2841526)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/lightning_logs
[2m[36m(RayTrainWorker pid=2841526)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=2841526)[0m 
[2m[36m(RayTrainWorker pid=2841526)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=2841526)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2841526)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=2841526)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=2841526)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=2841526)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=2841526)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=2841526)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=2841526)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=2841526)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=2841526)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2841526)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2841526)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2841526)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=2841526)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2841526)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=2841526)[0m /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=2841526)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-05 02:26:31,172	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000000)
2023-10-05 02:26:33,912	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.740 s, which may be a performance bottleneck.
2023-10-05 02:26:33,914	WARNING util.py:315 -- The `process_trial_result` operation took 2.743 s, which may be a performance bottleneck.
2023-10-05 02:26:33,914	WARNING util.py:315 -- Processing trial results took 2.744 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-05 02:26:33,914	WARNING util.py:315 -- The `process_trial_result` operation took 2.744 s, which may be a performance bottleneck.
2023-10-05 02:28:02,781	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000001)
2023-10-05 02:29:34,324	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000002)
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000003)
2023-10-05 02:31:05,867	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-05 02:32:37,462	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000004)
2023-10-05 02:34:09,052	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000005)
2023-10-05 02:35:40,876	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000006)
2023-10-05 02:37:12,721	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000007)
2023-10-05 02:38:44,484	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000008)
2023-10-05 02:40:16,381	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000009)
2023-10-05 02:41:48,181	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000010)
2023-10-05 02:43:19,985	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=2841526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-05_01-46-31/TorchTrainer_33acb1f0_4_batch_size=8,layer_size=8,lr=0.0000_2023-10-05_02-20-56/checkpoint_000011)
2023-10-05 02:44:51,722	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 356, in <module>
    trainer_2.test(model, dataloaders=[test_dataloader_2])
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 742, in test
    return call._call_and_handle_interrupt(
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 785, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 932, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 469, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
