Global seed set to 42
2023-11-15 01:30:49,046	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 01:30:59,015	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 01:30:59,018	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 01:30:59,044	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1030303)[0m Starting distributed worker processes: ['1031025 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1031025)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1031025)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1031025)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1031025)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1031025)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1031025)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1031025)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1031025)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1031025)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/lightning_logs
[2m[36m(RayTrainWorker pid=1031025)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1031025)[0m 
[2m[36m(RayTrainWorker pid=1031025)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1031025)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1031025)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1031025)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1031025)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1031025)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1031025)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1031025)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1031025)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1031025)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1031025)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1031025)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1031025)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1031025)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1031025)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1031025)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1031025)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1031025)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 01:33:00,116	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000000)
2023-11-15 01:33:02,834	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.718 s, which may be a performance bottleneck.
2023-11-15 01:33:02,835	WARNING util.py:315 -- The `process_trial_result` operation took 2.720 s, which may be a performance bottleneck.
2023-11-15 01:33:02,836	WARNING util.py:315 -- Processing trial results took 2.720 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:33:02,836	WARNING util.py:315 -- The `process_trial_result` operation took 2.721 s, which may be a performance bottleneck.
2023-11-15 01:34:16,438	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000001)
2023-11-15 01:35:32,422	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000002)
2023-11-15 01:36:48,103	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000003)
2023-11-15 01:38:03,356	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000004)
2023-11-15 01:39:18,453	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000005)
2023-11-15 01:40:33,208	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000006)
2023-11-15 01:41:48,106	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000007)
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000008)
2023-11-15 01:43:03,103	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 01:44:17,947	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000009)
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000010)
2023-11-15 01:45:32,774	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000011)
2023-11-15 01:46:47,572	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 01:48:02,434	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000012)
2023-11-15 01:49:17,230	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000013)
2023-11-15 01:50:32,261	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000014)
2023-11-15 01:51:47,302	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000015)
2023-11-15 01:53:02,298	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000016)
2023-11-15 01:54:17,177	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000017)
2023-11-15 01:55:32,016	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1031025)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       ptl/train_accuracy ▁▇▆▇▇▆▅▇▇▆█▆▇▇▇▇▇▆▇
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:           ptl/train_loss █▂▂▁▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             ptl/val_loss ▁▂▂▂▂▃▃▄▃▄▄▅▅▅▆▆▇█▇█
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         time_this_iter_s █▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       ptl/train_accuracy 0.53068
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:           ptl/train_loss 0.69149
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             ptl/val_loss 0.70416
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       time_since_restore 1517.98475
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:         time_this_iter_s 74.60293
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:             time_total_s 1517.98475
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:                timestamp 1699973806
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6f07b7cd_1_batch_size=8,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_01-30-59/wandb/offline-run-20231115_013130-6f07b7cd
[2m[36m(_WandbLoggingActor pid=1031020)[0m wandb: Find logs at: ./wandb/offline-run-20231115_013130-6f07b7cd/logs
[2m[36m(TorchTrainer pid=1044478)[0m Starting distributed worker processes: ['1044608 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1044608)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1044608)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1044608)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1044608)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1044608)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1044608)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1044608)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1044608)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1044608)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_69adb550_2_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0026_2023-11-15_01-31-21/lightning_logs
[2m[36m(RayTrainWorker pid=1044608)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1044608)[0m 
[2m[36m(RayTrainWorker pid=1044608)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1044608)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1044608)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1044608)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1044608)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1044608)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1044608)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1044608)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1044608)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1044608)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1044608)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1044608)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1044608)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1044608)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1044608)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1044608)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1044608)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1044608)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1044608)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_69adb550_2_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0026_2023-11-15_01-31-21/checkpoint_000000)
2023-11-15 01:58:49,839	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.263 s, which may be a performance bottleneck.
2023-11-15 01:58:49,841	WARNING util.py:315 -- The `process_trial_result` operation took 3.289 s, which may be a performance bottleneck.
2023-11-15 01:58:49,841	WARNING util.py:315 -- Processing trial results took 3.290 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:58:49,841	WARNING util.py:315 -- The `process_trial_result` operation took 3.290 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             ptl/val_loss 0.70795
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:       time_since_restore 95.55974
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:         time_this_iter_s 95.55974
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:             time_total_s 95.55974
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:                timestamp 1699973926
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_69adb550_2_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0026_2023-11-15_01-31-21/wandb/offline-run-20231115_015718-69adb550
[2m[36m(_WandbLoggingActor pid=1044605)[0m wandb: Find logs at: ./wandb/offline-run-20231115_015718-69adb550/logs
[2m[36m(TorchTrainer pid=1046079)[0m Starting distributed worker processes: ['1046209 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1046209)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1046209)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1046209)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1046209)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1046209)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1046209)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1046209)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1046209)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1046209)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_8fcfd718_3_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0010_2023-11-15_01-57-11/lightning_logs
[2m[36m(RayTrainWorker pid=1046209)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1046209)[0m 
[2m[36m(RayTrainWorker pid=1046209)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1046209)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1046209)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1046209)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1046209)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1046209)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1046209)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1046209)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1046209)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1046209)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1046209)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1046209)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1046209)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1046209)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1046209)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1046209)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1046209)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1046209)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1046209)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_8fcfd718_3_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0010_2023-11-15_01-57-11/checkpoint_000000)
2023-11-15 02:00:53,436	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.503 s, which may be a performance bottleneck.
2023-11-15 02:00:53,438	WARNING util.py:315 -- The `process_trial_result` operation took 4.519 s, which may be a performance bottleneck.
2023-11-15 02:00:53,438	WARNING util.py:315 -- Processing trial results took 4.519 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:00:53,438	WARNING util.py:315 -- The `process_trial_result` operation took 4.519 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             ptl/val_aupr 0.62815
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:            ptl/val_auroc 0.56618
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             ptl/val_loss 5.67372
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:       time_since_restore 98.50114
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:         time_this_iter_s 98.50114
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:             time_total_s 98.50114
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:                timestamp 1699974048
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_8fcfd718_3_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0010_2023-11-15_01-57-11/wandb/offline-run-20231115_015919-8fcfd718
[2m[36m(_WandbLoggingActor pid=1046206)[0m wandb: Find logs at: ./wandb/offline-run-20231115_015919-8fcfd718/logs
[2m[36m(TrainTrainable pid=1047680)[0m Trainable.setup took 17.545 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1047680)[0m Starting distributed worker processes: ['1047813 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1047813)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1047813)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1047813)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1047813)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1047813)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1047813)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1047813)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1047813)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1047813)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/lightning_logs
[2m[36m(RayTrainWorker pid=1047813)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1047813)[0m 
[2m[36m(RayTrainWorker pid=1047813)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1047813)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1047813)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1047813)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=1047813)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1047813)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1047813)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=1047813)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1047813)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=1047813)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1047813)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1047813)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1047813)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1047813)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1047813)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1047813)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1047813)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1047813)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:03:14,728	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000000)
2023-11-15 02:03:19,164	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.436 s, which may be a performance bottleneck.
2023-11-15 02:03:19,165	WARNING util.py:315 -- The `process_trial_result` operation took 4.440 s, which may be a performance bottleneck.
2023-11-15 02:03:19,165	WARNING util.py:315 -- Processing trial results took 4.440 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:03:19,165	WARNING util.py:315 -- The `process_trial_result` operation took 4.440 s, which may be a performance bottleneck.
2023-11-15 02:04:29,502	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000001)
2023-11-15 02:05:44,386	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000002)
2023-11-15 02:06:59,296	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000003)
2023-11-15 02:08:14,216	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000004)
2023-11-15 02:09:29,179	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000005)
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000006)
2023-11-15 02:10:44,045	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 02:11:58,997	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000007)
2023-11-15 02:13:13,898	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000008)
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000009)
2023-11-15 02:14:29,013	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 02:15:44,024	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000010)
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000011)
2023-11-15 02:16:58,973	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 02:18:13,945	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000012)
2023-11-15 02:19:28,854	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000013)
2023-11-15 02:20:43,814	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000014)
2023-11-15 02:21:58,679	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000015)
2023-11-15 02:23:13,591	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000016)
2023-11-15 02:24:28,583	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000017)
2023-11-15 02:25:43,547	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1047813)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       ptl/train_accuracy ▁▇▆▇▇▆▅▇▇▆█▆▇▇▇▇▇▆▇
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             ptl/val_loss ▁▂▁▁▁▂▂▃▃▃▄▄▅▅▆▆▇▇▇█
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       ptl/train_accuracy 0.53068
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:           ptl/train_loss 0.69207
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         ptl/val_accuracy 0.375
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             ptl/val_loss 0.69927
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                     step 440
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       time_since_restore 1519.60821
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:         time_this_iter_s 74.76423
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:             time_total_s 1519.60821
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:                timestamp 1699975618
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2368e2e9_4_batch_size=8,cell_type=mixed_meso,layer_size=16,lr=0.0002_2023-11-15_01-59-10/wandb/offline-run-20231115_020140-2368e2e9
[2m[36m(_WandbLoggingActor pid=1047810)[0m wandb: Find logs at: ./wandb/offline-run-20231115_020140-2368e2e9/logs
[2m[36m(TrainTrainable pid=1059190)[0m Trainable.setup took 10.615 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1059190)[0m Starting distributed worker processes: ['1059321 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1059321)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1059321)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1059321)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1059321)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1059321)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1059321)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1059321)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1059321)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1059321)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6b580eb0_5_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0123_2023-11-15_02-01-30/lightning_logs
[2m[36m(RayTrainWorker pid=1059321)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1059321)[0m 
[2m[36m(RayTrainWorker pid=1059321)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1059321)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1059321)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1059321)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=1059321)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1059321)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1059321)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=1059321)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1059321)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=1059321)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1059321)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1059321)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1059321)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1059321)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1059321)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1059321)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1059321)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1059321)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:29:21,998	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1059321)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6b580eb0_5_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0123_2023-11-15_02-01-30/checkpoint_000000)
2023-11-15 02:29:25,085	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.087 s, which may be a performance bottleneck.
2023-11-15 02:29:25,087	WARNING util.py:315 -- The `process_trial_result` operation took 3.091 s, which may be a performance bottleneck.
2023-11-15 02:29:25,087	WARNING util.py:315 -- Processing trial results took 3.091 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:29:25,087	WARNING util.py:315 -- The `process_trial_result` operation took 3.091 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1059321)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6b580eb0_5_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0123_2023-11-15_02-01-30/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       ptl/train_accuracy 0.55114
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:           ptl/train_loss 16.19067
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             ptl/val_loss 0.70494
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                     step 88
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       time_since_restore 185.9148
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:         time_this_iter_s 73.0795
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:             time_total_s 185.9148
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:                timestamp 1699975838
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_6b580eb0_5_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0123_2023-11-15_02-01-30/wandb/offline-run-20231115_022745-6b580eb0
[2m[36m(_WandbLoggingActor pid=1059316)[0m wandb: Find logs at: ./wandb/offline-run-20231115_022745-6b580eb0/logs
[2m[36m(TrainTrainable pid=1061056)[0m Trainable.setup took 24.045 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1061056)[0m Starting distributed worker processes: ['1061195 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1061195)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1061195)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1061195)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1061195)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1061195)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1061195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1061195)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1061195)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1061195)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_60444a7a_6_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0000_2023-11-15_02-27-29/lightning_logs
[2m[36m(RayTrainWorker pid=1061195)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1061195)[0m 
[2m[36m(RayTrainWorker pid=1061195)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1061195)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1061195)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1061195)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=1061195)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1061195)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1061195)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=1061195)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1061195)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=1061195)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1061195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1061195)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1061195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1061195)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1061195)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1061195)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1061195)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1061195)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1061195)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_60444a7a_6_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0000_2023-11-15_02-27-29/checkpoint_000000)
2023-11-15 02:33:18,051	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.291 s, which may be a performance bottleneck.
2023-11-15 02:33:18,052	WARNING util.py:315 -- The `process_trial_result` operation took 5.294 s, which may be a performance bottleneck.
2023-11-15 02:33:18,052	WARNING util.py:315 -- Processing trial results took 5.295 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:33:18,053	WARNING util.py:315 -- The `process_trial_result` operation took 5.295 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             ptl/val_loss 0.71673
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:       time_since_restore 120.06596
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:         time_this_iter_s 120.06596
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:             time_total_s 120.06596
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:                timestamp 1699975992
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_60444a7a_6_batch_size=4,cell_type=mixed_meso,layer_size=16,lr=0.0000_2023-11-15_02-27-29/wandb/offline-run-20231115_023128-60444a7a
[2m[36m(_WandbLoggingActor pid=1061190)[0m wandb: Find logs at: ./wandb/offline-run-20231115_023128-60444a7a/logs
[2m[36m(TrainTrainable pid=1062670)[0m Trainable.setup took 11.183 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1062670)[0m Starting distributed worker processes: ['1062800 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1062800)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1062800)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1062800)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1062800)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1062800)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1062800)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1062800)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1062800)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1062800)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2d4b0ce8_7_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0004_2023-11-15_02-31-12/lightning_logs
[2m[36m(RayTrainWorker pid=1062800)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1062800)[0m 
[2m[36m(RayTrainWorker pid=1062800)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1062800)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1062800)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1062800)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1062800)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1062800)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1062800)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1062800)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1062800)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1062800)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1062800)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1062800)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1062800)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1062800)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1062800)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1062800)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1062800)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1062800)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1062800)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2d4b0ce8_7_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0004_2023-11-15_02-31-12/checkpoint_000000)
2023-11-15 02:35:38,676	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.955 s, which may be a performance bottleneck.
2023-11-15 02:35:38,678	WARNING util.py:315 -- The `process_trial_result` operation took 2.959 s, which may be a performance bottleneck.
2023-11-15 02:35:38,678	WARNING util.py:315 -- Processing trial results took 2.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:35:38,678	WARNING util.py:315 -- The `process_trial_result` operation took 2.960 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             ptl/val_aupr 0.63175
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:            ptl/val_auroc 0.58578
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             ptl/val_loss 2.07247
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:       time_since_restore 109.47482
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:         time_this_iter_s 109.47482
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:             time_total_s 109.47482
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:                timestamp 1699976135
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_2d4b0ce8_7_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0004_2023-11-15_02-31-12/wandb/offline-run-20231115_023358-2d4b0ce8
[2m[36m(_WandbLoggingActor pid=1062797)[0m wandb: Find logs at: ./wandb/offline-run-20231115_023358-2d4b0ce8/logs
[2m[36m(TorchTrainer pid=1064270)[0m Starting distributed worker processes: ['1064400 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1064400)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1064400)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1064400)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1064400)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1064400)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1064400)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1064400)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1064400)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1064400)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/lightning_logs
[2m[36m(RayTrainWorker pid=1064400)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1064400)[0m 
[2m[36m(RayTrainWorker pid=1064400)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1064400)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1064400)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1064400)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1064400)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1064400)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1064400)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1064400)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1064400)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1064400)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1064400)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1064400)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1064400)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1064400)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1064400)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1064400)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1064400)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1064400)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:37:28,507	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1064400)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/checkpoint_000000)
2023-11-15 02:37:31,169	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.662 s, which may be a performance bottleneck.
2023-11-15 02:37:31,171	WARNING util.py:315 -- The `process_trial_result` operation took 2.666 s, which may be a performance bottleneck.
2023-11-15 02:37:31,171	WARNING util.py:315 -- Processing trial results took 2.666 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:37:31,171	WARNING util.py:315 -- The `process_trial_result` operation took 2.666 s, which may be a performance bottleneck.
2023-11-15 02:38:44,547	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1064400)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/checkpoint_000001)
2023-11-15 02:40:00,731	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1064400)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/checkpoint_000002)
[2m[36m(RayTrainWorker pid=1064400)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       ptl/train_accuracy ▁█▃
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         ptl/val_accuracy █▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             ptl/val_aupr ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:            ptl/val_auroc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         ptl/val_f1_score █▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             ptl/val_loss ▁▂▂█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:              ptl/val_mcc █▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:        ptl/val_precision █▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:           ptl/val_recall █▁▁▁
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       ptl/train_accuracy 0.46023
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:           ptl/train_loss 0.71534
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             ptl/val_loss 0.7769
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                     step 176
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       time_since_restore 320.83408
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:         time_this_iter_s 76.04179
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:             time_total_s 320.83408
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:                timestamp 1699976477
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_f18169f0_8_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0986_2023-11-15_02-33-46/wandb/offline-run-20231115_023600-f18169f0
[2m[36m(_WandbLoggingActor pid=1064397)[0m wandb: Find logs at: ./wandb/offline-run-20231115_023600-f18169f0/logs
[2m[36m(TorchTrainer pid=1067214)[0m Starting distributed worker processes: ['1067347 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1067347)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1067347)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1067347)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1067347)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1067347)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1067347)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1067347)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1067347)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1067347)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_d93162ea_9_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-35-53/lightning_logs
[2m[36m(RayTrainWorker pid=1067347)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1067347)[0m 
[2m[36m(RayTrainWorker pid=1067347)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1067347)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1067347)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1067347)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1067347)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1067347)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1067347)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1067347)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1067347)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1067347)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1067347)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1067347)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1067347)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1067347)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1067347)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1067347)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1067347)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1067347)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1067347)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_d93162ea_9_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-35-53/checkpoint_000000)
2023-11-15 02:43:10,320	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.811 s, which may be a performance bottleneck.
2023-11-15 02:43:10,322	WARNING util.py:315 -- The `process_trial_result` operation took 2.815 s, which may be a performance bottleneck.
2023-11-15 02:43:10,322	WARNING util.py:315 -- Processing trial results took 2.815 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:43:10,322	WARNING util.py:315 -- The `process_trial_result` operation took 2.815 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             ptl/val_loss 0.71291
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:       time_since_restore 94.87437
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:         time_this_iter_s 94.87437
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:             time_total_s 94.87437
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:                timestamp 1699976587
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_d93162ea_9_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-35-53/wandb/offline-run-20231115_024139-d93162ea
[2m[36m(_WandbLoggingActor pid=1067344)[0m wandb: Find logs at: ./wandb/offline-run-20231115_024139-d93162ea/logs
[2m[36m(TorchTrainer pid=1068817)[0m Starting distributed worker processes: ['1068948 (10.6.10.14)']
[2m[36m(RayTrainWorker pid=1068948)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1068948)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1068948)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1068948)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1068948)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1068948)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1068948)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1068948)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1068948)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_719644b4_10_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-41-32/lightning_logs
[2m[36m(RayTrainWorker pid=1068948)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1068948)[0m 
[2m[36m(RayTrainWorker pid=1068948)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1068948)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1068948)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1068948)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1068948)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1068948)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1068948)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1068948)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1068948)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1068948)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1068948)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1068948)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1068948)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1068948)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1068948)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1068948)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1068948)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1068948)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=1068948)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_719644b4_10_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-41-32/checkpoint_000000)
2023-11-15 02:45:06,243	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.592 s, which may be a performance bottleneck.
2023-11-15 02:45:06,244	WARNING util.py:315 -- The `process_trial_result` operation took 2.595 s, which may be a performance bottleneck.
2023-11-15 02:45:06,244	WARNING util.py:315 -- Processing trial results took 2.595 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:45:06,244	WARNING util.py:315 -- The `process_trial_result` operation took 2.595 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         ptl/val_accuracy 0.4
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             ptl/val_aupr 0.58621
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             ptl/val_loss 0.72567
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:              ptl/val_mcc -0.04597
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                     step 44
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:       time_since_restore 97.74262
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:         time_this_iter_s 97.74262
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:             time_total_s 97.74262
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:                timestamp 1699976703
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-30-43/TorchTrainer_719644b4_10_batch_size=4,cell_type=mixed_meso,layer_size=8,lr=0.0000_2023-11-15_02-41-32/wandb/offline-run-20231115_024335-719644b4
[2m[36m(_WandbLoggingActor pid=1068943)[0m wandb: Find logs at: ./wandb/offline-run-20231115_024335-719644b4/logs
