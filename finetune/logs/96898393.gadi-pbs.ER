Global seed set to 42
2023-10-03 16:28:09,697	INFO worker.py:1642 -- Started a local Ray instance.
2023-10-03 16:28:45,922	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-03 16:28:45,969	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1502646)[0m Starting distributed worker processes: ['1502795 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1502795)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1502795)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1502795)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1502795)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1502795)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1502795)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1502795)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1502795)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1502795)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/lightning_logs
[2m[36m(RayTrainWorker pid=1502795)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1502795)[0m 
[2m[36m(RayTrainWorker pid=1502795)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1502795)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1502795)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1502795)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1502795)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1502795)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1502795)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1502795)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1502795)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1502795)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1502795)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1502795)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1502795)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1502795)[0m   self.eval_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1502795)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1502795)[0m   self.eval_probs.append(torch.tensor(class_1_probs))
[2m[36m(RayTrainWorker pid=1502795)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1502795)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1502795)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1502795)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1502795)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1502795)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1502795)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1502795)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1502795)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1502795)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1502795)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1502795)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-03 16:44:07,043	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000000)
2023-10-03 16:44:09,922	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.878 s, which may be a performance bottleneck.
2023-10-03 16:44:09,924	WARNING util.py:315 -- The `process_trial_result` operation took 2.881 s, which may be a performance bottleneck.
2023-10-03 16:44:09,924	WARNING util.py:315 -- Processing trial results took 2.881 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 16:44:09,924	WARNING util.py:315 -- The `process_trial_result` operation took 2.881 s, which may be a performance bottleneck.
2023-10-03 16:58:45,897	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000001)
2023-10-03 17:13:24,197	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000002)
2023-10-03 17:28:01,975	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000003)
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000004)
2023-10-03 17:42:42,448	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-03 17:57:20,055	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000005)
2023-10-03 18:11:57,550	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000006)
2023-10-03 18:26:35,180	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000007)
2023-10-03 18:41:13,816	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000008)
[2m[36m(RayTrainWorker pid=1502795)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/checkpoint_000009)
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                    epoch ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: iterations_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       ptl/train_accuracy █▅▃▃▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           ptl/train_loss █▅▃▃▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         ptl/val_accuracy █▁▅█▅▇██▇▆
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             ptl/val_aupr ▁▃▇█▄▄▇█▅▃
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:            ptl/val_auroc ▁▄▇▇▄▄▇█▆▄
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         ptl/val_f1_score █▁▆█▆▇█▇▇▇
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             ptl/val_loss ▃█▃▁▂▁▁▁▂▂
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:              ptl/val_mcc ▇▁▅█▅▇██▇▆
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:        ptl/val_precision ▅█▁▆▂▄▅▇▇▃
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           ptl/val_recall ▇▁█▇█▇▇▆▅▇
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                     step ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       time_since_restore ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         time_this_iter_s █▁▂▁▂▁▁▁▂▂
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             time_total_s ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                timestamp ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           train_accuracy ▅██▁▅█████
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:               train_loss █▂▁▄▂▂▁▁▂▂
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       training_iteration ▁▂▃▃▄▅▆▆▇█
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                    epoch 9
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: iterations_since_restore 10
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       ptl/train_accuracy 0.22128
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           ptl/train_loss 0.22128
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         ptl/val_accuracy 0.90537
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             ptl/val_aupr 0.97339
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:            ptl/val_auroc 0.97092
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         ptl/val_f1_score 0.90759
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             ptl/val_loss 0.24663
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:              ptl/val_mcc 0.81312
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:        ptl/val_precision 0.87268
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           ptl/val_recall 0.9454
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                     step 5290
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       time_since_restore 8808.87972
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:         time_this_iter_s 879.1037
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:             time_total_s 8808.87972
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:                timestamp 1696319752
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:               train_loss 0.10508
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb:       training_iteration 10
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_bdff8dec_1_batch_size=4,layer_size=16,lr=0.0001_2023-10-03_16-28-45/wandb/offline-run-20231003_162906-bdff8dec
[2m[36m(_WandbLoggingActor pid=1502790)[0m wandb: Find logs at: ./wandb/offline-run-20231003_162906-bdff8dec/logs
[2m[36m(TorchTrainer pid=1524507)[0m Starting distributed worker processes: ['1524651 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1524651)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1524651)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1524651)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1524651)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1524651)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1524651)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1524651)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1524651)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1524651)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_518de50a_2_batch_size=4,layer_size=32,lr=0.0042_2023-10-03_16-28-59/lightning_logs
[2m[36m(RayTrainWorker pid=1524651)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1524651)[0m 
[2m[36m(RayTrainWorker pid=1524651)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1524651)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1524651)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1524651)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1524651)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1524651)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1524651)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1524651)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1524651)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1524651)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1524651)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1524651)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1524651)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1524651)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1524651)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1524651)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1524651)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1524651)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1524651)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1524651)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1524651)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1524651)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1524651)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1524651)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1524651)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1524651)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1524651)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_518de50a_2_batch_size=4,layer_size=32,lr=0.0042_2023-10-03_16-28-59/checkpoint_000000)
2023-10-03 19:11:21,572	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.348 s, which may be a performance bottleneck.
2023-10-03 19:11:21,573	WARNING util.py:315 -- The `process_trial_result` operation took 3.368 s, which may be a performance bottleneck.
2023-10-03 19:11:21,574	WARNING util.py:315 -- Processing trial results took 3.369 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 19:11:21,574	WARNING util.py:315 -- The `process_trial_result` operation took 3.369 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         ptl/val_accuracy 0.89124
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             ptl/val_aupr 0.96098
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:            ptl/val_auroc 0.95633
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         ptl/val_f1_score 0.8849
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             ptl/val_loss 0.28355
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:              ptl/val_mcc 0.78389
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:        ptl/val_precision 0.92212
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:           ptl/val_recall 0.85057
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:       time_since_restore 898.80402
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:         time_this_iter_s 898.80402
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:             time_total_s 898.80402
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:                timestamp 1696320678
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:               train_loss 0.38206
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_518de50a_2_batch_size=4,layer_size=32,lr=0.0042_2023-10-03_16-28-59/wandb/offline-run-20231003_185626-518de50a
[2m[36m(_WandbLoggingActor pid=1524648)[0m wandb: Find logs at: ./wandb/offline-run-20231003_185626-518de50a/logs
[2m[36m(TorchTrainer pid=1527794)[0m Starting distributed worker processes: ['1527925 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1527925)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1527925)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1527925)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1527925)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1527925)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1527925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1527925)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1527925)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1527925)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/lightning_logs
[2m[36m(RayTrainWorker pid=1527925)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1527925)[0m 
[2m[36m(RayTrainWorker pid=1527925)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1527925)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1527925)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1527925)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1527925)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1527925)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1527925)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1527925)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1527925)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1527925)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1527925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1527925)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1527925)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1527925)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1527925)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1527925)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1527925)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1527925)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1527925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1527925)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1527925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1527925)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1527925)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1527925)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1527925)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1527925)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1527925)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1527925)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1527925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/checkpoint_000000)
2023-10-03 19:26:45,604	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-10-03 19:26:48,522	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.918 s, which may be a performance bottleneck.
2023-10-03 19:26:48,524	WARNING util.py:315 -- The `process_trial_result` operation took 2.932 s, which may be a performance bottleneck.
2023-10-03 19:26:48,524	WARNING util.py:315 -- Processing trial results took 2.933 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 19:26:48,524	WARNING util.py:315 -- The `process_trial_result` operation took 2.933 s, which may be a performance bottleneck.
2023-10-03 19:41:24,008	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1527925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/checkpoint_000001)
2023-10-03 19:56:02,327	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1527925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/checkpoint_000002)
[2m[36m(RayTrainWorker pid=1527925)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       ptl/train_accuracy █▁▁
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         ptl/val_accuracy ▁▆▇█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             ptl/val_aupr ▁▄▅█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:            ptl/val_auroc ▁▄▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         ptl/val_f1_score ▁▆▇█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             ptl/val_loss █▁▂▁
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:              ptl/val_mcc ▁▆▇█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:        ptl/val_precision █▆▁▃
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           ptl/val_recall ▁▅█▇
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           train_accuracy ▆██▁
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:               train_loss █▁▁▅
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       ptl/train_accuracy 0.25637
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           ptl/train_loss 0.25637
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         ptl/val_accuracy 0.91949
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             ptl/val_aupr 0.97686
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:            ptl/val_auroc 0.97395
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         ptl/val_f1_score 0.91938
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             ptl/val_loss 0.21538
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:              ptl/val_mcc 0.83897
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:        ptl/val_precision 0.90529
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           ptl/val_recall 0.93391
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                     step 2116
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       time_since_restore 3540.39128
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:         time_this_iter_s 878.24479
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:             time_total_s 3540.39128
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:                timestamp 1696324240
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:           train_accuracy 0.25
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:               train_loss 0.87942
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_58a351da_3_batch_size=4,layer_size=32,lr=0.0005_2023-10-03_18-56-19/wandb/offline-run-20231003_191145-58a351da
[2m[36m(_WandbLoggingActor pid=1527922)[0m wandb: Find logs at: ./wandb/offline-run-20231003_191145-58a351da/logs
[2m[36m(TorchTrainer pid=1536828)[0m Starting distributed worker processes: ['1536957 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1536957)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1536957)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1536957)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1536957)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1536957)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1536957)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1536957)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1536957)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1536957)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_57a1ff17_4_batch_size=4,layer_size=16,lr=0.0002_2023-10-03_19-11-37/lightning_logs
[2m[36m(RayTrainWorker pid=1536957)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1536957)[0m 
[2m[36m(RayTrainWorker pid=1536957)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1536957)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1536957)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1536957)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1536957)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1536957)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1536957)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1536957)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1536957)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1536957)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1536957)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1536957)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1536957)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1536957)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1536957)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1536957)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1536957)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1536957)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1536957)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1536957)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1536957)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1536957)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1536957)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1536957)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1536957)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1536957)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1536957)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1536957)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1536957)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_57a1ff17_4_batch_size=4,layer_size=16,lr=0.0002_2023-10-03_19-11-37/checkpoint_000000)
2023-10-03 20:25:55,334	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.702 s, which may be a performance bottleneck.
2023-10-03 20:25:55,335	WARNING util.py:315 -- The `process_trial_result` operation took 2.705 s, which may be a performance bottleneck.
2023-10-03 20:25:55,336	WARNING util.py:315 -- Processing trial results took 2.706 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 20:25:55,336	WARNING util.py:315 -- The `process_trial_result` operation took 2.706 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         ptl/val_accuracy 0.83616
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             ptl/val_aupr 0.96837
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:            ptl/val_auroc 0.96463
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         ptl/val_f1_score 0.80405
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             ptl/val_loss 1.11352
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:              ptl/val_mcc 0.70135
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:        ptl/val_precision 0.97541
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:           ptl/val_recall 0.68391
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:       time_since_restore 896.79166
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:         time_this_iter_s 896.79166
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:             time_total_s 896.79166
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:                timestamp 1696325152
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:               train_loss 2.99422
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_57a1ff17_4_batch_size=4,layer_size=16,lr=0.0002_2023-10-03_19-11-37/wandb/offline-run-20231003_201102-57a1ff17
[2m[36m(_WandbLoggingActor pid=1536954)[0m wandb: Find logs at: ./wandb/offline-run-20231003_201102-57a1ff17/logs
[2m[36m(TorchTrainer pid=1539318)[0m Starting distributed worker processes: ['1539448 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1539448)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1539448)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1539448)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1539448)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1539448)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1539448)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1539448)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1539448)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1539448)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6491d15f_5_batch_size=4,layer_size=32,lr=0.0015_2023-10-03_20-10-55/lightning_logs
[2m[36m(RayTrainWorker pid=1539448)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1539448)[0m 
[2m[36m(RayTrainWorker pid=1539448)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1539448)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1539448)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1539448)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1539448)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1539448)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1539448)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1539448)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1539448)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1539448)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1539448)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1539448)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1539448)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1539448)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1539448)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1539448)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1539448)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1539448)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1539448)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1539448)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1539448)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1539448)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1539448)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1539448)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1539448)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1539448)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1539448)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1539448)[0m   self.train_accuracy.append(torch.tensor(loss))
2023-10-03 20:41:07,495	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1539448)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6491d15f_5_batch_size=4,layer_size=32,lr=0.0015_2023-10-03_20-10-55/checkpoint_000000)
2023-10-03 20:41:10,715	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.219 s, which may be a performance bottleneck.
2023-10-03 20:41:10,717	WARNING util.py:315 -- The `process_trial_result` operation took 3.225 s, which may be a performance bottleneck.
2023-10-03 20:41:10,717	WARNING util.py:315 -- Processing trial results took 3.225 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 20:41:10,717	WARNING util.py:315 -- The `process_trial_result` operation took 3.225 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1539448)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6491d15f_5_batch_size=4,layer_size=32,lr=0.0015_2023-10-03_20-10-55/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         ptl/val_accuracy █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             ptl/val_aupr ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:            ptl/val_auroc ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         ptl/val_f1_score █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:              ptl/val_mcc █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:        ptl/val_precision ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           ptl/val_recall █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           train_accuracy ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:               train_loss █▁
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       ptl/train_accuracy 3.78635
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           ptl/train_loss 3.78635
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         ptl/val_accuracy 0.90113
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             ptl/val_aupr 0.97363
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:            ptl/val_auroc 0.9706
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         ptl/val_f1_score 0.8913
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             ptl/val_loss 0.28893
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:              ptl/val_mcc 0.81012
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:        ptl/val_precision 0.96959
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           ptl/val_recall 0.82471
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                     step 1058
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       time_since_restore 1772.63641
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:         time_this_iter_s 874.84987
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:             time_total_s 1772.63641
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:                timestamp 1696326945
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:               train_loss 0.05154
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6491d15f_5_batch_size=4,layer_size=32,lr=0.0015_2023-10-03_20-10-55/wandb/offline-run-20231003_202617-6491d15f
[2m[36m(_WandbLoggingActor pid=1539445)[0m wandb: Find logs at: ./wandb/offline-run-20231003_202617-6491d15f/logs
[2m[36m(TorchTrainer pid=1544535)[0m Starting distributed worker processes: ['1544664 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1544664)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1544664)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1544664)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1544664)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1544664)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1544664)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1544664)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1544664)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1544664)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_10d87e0d_6_batch_size=4,layer_size=8,lr=0.0000_2023-10-03_20-26-09/lightning_logs
[2m[36m(RayTrainWorker pid=1544664)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1544664)[0m 
[2m[36m(RayTrainWorker pid=1544664)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1544664)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1544664)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1544664)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1544664)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1544664)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1544664)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1544664)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1544664)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1544664)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1544664)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1544664)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1544664)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1544664)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1544664)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1544664)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1544664)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1544664)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1544664)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1544664)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1544664)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1544664)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1544664)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1544664)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1544664)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1544664)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1544664)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1544664)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1544664)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_10d87e0d_6_batch_size=4,layer_size=8,lr=0.0000_2023-10-03_20-26-09/checkpoint_000000)
2023-10-03 21:10:59,798	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.258 s, which may be a performance bottleneck.
2023-10-03 21:10:59,800	WARNING util.py:315 -- The `process_trial_result` operation took 3.263 s, which may be a performance bottleneck.
2023-10-03 21:10:59,801	WARNING util.py:315 -- Processing trial results took 3.263 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 21:10:59,801	WARNING util.py:315 -- The `process_trial_result` operation took 3.263 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         ptl/val_accuracy 0.91808
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             ptl/val_aupr 0.9688
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:            ptl/val_auroc 0.96483
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         ptl/val_f1_score 0.91317
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             ptl/val_loss 0.25149
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:              ptl/val_mcc 0.83812
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:        ptl/val_precision 0.95312
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:           ptl/val_recall 0.87644
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:       time_since_restore 896.91143
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:         time_this_iter_s 896.91143
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:             time_total_s 896.91143
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:                timestamp 1696327856
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:               train_loss 0.62857
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_10d87e0d_6_batch_size=4,layer_size=8,lr=0.0000_2023-10-03_20-26-09/wandb/offline-run-20231003_205607-10d87e0d
[2m[36m(_WandbLoggingActor pid=1544661)[0m wandb: Find logs at: ./wandb/offline-run-20231003_205607-10d87e0d/logs
[2m[36m(TorchTrainer pid=1547023)[0m Starting distributed worker processes: ['1547153 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1547153)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1547153)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1547153)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1547153)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1547153)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1547153)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1547153)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1547153)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1547153)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_31ccfb46_7_batch_size=4,layer_size=32,lr=0.0819_2023-10-03_20-55-59/lightning_logs
[2m[36m(RayTrainWorker pid=1547153)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1547153)[0m 
[2m[36m(RayTrainWorker pid=1547153)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1547153)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1547153)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1547153)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1547153)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1547153)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1547153)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1547153)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1547153)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1547153)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1547153)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1547153)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1547153)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1547153)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1547153)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1547153)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1547153)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1547153)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1547153)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1547153)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1547153)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1547153)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1547153)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1547153)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1547153)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1547153)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1547153)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1547153)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1547153)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_31ccfb46_7_batch_size=4,layer_size=32,lr=0.0819_2023-10-03_20-55-59/checkpoint_000000)
2023-10-03 21:26:16,699	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.364 s, which may be a performance bottleneck.
2023-10-03 21:26:16,701	WARNING util.py:315 -- The `process_trial_result` operation took 3.369 s, which may be a performance bottleneck.
2023-10-03 21:26:16,701	WARNING util.py:315 -- Processing trial results took 3.369 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 21:26:16,702	WARNING util.py:315 -- The `process_trial_result` operation took 3.369 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         ptl/val_accuracy 0.87429
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             ptl/val_aupr 0.82599
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:            ptl/val_auroc 0.88235
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         ptl/val_f1_score 0.87552
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             ptl/val_loss 138.71587
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:              ptl/val_mcc 0.7491
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:        ptl/val_precision 0.85286
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:           ptl/val_recall 0.89943
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:       time_since_restore 898.32378
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:         time_this_iter_s 898.32378
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:             time_total_s 898.32378
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:                timestamp 1696328773
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:               train_loss 20.77437
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_31ccfb46_7_batch_size=4,layer_size=32,lr=0.0819_2023-10-03_20-55-59/wandb/offline-run-20231003_211122-31ccfb46
[2m[36m(_WandbLoggingActor pid=1547150)[0m wandb: Find logs at: ./wandb/offline-run-20231003_211122-31ccfb46/logs
[2m[36m(TorchTrainer pid=1550294)[0m Starting distributed worker processes: ['1550427 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1550427)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1550427)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1550427)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1550427)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1550427)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1550427)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1550427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1550427)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1550427)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_db3ec43f_8_batch_size=4,layer_size=32,lr=0.0342_2023-10-03_21-11-15/lightning_logs
[2m[36m(RayTrainWorker pid=1550427)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1550427)[0m 
[2m[36m(RayTrainWorker pid=1550427)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1550427)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1550427)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1550427)[0m 1 | model             | BinaryAdapterWrapper   | 270 M 
[2m[36m(RayTrainWorker pid=1550427)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1550427)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1550427)[0m 270 M     Trainable params
[2m[36m(RayTrainWorker pid=1550427)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1550427)[0m 270 M     Total params
[2m[36m(RayTrainWorker pid=1550427)[0m 1,083.529 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1550427)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1550427)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1550427)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1550427)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1550427)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1550427)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1550427)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1550427)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1550427)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1550427)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1550427)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1550427)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1550427)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1550427)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1550427)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1550427)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1550427)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1550427)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1550427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_db3ec43f_8_batch_size=4,layer_size=32,lr=0.0342_2023-10-03_21-11-15/checkpoint_000000)
2023-10-03 21:41:32,215	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.168 s, which may be a performance bottleneck.
2023-10-03 21:41:32,217	WARNING util.py:315 -- The `process_trial_result` operation took 3.172 s, which may be a performance bottleneck.
2023-10-03 21:41:32,217	WARNING util.py:315 -- Processing trial results took 3.172 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 21:41:32,217	WARNING util.py:315 -- The `process_trial_result` operation took 3.172 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         ptl/val_accuracy 0.66243
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             ptl/val_aupr 0.68458
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:            ptl/val_auroc 0.69101
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         ptl/val_f1_score 0.4793
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             ptl/val_loss 137.4118
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:              ptl/val_mcc 0.4303
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:        ptl/val_precision 0.99099
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:           ptl/val_recall 0.31609
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:       time_since_restore 896.98951
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:         time_this_iter_s 896.98951
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:             time_total_s 896.98951
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:                timestamp 1696329689
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:               train_loss 59.60542
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_db3ec43f_8_batch_size=4,layer_size=32,lr=0.0342_2023-10-03_21-11-15/wandb/offline-run-20231003_212639-db3ec43f
[2m[36m(_WandbLoggingActor pid=1550424)[0m wandb: Find logs at: ./wandb/offline-run-20231003_212639-db3ec43f/logs
[2m[36m(TorchTrainer pid=1553581)[0m Starting distributed worker processes: ['1553711 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1553711)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1553711)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1553711)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1553711)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1553711)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1553711)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1553711)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1553711)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1553711)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6691aa55_9_batch_size=4,layer_size=16,lr=0.0070_2023-10-03_21-26-32/lightning_logs
[2m[36m(RayTrainWorker pid=1553711)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1553711)[0m 
[2m[36m(RayTrainWorker pid=1553711)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1553711)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1553711)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1553711)[0m 1 | model             | BinaryAdapterWrapper   | 261 M 
[2m[36m(RayTrainWorker pid=1553711)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1553711)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1553711)[0m 261 M     Trainable params
[2m[36m(RayTrainWorker pid=1553711)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1553711)[0m 261 M     Total params
[2m[36m(RayTrainWorker pid=1553711)[0m 1,044.207 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1553711)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1553711)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1553711)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1553711)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1553711)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1553711)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1553711)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1553711)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1553711)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1553711)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1553711)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1553711)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1553711)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('ptl/val_mcc', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
[2m[36m(RayTrainWorker pid=1553711)[0m   warning_cache.warn(
[2m[36m(RayTrainWorker pid=1553711)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1553711)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1553711)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1553711)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1553711)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6691aa55_9_batch_size=4,layer_size=16,lr=0.0070_2023-10-03_21-26-32/checkpoint_000000)
2023-10-03 21:56:48,039	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.740 s, which may be a performance bottleneck.
2023-10-03 21:56:48,041	WARNING util.py:315 -- The `process_trial_result` operation took 3.744 s, which may be a performance bottleneck.
2023-10-03 21:56:48,042	WARNING util.py:315 -- Processing trial results took 3.745 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 21:56:48,042	WARNING util.py:315 -- The `process_trial_result` operation took 3.745 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         ptl/val_accuracy 0.51695
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             ptl/val_aupr 0.8813
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:            ptl/val_auroc 0.88832
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         ptl/val_f1_score 0.66989
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             ptl/val_loss 0.9877
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:              ptl/val_mcc 0.13927
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:        ptl/val_precision 0.50511
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:           ptl/val_recall 0.99425
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                     step 529
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:       time_since_restore 896.27694
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:         time_this_iter_s 896.27694
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:             time_total_s 896.27694
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:                timestamp 1696330604
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:           train_accuracy 1.0
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:               train_loss 0.2292
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_6691aa55_9_batch_size=4,layer_size=16,lr=0.0070_2023-10-03_21-26-32/wandb/offline-run-20231003_214155-6691aa55
[2m[36m(_WandbLoggingActor pid=1553708)[0m wandb: Find logs at: ./wandb/offline-run-20231003_214155-6691aa55/logs
[2m[36m(TorchTrainer pid=1556064)[0m Starting distributed worker processes: ['1556205 (10.6.28.18)']
[2m[36m(RayTrainWorker pid=1556205)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1556205)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1556205)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1556205)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1556205)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1556205)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1556205)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1556205)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1556205)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_9f20d775_10_batch_size=8,layer_size=8,lr=0.0011_2023-10-03_21-41-48/lightning_logs
[2m[36m(RayTrainWorker pid=1556205)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1556205)[0m 
[2m[36m(RayTrainWorker pid=1556205)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1556205)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1556205)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1556205)[0m 1 | model             | BinaryAdapterWrapper   | 256 M 
[2m[36m(RayTrainWorker pid=1556205)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1556205)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1556205)[0m 256 M     Trainable params
[2m[36m(RayTrainWorker pid=1556205)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1556205)[0m 256 M     Total params
[2m[36m(RayTrainWorker pid=1556205)[0m 1,024.546 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1556205)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1556205)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1556205)[0m finetune/fine_tune_tidy.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1556205)[0m   self.eval_accuracy.append(torch.tensor(accuracy))
[2m[36m(RayTrainWorker pid=1556205)[0m finetune/fine_tune_tidy.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1556205)[0m   self.eval_target.append(torch.tensor(target.int()))
[2m[36m(RayTrainWorker pid=1556205)[0m finetune/fine_tune_tidy.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1556205)[0m   self.eval_preds.append(preds)
[2m[36m(RayTrainWorker pid=1556205)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1556205)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1556205)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1556205)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1556205)[0m finetune/fine_tune_tidy.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1556205)[0m   self.train_loss.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1556205)[0m finetune/fine_tune_tidy.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1556205)[0m   self.train_accuracy.append(torch.tensor(loss))
[2m[36m(RayTrainWorker pid=1556205)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_9f20d775_10_batch_size=8,layer_size=8,lr=0.0011_2023-10-03_21-41-48/checkpoint_000000)
2023-10-03 22:12:10,859	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.714 s, which may be a performance bottleneck.
2023-10-03 22:12:10,861	WARNING util.py:315 -- The `process_trial_result` operation took 2.719 s, which may be a performance bottleneck.
2023-10-03 22:12:10,861	WARNING util.py:315 -- Processing trial results took 2.719 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-10-03 22:12:10,862	WARNING util.py:315 -- The `process_trial_result` operation took 2.719 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:           train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:               train_loss ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         ptl/val_accuracy 0.875
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             ptl/val_aupr 0.95253
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:            ptl/val_auroc 0.95822
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         ptl/val_f1_score 0.88243
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             ptl/val_loss 0.90045
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:              ptl/val_mcc 0.75982
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:        ptl/val_precision 0.81663
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:           ptl/val_recall 0.95977
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                     step 265
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:       time_since_restore 896.05089
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:         time_this_iter_s 896.05089
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:             time_total_s 896.05089
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:                timestamp 1696331528
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:           train_accuracy 0.75
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:               train_loss 0.69513
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-10-03_16-28-45/TorchTrainer_9f20d775_10_batch_size=8,layer_size=8,lr=0.0011_2023-10-03_21-41-48/wandb/offline-run-20231003_215727-9f20d775
[2m[36m(_WandbLoggingActor pid=1556200)[0m wandb: Find logs at: ./wandb/offline-run-20231003_215727-9f20d775/logs
