Global seed set to 42
2023-11-15 02:49:34,855	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 02:49:46,037	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 02:49:46,047	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 02:49:46,136	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1209844)[0m Starting distributed worker processes: ['1210562 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1210562)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1210562)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1210562)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1210562)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1210562)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1210562)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1210562)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1210562)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1210562)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/lightning_logs
[2m[36m(RayTrainWorker pid=1210562)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1210562)[0m 
[2m[36m(RayTrainWorker pid=1210562)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1210562)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1210562)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1210562)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=1210562)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1210562)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1210562)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=1210562)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1210562)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=1210562)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1210562)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1210562)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1210562)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1210562)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1210562)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1210562)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1210562)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1210562)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:59:34,750	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000000)
2023-11-15 02:59:48,089	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 13.338 s, which may be a performance bottleneck.
2023-11-15 02:59:48,091	WARNING util.py:315 -- The `process_trial_result` operation took 13.341 s, which may be a performance bottleneck.
2023-11-15 02:59:48,091	WARNING util.py:315 -- Processing trial results took 13.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:59:48,091	WARNING util.py:315 -- The `process_trial_result` operation took 13.342 s, which may be a performance bottleneck.
2023-11-15 03:08:36,565	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000001)
2023-11-15 03:17:37,961	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000002)
2023-11-15 03:26:38,659	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000003)
2023-11-15 03:35:40,037	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000004)
2023-11-15 03:44:40,986	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000005)
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000006)
2023-11-15 03:53:41,574	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 04:02:43,351	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000007)
2023-11-15 04:11:44,208	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000008)
2023-11-15 04:20:45,074	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000009)
2023-11-15 04:29:46,101	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000010)
2023-11-15 04:38:46,923	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000011)
2023-11-15 04:47:48,258	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000012)
2023-11-15 04:56:49,081	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000013)
2023-11-15 05:05:51,101	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000014)
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000015)
2023-11-15 05:14:52,781	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 05:23:54,225	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000016)
2023-11-15 05:32:55,105	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000017)
2023-11-15 05:41:56,270	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1210562)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       ptl/train_accuracy ▅▁▅▅▁█████▅▅▅▅██▅▁█
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:           ptl/train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         ptl/val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             ptl/val_aupr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:            ptl/val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         ptl/val_f1_score ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             ptl/val_loss █▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:              ptl/val_mcc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:        ptl/val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:           ptl/val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         time_this_iter_s █▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       ptl/train_accuracy 0.50153
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:           ptl/train_loss 0.69319
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         ptl/val_accuracy 0.47936
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         ptl/val_f1_score 0.64587
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             ptl/val_loss 0.69356
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:              ptl/val_mcc -0.00443
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:        ptl/val_precision 0.47696
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                     step 6520
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       time_since_restore 10837.78329
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:         time_this_iter_s 541.0108
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:             time_total_s 10837.78329
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:                timestamp 1699987857
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28d8f11e_1_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0001_2023-11-15_02-49-46/wandb/offline-run-20231115_025010-28d8f11e
[2m[36m(_WandbLoggingActor pid=1210557)[0m wandb: Find logs at: ./wandb/offline-run-20231115_025010-28d8f11e/logs
[2m[36m(TrainTrainable pid=1225112)[0m Trainable.setup took 17.262 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1225112)[0m Starting distributed worker processes: ['1225242 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1225242)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1225242)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1225242)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1225242)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1225242)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1225242)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1225242)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1225242)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1225242)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/lightning_logs
[2m[36m(RayTrainWorker pid=1225242)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1225242)[0m 
[2m[36m(RayTrainWorker pid=1225242)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1225242)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1225242)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1225242)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1225242)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1225242)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1225242)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1225242)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1225242)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1225242)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1225242)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1225242)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1225242)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1225242)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1225242)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1225242)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1225242)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1225242)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 06:01:06,652	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1225242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/checkpoint_000000)
2023-11-15 06:01:09,475	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.823 s, which may be a performance bottleneck.
2023-11-15 06:01:09,476	WARNING util.py:315 -- The `process_trial_result` operation took 2.827 s, which may be a performance bottleneck.
2023-11-15 06:01:09,477	WARNING util.py:315 -- Processing trial results took 2.828 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 06:01:09,477	WARNING util.py:315 -- The `process_trial_result` operation took 2.828 s, which may be a performance bottleneck.
2023-11-15 06:10:06,345	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1225242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/checkpoint_000001)
2023-11-15 06:19:06,776	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1225242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/checkpoint_000002)
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1225242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                    epoch ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: iterations_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       ptl/train_accuracy ▂█▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:           ptl/train_loss █▁▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         ptl/val_accuracy █▁█▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             ptl/val_aupr ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:            ptl/val_auroc ▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         ptl/val_f1_score ▁█▁█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             ptl/val_loss ▁▂▁█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:              ptl/val_mcc █▁█▁
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:        ptl/val_precision ▁█▁█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:           ptl/val_recall ▁█▁█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                     step ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       time_since_restore ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         time_this_iter_s █▁▂▂
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             time_total_s ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                timestamp ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       training_iteration ▁▃▆█
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       ptl/train_accuracy 0.49617
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:           ptl/train_loss 0.69912
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         ptl/val_accuracy 0.47936
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         ptl/val_f1_score 0.64587
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             ptl/val_loss 0.70795
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:              ptl/val_mcc -0.00443
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:        ptl/val_precision 0.47696
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                     step 1304
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       time_since_restore 2179.6371
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:         time_this_iter_s 540.40924
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:             time_total_s 2179.6371
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:                timestamp 1699990087
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_94d9f6a3_2_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0485_2023-11-15_02-50-01/wandb/offline-run-20231115_055151-94d9f6a3
[2m[36m(_WandbLoggingActor pid=1225239)[0m wandb: Find logs at: ./wandb/offline-run-20231115_055151-94d9f6a3/logs
[2m[36m(TorchTrainer pid=1228235)[0m Starting distributed worker processes: ['1228364 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1228364)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1228364)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1228364)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1228364)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1228364)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1228364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1228364)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1228364)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1228364)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/lightning_logs
[2m[36m(RayTrainWorker pid=1228364)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1228364)[0m 
[2m[36m(RayTrainWorker pid=1228364)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1228364)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1228364)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1228364)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=1228364)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1228364)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1228364)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=1228364)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1228364)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=1228364)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1228364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1228364)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1228364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1228364)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1228364)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1228364)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1228364)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1228364)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 06:37:39,620	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000000)
2023-11-15 06:37:42,457	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.837 s, which may be a performance bottleneck.
2023-11-15 06:37:42,458	WARNING util.py:315 -- The `process_trial_result` operation took 2.839 s, which may be a performance bottleneck.
2023-11-15 06:37:42,458	WARNING util.py:315 -- Processing trial results took 2.839 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 06:37:42,458	WARNING util.py:315 -- The `process_trial_result` operation took 2.839 s, which may be a performance bottleneck.
2023-11-15 06:46:40,191	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000001)
2023-11-15 06:55:41,343	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000002)
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000003)
2023-11-15 07:04:42,410	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 07:13:43,970	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000004)
2023-11-15 07:22:44,968	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000005)
2023-11-15 07:31:46,193	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000006)
2023-11-15 07:40:49,716	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000007)
2023-11-15 07:49:50,880	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000008)
2023-11-15 07:58:51,739	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000009)
2023-11-15 08:07:59,243	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000010)
2023-11-15 08:17:00,744	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000011)
2023-11-15 08:26:01,621	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000012)
2023-11-15 08:35:02,406	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000013)
2023-11-15 08:44:04,402	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000014)
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000015)
2023-11-15 08:53:06,111	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 09:02:07,344	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000016)
2023-11-15 09:11:08,340	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000017)
2023-11-15 09:20:09,348	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1228364)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       ptl/train_accuracy ▁▅▆▅▆▆▇▇▇▇▇▇█▇█▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:           ptl/train_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         ptl/val_accuracy ▆▆▁▄█▇▄▇▅▇▇▆▇▆▄▇▅▇▄▆
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             ptl/val_aupr ▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇████
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:            ptl/val_auroc ▁▄▅▆▆▇▇▇▇▇▇█▇▇▇▇▇█▇▇
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         ptl/val_f1_score ▁▅▂▅█▇▄▇▅█▆▆▆▇▅▇▅▇▅▁
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             ptl/val_loss ▃▂█▄▁▂▆▁▄▁▁▃▁▃▆▁▅▁▅▃
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:              ptl/val_mcc ▄▅▁▅█▇▄▇▅▇▇▆▆▆▄▇▅▇▄▆
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:        ptl/val_precision ▆▄▁▃▆▄▂▆▃▅▆▄▆▄▃▅▃▅▃█
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:           ptl/val_recall ▂▆██▆▇█▅▇▆▄▇▄▇▇▅▇▅▇▁
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▃▂▂▄▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       ptl/train_accuracy 0.82209
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:           ptl/train_loss 0.37875
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         ptl/val_accuracy 0.76376
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             ptl/val_aupr 0.87992
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:            ptl/val_auroc 0.88551
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         ptl/val_f1_score 0.7069
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             ptl/val_loss 0.51918
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:              ptl/val_mcc 0.54914
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:        ptl/val_precision 0.87234
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:           ptl/val_recall 0.5942
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                     step 6520
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       time_since_restore 10842.40536
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:         time_this_iter_s 540.66125
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:             time_total_s 10842.40536
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:                timestamp 1700000950
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_80ff0161_3_batch_size=4,cell_type=spinalcord,layer_size=32,lr=0.0002_2023-11-15_05-51-44/wandb/offline-run-20231115_062827-80ff0161
[2m[36m(_WandbLoggingActor pid=1228361)[0m wandb: Find logs at: ./wandb/offline-run-20231115_062827-80ff0161/logs
[2m[36m(TrainTrainable pid=1240845)[0m Trainable.setup took 17.030 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1240845)[0m Starting distributed worker processes: ['1240983 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1240983)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1240983)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1240983)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1240983)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1240983)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1240983)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1240983)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1240983)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1240983)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/lightning_logs
[2m[36m(RayTrainWorker pid=1240983)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1240983)[0m 
[2m[36m(RayTrainWorker pid=1240983)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1240983)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1240983)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1240983)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1240983)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1240983)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1240983)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1240983)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1240983)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1240983)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1240983)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1240983)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1240983)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1240983)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1240983)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1240983)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1240983)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1240983)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 09:39:12,983	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000000)
2023-11-15 09:39:16,049	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.066 s, which may be a performance bottleneck.
2023-11-15 09:39:16,050	WARNING util.py:315 -- The `process_trial_result` operation took 3.070 s, which may be a performance bottleneck.
2023-11-15 09:39:16,050	WARNING util.py:315 -- Processing trial results took 3.070 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 09:39:16,051	WARNING util.py:315 -- The `process_trial_result` operation took 3.070 s, which may be a performance bottleneck.
2023-11-15 09:48:02,853	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000001)
2023-11-15 09:56:52,609	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000002)
2023-11-15 10:05:42,059	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000003)
2023-11-15 10:14:30,809	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000004)
2023-11-15 10:23:20,094	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000005)
2023-11-15 10:32:09,087	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000006)
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000007)
2023-11-15 10:41:03,273	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 10:49:52,504	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000008)
2023-11-15 10:58:41,674	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000009)
2023-11-15 11:07:30,772	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000010)
2023-11-15 11:16:20,497	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000011)
2023-11-15 11:25:09,580	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000012)
2023-11-15 11:33:58,530	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000013)
2023-11-15 11:42:47,446	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000014)
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000015)
2023-11-15 11:51:38,414	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 12:00:27,871	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000016)
2023-11-15 12:09:16,547	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000017)
2023-11-15 12:18:05,214	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000018)
[2m[36m(RayTrainWorker pid=1240983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       ptl/train_accuracy ▁▄▅▆▆▆▇▇█▇▇▇███████
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:           ptl/train_loss █▆▅▄▄▃▃▃▂▃▂▂▂▂▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         ptl/val_accuracy ▆▁▄▅▁▅▆▆▅▅▅▆▅█▇▅▆▆▇▆
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             ptl/val_aupr ▁▃▃▄▅▅▅▆▆▆▇▇▇▇██████
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:            ptl/val_auroc ▁▃▄▅▅▆▆▆▇▇▇▇▇███████
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         ptl/val_f1_score ▅▁▃▅▁▅▅▆▅▅▅▆▅█▇▅▆▆▇▆
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             ptl/val_loss ▆▇▅▃█▃▂▂▃▃▃▂▄▁▂▃▂▂▂▃
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:              ptl/val_mcc ▅▁▄▅▂▅▅▆▅▅▅▆▅█▇▅▆▆▇▆
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:        ptl/val_precision ▅▁▃▄▁▄▅▆▄▄▄▅▄█▆▄▅▅▆▅
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:           ptl/val_recall ▁█▇▅█▅▄▂▅▅▅▅▇▁▄▅▅▅▅▅
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         time_this_iter_s █▁▂▂▁▂▂▃▂▂▂▂▁▂▁▂▂▁▁▁
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       ptl/train_accuracy 0.81212
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:           ptl/train_loss 0.42312
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         ptl/val_accuracy 0.75227
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             ptl/val_aupr 0.86937
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:            ptl/val_auroc 0.8851
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         ptl/val_f1_score 0.78244
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             ptl/val_loss 0.49028
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:              ptl/val_mcc 0.55041
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:        ptl/val_precision 0.66667
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:           ptl/val_recall 0.94686
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                     step 3260
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       time_since_restore 10609.38127
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:         time_this_iter_s 528.66337
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:             time_total_s 10609.38127
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:                timestamp 1700011614
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_89f65d90_4_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0000_2023-11-15_06-28-21/wandb/offline-run-20231115_093006-89f65d90
[2m[36m(_WandbLoggingActor pid=1240980)[0m wandb: Find logs at: ./wandb/offline-run-20231115_093006-89f65d90/logs
[2m[36m(TrainTrainable pid=1253415)[0m Trainable.setup took 34.682 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=1253415)[0m Starting distributed worker processes: ['1253550 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1253550)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1253550)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1253550)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1253550)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1253550)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1253550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1253550)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1253550)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1253550)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d1be5060_5_batch_size=8,cell_type=spinalcord,layer_size=16,lr=0.0048_2023-11-15_09-29-58/lightning_logs
[2m[36m(RayTrainWorker pid=1253550)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1253550)[0m 
[2m[36m(RayTrainWorker pid=1253550)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1253550)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1253550)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1253550)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=1253550)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1253550)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1253550)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=1253550)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1253550)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=1253550)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1253550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1253550)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1253550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1253550)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1253550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1253550)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1253550)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1253550)[0m   
[2m[36m(RayTrainWorker pid=1253550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d1be5060_5_batch_size=8,cell_type=spinalcord,layer_size=16,lr=0.0048_2023-11-15_09-29-58/checkpoint_000000)
2023-11-15 12:37:46,033	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.128 s, which may be a performance bottleneck.
2023-11-15 12:37:46,035	WARNING util.py:315 -- The `process_trial_result` operation took 3.132 s, which may be a performance bottleneck.
2023-11-15 12:37:46,035	WARNING util.py:315 -- Processing trial results took 3.133 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 12:37:46,035	WARNING util.py:315 -- The `process_trial_result` operation took 3.133 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         ptl/val_accuracy 0.51591
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             ptl/val_loss 0.69275
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:              ptl/val_mcc 0.00443
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                     step 163
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:       time_since_restore 561.38926
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:         time_this_iter_s 561.38926
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:             time_total_s 561.38926
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:                timestamp 1700012262
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d1be5060_5_batch_size=8,cell_type=spinalcord,layer_size=16,lr=0.0048_2023-11-15_09-29-58/wandb/offline-run-20231115_122830-d1be5060
[2m[36m(_WandbLoggingActor pid=1253547)[0m wandb: Find logs at: ./wandb/offline-run-20231115_122830-d1be5060/logs
[2m[36m(TorchTrainer pid=1255059)[0m Starting distributed worker processes: ['1255188 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1255188)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1255188)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1255188)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1255188)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1255188)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1255188)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1255188)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1255188)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1255188)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_a3f9e7db_6_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0007_2023-11-15_12-28-21/lightning_logs
[2m[36m(RayTrainWorker pid=1255188)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1255188)[0m 
[2m[36m(RayTrainWorker pid=1255188)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1255188)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1255188)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1255188)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1255188)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1255188)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1255188)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1255188)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1255188)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1255188)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1255188)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1255188)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1255188)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1255188)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1255188)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1255188)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1255188)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1255188)[0m   
[2m[36m(RayTrainWorker pid=1255188)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_a3f9e7db_6_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0007_2023-11-15_12-28-21/checkpoint_000000)
2023-11-15 12:47:24,600	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.987 s, which may be a performance bottleneck.
2023-11-15 12:47:24,602	WARNING util.py:315 -- The `process_trial_result` operation took 2.991 s, which may be a performance bottleneck.
2023-11-15 12:47:24,602	WARNING util.py:315 -- Processing trial results took 2.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 12:47:24,602	WARNING util.py:315 -- The `process_trial_result` operation took 2.992 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         ptl/val_accuracy 0.47936
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         ptl/val_f1_score 0.64587
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             ptl/val_loss 0.69685
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:              ptl/val_mcc -0.00443
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:        ptl/val_precision 0.47696
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                     step 326
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:       time_since_restore 558.26259
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:         time_this_iter_s 558.26259
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:             time_total_s 558.26259
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:                timestamp 1700012841
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_a3f9e7db_6_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0007_2023-11-15_12-28-21/wandb/offline-run-20231115_123811-a3f9e7db
[2m[36m(_WandbLoggingActor pid=1255185)[0m wandb: Find logs at: ./wandb/offline-run-20231115_123811-a3f9e7db/logs
[2m[36m(TorchTrainer pid=1256690)[0m Starting distributed worker processes: ['1256824 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1256824)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1256824)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1256824)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1256824)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1256824)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1256824)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1256824)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1256824)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1256824)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d30ce9f2_7_batch_size=8,cell_type=spinalcord,layer_size=32,lr=0.0293_2023-11-15_12-38-03/lightning_logs
[2m[36m(RayTrainWorker pid=1256824)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1256824)[0m 
[2m[36m(RayTrainWorker pid=1256824)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1256824)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1256824)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1256824)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=1256824)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1256824)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1256824)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=1256824)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1256824)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=1256824)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1256824)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1256824)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1256824)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1256824)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1256824)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1256824)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1256824)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1256824)[0m   
[2m[36m(RayTrainWorker pid=1256824)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d30ce9f2_7_batch_size=8,cell_type=spinalcord,layer_size=32,lr=0.0293_2023-11-15_12-38-03/checkpoint_000000)
2023-11-15 12:56:56,092	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.698 s, which may be a performance bottleneck.
2023-11-15 12:56:56,093	WARNING util.py:315 -- The `process_trial_result` operation took 2.701 s, which may be a performance bottleneck.
2023-11-15 12:56:56,093	WARNING util.py:315 -- Processing trial results took 2.702 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 12:56:56,093	WARNING util.py:315 -- The `process_trial_result` operation took 2.702 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         ptl/val_accuracy 0.51591
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             ptl/val_loss 0.71005
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:              ptl/val_mcc 0.00443
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                     step 163
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:       time_since_restore 552.25468
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:         time_this_iter_s 552.25468
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:             time_total_s 552.25468
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:                timestamp 1700013413
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_d30ce9f2_7_batch_size=8,cell_type=spinalcord,layer_size=32,lr=0.0293_2023-11-15_12-38-03/wandb/offline-run-20231115_124749-d30ce9f2
[2m[36m(_WandbLoggingActor pid=1256821)[0m wandb: Find logs at: ./wandb/offline-run-20231115_124749-d30ce9f2/logs
[2m[36m(TorchTrainer pid=1258604)[0m Starting distributed worker processes: ['1258734 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1258734)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1258734)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1258734)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1258734)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1258734)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=1258734)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1258734)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1258734)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1258734)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28fc7d1e_8_batch_size=4,cell_type=spinalcord,layer_size=16,lr=0.0001_2023-11-15_12-47-41/lightning_logs
[2m[36m(RayTrainWorker pid=1258734)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1258734)[0m 
[2m[36m(RayTrainWorker pid=1258734)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1258734)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1258734)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1258734)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=1258734)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1258734)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1258734)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=1258734)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1258734)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=1258734)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1258734)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1258734)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1258734)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1258734)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1258734)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1258734)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1258734)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1258734)[0m   
[2m[36m(RayTrainWorker pid=1258734)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28fc7d1e_8_batch_size=4,cell_type=spinalcord,layer_size=16,lr=0.0001_2023-11-15_12-47-41/checkpoint_000000)
2023-11-15 13:06:31,427	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.801 s, which may be a performance bottleneck.
2023-11-15 13:06:31,429	WARNING util.py:315 -- The `process_trial_result` operation took 2.805 s, which may be a performance bottleneck.
2023-11-15 13:06:31,429	WARNING util.py:315 -- Processing trial results took 2.805 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 13:06:31,429	WARNING util.py:315 -- The `process_trial_result` operation took 2.805 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         ptl/val_accuracy 0.47936
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         ptl/val_f1_score 0.64587
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             ptl/val_loss 0.93947
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:              ptl/val_mcc -0.00443
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:        ptl/val_precision 0.47696
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                     step 326
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:       time_since_restore 557.37965
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:         time_this_iter_s 557.37965
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:             time_total_s 557.37965
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:                timestamp 1700013988
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_28fc7d1e_8_batch_size=4,cell_type=spinalcord,layer_size=16,lr=0.0001_2023-11-15_12-47-41/wandb/offline-run-20231115_125718-28fc7d1e
[2m[36m(_WandbLoggingActor pid=1258731)[0m wandb: Find logs at: ./wandb/offline-run-20231115_125718-28fc7d1e/logs
[2m[36m(TorchTrainer pid=1260348)[0m Starting distributed worker processes: ['1260478 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1260478)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1260478)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1260478)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1260478)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1260478)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1260478)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1260478)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1260478)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1260478)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_cb51acc4_9_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0082_2023-11-15_12-57-11/lightning_logs
[2m[36m(RayTrainWorker pid=1260478)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1260478)[0m 
[2m[36m(RayTrainWorker pid=1260478)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1260478)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1260478)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1260478)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1260478)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1260478)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1260478)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1260478)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1260478)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1260478)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1260478)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1260478)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1260478)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1260478)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1260478)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1260478)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1260478)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1260478)[0m   
2023-11-15 13:15:57,303	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=1260478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_cb51acc4_9_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0082_2023-11-15_12-57-11/checkpoint_000000)
2023-11-15 13:16:00,152	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.848 s, which may be a performance bottleneck.
2023-11-15 13:16:00,154	WARNING util.py:315 -- The `process_trial_result` operation took 2.853 s, which may be a performance bottleneck.
2023-11-15 13:16:00,155	WARNING util.py:315 -- Processing trial results took 2.853 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 13:16:00,155	WARNING util.py:315 -- The `process_trial_result` operation took 2.854 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(RayTrainWorker pid=1260478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_cb51acc4_9_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0082_2023-11-15_12-57-11/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       ptl/train_accuracy 0.49898
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:           ptl/train_loss 0.7313
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         ptl/val_accuracy 0.51591
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             ptl/val_loss 0.69339
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:              ptl/val_mcc 0.00443
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                     step 326
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       time_since_restore 1076.66419
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:         time_this_iter_s 526.01828
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:             time_total_s 1076.66419
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:                timestamp 1700015086
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_cb51acc4_9_batch_size=8,cell_type=spinalcord,layer_size=8,lr=0.0082_2023-11-15_12-57-11/wandb/offline-run-20231115_130653-cb51acc4
[2m[36m(_WandbLoggingActor pid=1260475)[0m wandb: Find logs at: ./wandb/offline-run-20231115_130653-cb51acc4/logs
[2m[36m(TorchTrainer pid=1262304)[0m Starting distributed worker processes: ['1262433 (10.6.28.15)']
[2m[36m(RayTrainWorker pid=1262433)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1262433)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1262433)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1262433)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1262433)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1262433)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=1262433)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=1262433)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=1262433)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_2bbcd77c_10_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0325_2023-11-15_13-06-46/lightning_logs
[2m[36m(RayTrainWorker pid=1262433)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=1262433)[0m 
[2m[36m(RayTrainWorker pid=1262433)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=1262433)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1262433)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=1262433)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=1262433)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=1262433)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=1262433)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=1262433)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=1262433)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=1262433)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=1262433)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=1262433)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=1262433)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=1262433)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1262433)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=1262433)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=1262433)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=1262433)[0m   
[2m[36m(RayTrainWorker pid=1262433)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_2bbcd77c_10_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0325_2023-11-15_13-06-46/checkpoint_000000)
2023-11-15 13:34:20,251	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.735 s, which may be a performance bottleneck.
2023-11-15 13:34:20,253	WARNING util.py:315 -- The `process_trial_result` operation took 2.739 s, which may be a performance bottleneck.
2023-11-15 13:34:20,253	WARNING util.py:315 -- Processing trial results took 2.739 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 13:34:20,253	WARNING util.py:315 -- The `process_trial_result` operation took 2.739 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         ptl/val_accuracy 0.52064
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             ptl/val_aupr 0.47696
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             ptl/val_loss 0.72769
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:              ptl/val_mcc 0.00443
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                     step 326
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:       time_since_restore 556.92566
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:         time_this_iter_s 556.92566
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:             time_total_s 556.92566
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:                timestamp 1700015657
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_02-49-29/TorchTrainer_2bbcd77c_10_batch_size=4,cell_type=spinalcord,layer_size=8,lr=0.0325_2023-11-15_13-06-46/wandb/offline-run-20231115_132507-2bbcd77c
[2m[36m(_WandbLoggingActor pid=1262430)[0m wandb: Find logs at: ./wandb/offline-run-20231115_132507-2bbcd77c/logs
