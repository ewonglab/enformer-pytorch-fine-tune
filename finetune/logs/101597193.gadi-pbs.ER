Global seed set to 42
2023-11-18 00:39:44,607	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-18 00:40:04,082	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-18 00:40:04,084	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-18 00:40:04,218	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=650274)[0m Starting distributed worker processes: ['650994 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=650994)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=650994)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=650994)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=650994)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=650994)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:40:50,643	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_037a9332
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=650274, ip=10.6.9.1, actor_id=9bb94c8353c7a0891b300c8f01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=650994, ip=10.6.9.1, actor_id=7b5d1fa66bbee17e59cd00f101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x15238221e250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_037a9332_1_batch_size=4,cell_type=endothelium,layer_size=16,lr=0.0002_2023-11-18_00-40-04/wandb/offline-run-20231118_004034-037a9332
[2m[36m(_WandbLoggingActor pid=650989)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004034-037a9332/logs
[2m[36m(TorchTrainer pid=651423)[0m Starting distributed worker processes: ['651554 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=651554)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=651554)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=651554)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=651554)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=651554)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:41:36,933	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_67bd0079
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=651423, ip=10.6.9.1, actor_id=a20dd4098815c169bc03c0db01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=651554, ip=10.6.9.1, actor_id=328af1e3f30f30945a52d3c401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x15149d0d71c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_67bd0079_2_batch_size=8,cell_type=endothelium,layer_size=8,lr=0.0946_2023-11-18_00-40-21/wandb/offline-run-20231118_004127-67bd0079
[2m[36m(_WandbLoggingActor pid=651549)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004127-67bd0079/logs
[2m[36m(TorchTrainer pid=651943)[0m Starting distributed worker processes: ['652074 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=652074)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=652074)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=652074)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=652074)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=652074)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-18 00:42:14,697	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_8e469c2c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=651943, ip=10.6.9.1, actor_id=596d58a56feb376a43d4faae01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=652074, ip=10.6.9.1, actor_id=3dad2b87db1c1374fb692a3901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14c9c05d11c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_8e469c2c_3_batch_size=4,cell_type=endothelium,layer_size=16,lr=0.0487_2023-11-18_00-41-17/wandb/offline-run-20231118_004205-8e469c2c
[2m[36m(_WandbLoggingActor pid=652069)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004205-8e469c2c/logs
[2m[36m(TrainTrainable pid=652463)[0m Trainable.setup took 12.000 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=652463)[0m Starting distributed worker processes: ['652601 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=652601)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=652601)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=652601)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=652601)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=652601)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:43:14,790	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_98a859cc
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=652463, ip=10.6.9.1, actor_id=c2a40e6debcc7eccc24aa06001000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=652601, ip=10.6.9.1, actor_id=78feeb21791d3becc010b4ff01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x151b0ccd5160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_98a859cc_4_batch_size=4,cell_type=endothelium,layer_size=32,lr=0.0046_2023-11-18_00-41-57/wandb/offline-run-20231118_004300-98a859cc
[2m[36m(_WandbLoggingActor pid=652596)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004300-98a859cc/logs
[2m[36m(TorchTrainer pid=652991)[0m Starting distributed worker processes: ['653121 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=653121)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=653121)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=653121)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=653121)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=653121)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:43:57,146	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_d012816f
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=652991, ip=10.6.9.1, actor_id=4d0ca500a5e00d5d92f869f501000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=653121, ip=10.6.9.1, actor_id=c69b00c220facd4fc14dc17501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14aa0eea5220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_d012816f_5_batch_size=4,cell_type=endothelium,layer_size=16,lr=0.0001_2023-11-18_00-42-44/wandb/offline-run-20231118_004347-d012816f
[2m[36m(_WandbLoggingActor pid=653117)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004347-d012816f/logs
[2m[36m(TrainTrainable pid=653511)[0m Trainable.setup took 10.751 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=653511)[0m Starting distributed worker processes: ['653641 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=653641)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=653641)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=653641)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=653641)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=653641)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:44:47,435	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_258a6bc0
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=653511, ip=10.6.9.1, actor_id=f050f010f9199287c3dd90af01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=653641, ip=10.6.9.1, actor_id=0acfcfe06ed409235fb2143701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x145b33a6b220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_258a6bc0_6_batch_size=4,cell_type=endothelium,layer_size=16,lr=0.0010_2023-11-18_00-43-39/wandb/offline-run-20231118_004434-258a6bc0
[2m[36m(_WandbLoggingActor pid=653636)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004434-258a6bc0/logs
[2m[36m(TrainTrainable pid=654038)[0m Trainable.setup took 14.646 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=654038)[0m Starting distributed worker processes: ['654169 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=654169)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=654169)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=654169)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=654169)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=654169)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:45:52,835	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_5811fd65
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=654038, ip=10.6.9.1, actor_id=02b83d7aab7ffe6d8701f8c901000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=654169, ip=10.6.9.1, actor_id=af70c10bfef7dad0b1ba26bf01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14d8df1a61c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_5811fd65_7_batch_size=8,cell_type=endothelium,layer_size=8,lr=0.0007_2023-11-18_00-44-22/wandb/offline-run-20231118_004542-5811fd65
[2m[36m(_WandbLoggingActor pid=654164)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004542-5811fd65/logs
[2m[36m(TorchTrainer pid=654563)[0m Starting distributed worker processes: ['654693 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=654693)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=654693)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=654693)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=654693)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=654693)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:46:30,254	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_367e1b7c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=654563, ip=10.6.9.1, actor_id=d26829551b5495b46a210cfe01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=654693, ip=10.6.9.1, actor_id=06c6f7c569f6251ba613ee6901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14a8260dd250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_367e1b7c_8_batch_size=4,cell_type=endothelium,layer_size=32,lr=0.0528_2023-11-18_00-45-30/wandb/offline-run-20231118_004620-367e1b7c
[2m[36m(_WandbLoggingActor pid=654688)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004620-367e1b7c/logs
[2m[36m(TorchTrainer pid=655083)[0m Starting distributed worker processes: ['655221 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=655221)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=655221)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=655221)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=655221)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=655221)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:06,998	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_7d83a4f5
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=655083, ip=10.6.9.1, actor_id=4bbac75cdee64de67432cd1901000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=655221, ip=10.6.9.1, actor_id=a9384c69cc159306ce2d3f6701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146f2aba3280>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_7d83a4f5_9_batch_size=8,cell_type=endothelium,layer_size=32,lr=0.0837_2023-11-18_00-46-12/wandb/offline-run-20231118_004656-7d83a4f5
[2m[36m(_WandbLoggingActor pid=655217)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004656-7d83a4f5/logs
[2m[36m(TorchTrainer pid=655611)[0m Starting distributed worker processes: ['655741 (10.6.9.1)']
[2m[36m(RayTrainWorker pid=655741)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=655741)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=655741)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=655741)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=655741)[0m HPU available: False, using: 0 HPUs
2023-11-18 00:47:44,528	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_461cf82c
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=655611, ip=10.6.9.1, actor_id=bd8e0be2e4d9a19d4b86dbc201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=655741, ip=10.6.9.1, actor_id=6dd81cd9c1a227a85993e7ee01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x146593aeb2e0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-18_00-39-38/TorchTrainer_461cf82c_10_batch_size=4,cell_type=endothelium,layer_size=16,lr=0.0001_2023-11-18_00-46-48/wandb/offline-run-20231118_004734-461cf82c
[2m[36m(_WandbLoggingActor pid=655737)[0m wandb: Find logs at: ./wandb/offline-run-20231118_004734-461cf82c/logs
2023-11-18 00:47:50,381	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_037a9332, TorchTrainer_67bd0079, TorchTrainer_8e469c2c, TorchTrainer_98a859cc, TorchTrainer_d012816f, TorchTrainer_258a6bc0, TorchTrainer_5811fd65, TorchTrainer_367e1b7c, TorchTrainer_7d83a4f5, TorchTrainer_461cf82c]
2023-11-18 00:47:50,425	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 368, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
