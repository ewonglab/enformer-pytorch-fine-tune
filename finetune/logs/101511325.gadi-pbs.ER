Global seed set to 42
2023-11-17 05:59:40,492	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-17 05:59:46,829	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-17 05:59:46,856	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-17 05:59:47,644	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=1746001)[0m Starting distributed worker processes: ['1750070 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1750070)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1750070)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1750070)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1750070)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1750070)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:00:18,996	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_86af0e74
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1746001, ip=10.6.10.1, actor_id=bc417e09af2904e8b543b9ae01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1750070, ip=10.6.10.1, actor_id=b56de59e8a8b1dce100a273201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14b75eb231f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_86af0e74_1_batch_size=4,cell_type=gut,layer_size=8,lr=0.0000_2023-11-17_05-59-47/wandb/offline-run-20231117_060008-86af0e74
[2m[36m(_WandbLoggingActor pid=1750065)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060008-86af0e74/logs
[2m[36m(TorchTrainer pid=1750467)[0m Starting distributed worker processes: ['1751784 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1751784)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1751784)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1751784)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1751784)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1751784)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:00:50,599	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_9071cbfb
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1750467, ip=10.6.10.1, actor_id=b337ee4d28725d036f59073c01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1751784, ip=10.6.10.1, actor_id=9c7c4f0ce8023ddd5091b05e01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14ec331e61c0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_9071cbfb_2_batch_size=8,cell_type=gut,layer_size=32,lr=0.0032_2023-11-17_06-00-01/wandb/offline-run-20231117_060041-9071cbfb
[2m[36m(_WandbLoggingActor pid=1751776)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060041-9071cbfb/logs
[2m[36m(TorchTrainer pid=1752177)[0m Starting distributed worker processes: ['1753494 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1753494)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1753494)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1753494)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1753494)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1753494)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:01:21,627	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_916b177f
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1752177, ip=10.6.10.1, actor_id=b65c84711bb84285ee5cd26201000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1753494, ip=10.6.10.1, actor_id=3f703f82a347e8fcef70e67401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1477b808e160>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_916b177f_3_batch_size=4,cell_type=gut,layer_size=32,lr=0.0000_2023-11-17_06-00-35/wandb/offline-run-20231117_060113-916b177f
[2m[36m(_WandbLoggingActor pid=1753489)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060113-916b177f/logs
[2m[36m(TorchTrainer pid=1757132)[0m Starting distributed worker processes: ['1758969 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1758969)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1758969)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1758969)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1758969)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1758969)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:01:55,959	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_16097cc6
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1757132, ip=10.6.10.1, actor_id=e574173259d0d60575e2967c01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1758969, ip=10.6.10.1, actor_id=e1e7b26105449d8bd9b9791e01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x145dd98db220>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_16097cc6_4_batch_size=4,cell_type=gut,layer_size=16,lr=0.0008_2023-11-17_06-01-06/wandb/offline-run-20231117_060146-16097cc6
[2m[36m(_WandbLoggingActor pid=1758964)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060146-16097cc6/logs
[2m[36m(TorchTrainer pid=1762734)[0m Starting distributed worker processes: ['1762864 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1762864)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1762864)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1762864)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1762864)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1762864)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:02:28,191	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_7629128f
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1762734, ip=10.6.10.1, actor_id=63297abfba83a33f2a2fad9401000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1762864, ip=10.6.10.1, actor_id=d2514ba89c3431db47f239be01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14a9c404e250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_7629128f_5_batch_size=8,cell_type=gut,layer_size=16,lr=0.0027_2023-11-17_06-01-39/wandb/offline-run-20231117_060219-7629128f
[2m[36m(_WandbLoggingActor pid=1762861)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060219-7629128f/logs
[2m[36m(TorchTrainer pid=1764445)[0m Starting distributed worker processes: ['1765759 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1765759)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1765759)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1765759)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1765759)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1765759)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:02:59,750	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_ec323c11
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1764445, ip=10.6.10.1, actor_id=e8c176ea15430f4a33e34e6a01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1765759, ip=10.6.10.1, actor_id=63dc69181217e85c5b54ad5101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x151dbfdab280>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_ec323c11_6_batch_size=8,cell_type=gut,layer_size=16,lr=0.0001_2023-11-17_06-02-12/wandb/offline-run-20231117_060251-ec323c11
[2m[36m(_WandbLoggingActor pid=1765756)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060251-ec323c11/logs
[2m[36m(TorchTrainer pid=1767343)[0m Starting distributed worker processes: ['1771037 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1771037)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1771037)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1771037)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1771037)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1771037)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:03:36,619	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_15abee29
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1767343, ip=10.6.10.1, actor_id=fd9ca49df8e3a84a5086f25801000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1771037, ip=10.6.10.1, actor_id=e143f23308f1879422f3f57f01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14f640813250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_15abee29_7_batch_size=4,cell_type=gut,layer_size=32,lr=0.0001_2023-11-17_06-02-44/wandb/offline-run-20231117_060326-15abee29
[2m[36m(_WandbLoggingActor pid=1771034)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060326-15abee29/logs
[2m[36m(TorchTrainer pid=1771437)[0m Starting distributed worker processes: ['1772755 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1772755)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1772755)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1772755)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1772755)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1772755)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:04:08,908	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_5ca3df0f
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1771437, ip=10.6.10.1, actor_id=cb1d6b97e32d08db11d41dbd01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1772755, ip=10.6.10.1, actor_id=6241150385a267049720eb0601000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x143fcbd6b250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_5ca3df0f_8_batch_size=8,cell_type=gut,layer_size=32,lr=0.0002_2023-11-17_06-03-17/wandb/offline-run-20231117_060359-5ca3df0f
[2m[36m(_WandbLoggingActor pid=1772750)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060359-5ca3df0f/logs
[2m[36m(TorchTrainer pid=1774337)[0m Starting distributed worker processes: ['1775654 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1775654)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=1775654)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1775654)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1775654)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1775654)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2023-11-17 06:04:41,730	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_3e78b25e
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1774337, ip=10.6.10.1, actor_id=2c1f01aa7bab3d8ce27f7a6a01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1775654, ip=10.6.10.1, actor_id=beb04853b77633aec72639ae01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x144ff3aab1f0>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_3e78b25e_9_batch_size=8,cell_type=gut,layer_size=16,lr=0.0062_2023-11-17_06-03-52/wandb/offline-run-20231117_060433-3e78b25e
[2m[36m(_WandbLoggingActor pid=1775651)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060433-3e78b25e/logs
[2m[36m(TorchTrainer pid=1778711)[0m Starting distributed worker processes: ['1783304 (10.6.10.1)']
[2m[36m(RayTrainWorker pid=1783304)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=1783304)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=1783304)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=1783304)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=1783304)[0m HPU available: False, using: 0 HPUs
2023-11-17 06:05:15,082	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_550f708a
Traceback (most recent call last):
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): [36mray::_Inner.train()[39m (pid=1778711, ip=10.6.10.1, actor_id=21c7e992b164f0857ac1c52e01000000, repr=TorchTrainer)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(TypeError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=1783304, ip=10.6.10.1, actor_id=26c75193913b7572691bfdc401000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14e1835a8250>)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/train/_internal/utils.py", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "finetune/fine_tune_tidy.py", line 340, in train_func
    model = EnformerFineTuneModel('EleutherAI/enformer-official-rough', config)
  File "finetune/fine_tune_tidy.py", line 66, in __init__
    self.model = BinaryAdapterWrapper(
TypeError: __init__() got an unexpected keyword argument 'num_tracks'
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-17_05-59-33/TorchTrainer_550f708a_10_batch_size=8,cell_type=gut,layer_size=8,lr=0.0003_2023-11-17_06-04-25/wandb/offline-run-20231117_060505-550f708a
[2m[36m(_WandbLoggingActor pid=1783299)[0m wandb: Find logs at: ./wandb/offline-run-20231117_060505-550f708a/logs
2023-11-17 06:05:20,672	ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_86af0e74, TorchTrainer_9071cbfb, TorchTrainer_916b177f, TorchTrainer_16097cc6, TorchTrainer_7629128f, TorchTrainer_ec323c11, TorchTrainer_15abee29, TorchTrainer_5ca3df0f, TorchTrainer_3e78b25e, TorchTrainer_550f708a]
2023-11-17 06:05:20,719	WARNING experiment_analysis.py:596 -- Could not find best trial. Did you pass the correct `metric` parameter?
Traceback (most recent call last):
  File "finetune/fine_tune_tidy.py", line 366, in <module>
    end = results.get_best_result(metric="ptl/val_loss", mode="min")
  File "/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/ray/tune/result_grid.py", line 165, in get_best_result
    raise RuntimeError(error_msg)
RuntimeError: No best trial found for the given metric: ptl/val_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.
