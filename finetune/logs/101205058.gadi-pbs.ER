Global seed set to 42
2023-11-15 01:26:36,800	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 01:26:43,862	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 01:26:43,866	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 01:26:43,894	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=3745644)[0m Starting distributed worker processes: ['3746366 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3746366)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3746366)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3746366)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3746366)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3746366)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3746366)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3746366)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3746366)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3746366)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/lightning_logs
[2m[36m(RayTrainWorker pid=3746366)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3746366)[0m 
[2m[36m(RayTrainWorker pid=3746366)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3746366)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3746366)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3746366)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3746366)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3746366)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3746366)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3746366)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3746366)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3746366)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3746366)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3746366)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3746366)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3746366)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3746366)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3746366)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3746366)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3746366)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 01:31:26,594	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000000)
2023-11-15 01:31:29,756	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.162 s, which may be a performance bottleneck.
2023-11-15 01:31:29,758	WARNING util.py:315 -- The `process_trial_result` operation took 3.164 s, which may be a performance bottleneck.
2023-11-15 01:31:29,758	WARNING util.py:315 -- Processing trial results took 3.165 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:31:29,758	WARNING util.py:315 -- The `process_trial_result` operation took 3.165 s, which may be a performance bottleneck.
2023-11-15 01:35:27,018	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000001)
2023-11-15 01:39:27,994	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000002)
2023-11-15 01:43:27,620	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000003)
2023-11-15 01:47:26,963	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000004)
2023-11-15 01:51:26,096	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000005)
2023-11-15 01:55:25,366	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000006)
2023-11-15 01:59:24,403	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000007)
2023-11-15 02:03:23,808	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000008)
2023-11-15 02:07:23,348	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000009)
2023-11-15 02:11:22,685	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000010)
2023-11-15 02:15:21,943	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000011)
2023-11-15 02:19:21,604	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000012)
2023-11-15 02:23:20,838	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000013)
2023-11-15 02:27:20,215	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000014)
2023-11-15 02:31:19,755	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000015)
2023-11-15 02:35:19,883	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000016)
2023-11-15 02:39:19,531	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000017)
2023-11-15 02:43:18,871	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000018)
[2m[36m(RayTrainWorker pid=3746366)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       ptl/train_accuracy ‚ñá‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:           ptl/train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         ptl/val_accuracy ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         ptl/val_f1_score ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             ptl/val_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:              ptl/val_mcc ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:        ptl/val_precision ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:           ptl/val_recall ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       ptl/train_accuracy 0.46119
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:           ptl/train_loss 0.70107
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         ptl/val_accuracy 0.575
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         ptl/val_f1_score 0.71761
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             ptl/val_loss 0.6833
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:              ptl/val_mcc 0.01728
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:        ptl/val_precision 0.55959
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                     step 1460
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       time_since_restore 4812.77127
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:         time_this_iter_s 238.99637
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:             time_total_s 4812.77127
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:                timestamp 1699976838
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_ced77b96_1_batch_size=8,cell_type=forebrain,layer_size=16,lr=0.0786_2023-11-15_01-26-43/wandb/offline-run-20231115_012705-ced77b96
[2m[36m(_WandbLoggingActor pid=3746361)[0m wandb: Find logs at: ./wandb/offline-run-20231115_012705-ced77b96/logs
[2m[36m(TrainTrainable pid=3760160)[0m Trainable.setup took 31.223 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3760160)[0m Starting distributed worker processes: ['3760298 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3760298)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3760298)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3760298)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3760298)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3760298)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3760298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3760298)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3760298)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3760298)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/lightning_logs
[2m[36m(RayTrainWorker pid=3760298)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3760298)[0m 
[2m[36m(RayTrainWorker pid=3760298)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3760298)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3760298)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3760298)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3760298)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3760298)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3760298)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3760298)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3760298)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3760298)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3760298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3760298)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3760298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3760298)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3760298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3760298)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3760298)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3760298)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 02:53:12,682	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3760298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/checkpoint_000000)
2023-11-15 02:53:17,166	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.483 s, which may be a performance bottleneck.
2023-11-15 02:53:17,167	WARNING util.py:315 -- The `process_trial_result` operation took 4.488 s, which may be a performance bottleneck.
2023-11-15 02:53:17,167	WARNING util.py:315 -- Processing trial results took 4.488 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:53:17,168	WARNING util.py:315 -- The `process_trial_result` operation took 4.489 s, which may be a performance bottleneck.
2023-11-15 02:57:16,445	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3760298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/checkpoint_000001)
2023-11-15 03:01:20,410	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3760298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/checkpoint_000002)
[2m[36m(RayTrainWorker pid=3760298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/checkpoint_000003)
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                    epoch ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: iterations_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:           ptl/train_loss ‚ñà‚ñÉ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         ptl/val_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         ptl/val_f1_score ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             ptl/val_loss ‚ñÅ‚ñÉ‚ñÖ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:              ptl/val_mcc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:        ptl/val_precision ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:           ptl/val_recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                     step ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       time_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             time_total_s ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                timestamp ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       training_iteration ‚ñÅ‚ñÉ‚ñÜ‚ñà
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                    epoch 3
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: iterations_since_restore 4
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       ptl/train_accuracy 0.47126
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:           ptl/train_loss 0.69506
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         ptl/val_accuracy 0.56633
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         ptl/val_f1_score 0.71761
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             ptl/val_loss 0.69031
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:              ptl/val_mcc 0.01728
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:        ptl/val_precision 0.55959
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                     step 580
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       time_since_restore 996.67973
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:         time_this_iter_s 244.10303
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:             time_total_s 996.67973
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:                timestamp 1699977924
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb:       training_iteration 4
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_94c55952_2_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0001_2023-11-15_01-26-58/wandb/offline-run-20231115_024851-94c55952
[2m[36m(_WandbLoggingActor pid=3760295)[0m wandb: Find logs at: ./wandb/offline-run-20231115_024851-94c55952/logs
[2m[36m(TrainTrainable pid=3763168)[0m Trainable.setup took 17.100 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3763168)[0m Starting distributed worker processes: ['3763298 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3763298)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3763298)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3763298)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3763298)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3763298)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3763298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3763298)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3763298)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3763298)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_cf1b3875_3_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0005_2023-11-15_02-48-43/lightning_logs
[2m[36m(RayTrainWorker pid=3763298)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3763298)[0m 
[2m[36m(RayTrainWorker pid=3763298)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3763298)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3763298)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3763298)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3763298)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3763298)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3763298)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3763298)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3763298)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3763298)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3763298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3763298)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3763298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3763298)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3763298)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3763298)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3763298)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3763298)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3763298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_cf1b3875_3_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0005_2023-11-15_02-48-43/checkpoint_000000)
2023-11-15 03:10:40,271	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 03:10:43,126	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.855 s, which may be a performance bottleneck.
2023-11-15 03:10:43,127	WARNING util.py:315 -- The `process_trial_result` operation took 2.860 s, which may be a performance bottleneck.
2023-11-15 03:10:43,128	WARNING util.py:315 -- Processing trial results took 2.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:10:43,128	WARNING util.py:315 -- The `process_trial_result` operation took 2.860 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=3763298)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_cf1b3875_3_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0005_2023-11-15_02-48-43/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                    epoch ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: iterations_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       ptl/train_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:           ptl/train_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         ptl/val_accuracy ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         ptl/val_f1_score ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             ptl/val_loss ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:              ptl/val_mcc ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:        ptl/val_precision ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:           ptl/val_recall ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                     step ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       time_since_restore ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             time_total_s ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                timestamp ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       training_iteration ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       ptl/train_accuracy 0.64655
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:           ptl/train_loss 0.79121
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         ptl/val_accuracy 0.73469
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             ptl/val_aupr 0.92362
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:            ptl/val_auroc 0.92527
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         ptl/val_f1_score 0.80451
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             ptl/val_loss 0.9074
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:              ptl/val_mcc 0.50344
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:        ptl/val_precision 0.67722
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:           ptl/val_recall 0.99074
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                     step 290
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       time_since_restore 520.65125
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:         time_this_iter_s 241.24107
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:             time_total_s 520.65125
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:                timestamp 1699978484
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_cf1b3875_3_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0005_2023-11-15_02-48-43/wandb/offline-run-20231115_030621-cf1b3875
[2m[36m(_WandbLoggingActor pid=3763293)[0m wandb: Find logs at: ./wandb/offline-run-20231115_030621-cf1b3875/logs
[2m[36m(TorchTrainer pid=3765413)[0m Starting distributed worker processes: ['3765544 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3765544)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3765544)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3765544)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3765544)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3765544)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3765544)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3765544)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3765544)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3765544)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_27b583f3_4_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0027_2023-11-15_03-06-00/lightning_logs
[2m[36m(RayTrainWorker pid=3765544)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3765544)[0m 
[2m[36m(RayTrainWorker pid=3765544)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3765544)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3765544)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3765544)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3765544)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3765544)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3765544)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3765544)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3765544)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3765544)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3765544)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3765544)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3765544)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3765544)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3765544)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3765544)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3765544)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3765544)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3765544)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_27b583f3_4_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0027_2023-11-15_03-06-00/checkpoint_000000)
2023-11-15 03:19:29,894	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.914 s, which may be a performance bottleneck.
2023-11-15 03:19:29,896	WARNING util.py:315 -- The `process_trial_result` operation took 3.919 s, which may be a performance bottleneck.
2023-11-15 03:19:29,896	WARNING util.py:315 -- Processing trial results took 3.919 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:19:29,896	WARNING util.py:315 -- The `process_trial_result` operation took 3.919 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         ptl/val_accuracy 0.43367
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             ptl/val_loss 0.73969
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:              ptl/val_mcc -0.01728
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                     step 145
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:       time_since_restore 266.11235
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:         time_this_iter_s 266.11235
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:             time_total_s 266.11235
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:                timestamp 1699978765
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_27b583f3_4_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0027_2023-11-15_03-06-00/wandb/offline-run-20231115_031507-27b583f3
[2m[36m(_WandbLoggingActor pid=3765541)[0m wandb: Find logs at: ./wandb/offline-run-20231115_031507-27b583f3/logs
[2m[36m(TorchTrainer pid=3767208)[0m Starting distributed worker processes: ['3767357 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3767357)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3767357)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3767357)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3767357)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3767357)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3767357)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3767357)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3767357)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3767357)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/lightning_logs
[2m[36m(RayTrainWorker pid=3767357)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3767357)[0m 
[2m[36m(RayTrainWorker pid=3767357)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3767357)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3767357)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3767357)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3767357)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3767357)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3767357)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3767357)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3767357)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3767357)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3767357)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3767357)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3767357)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3767357)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3767357)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3767357)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3767357)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3767357)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 03:24:17,806	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000000)
2023-11-15 03:24:20,836	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.030 s, which may be a performance bottleneck.
2023-11-15 03:24:20,837	WARNING util.py:315 -- The `process_trial_result` operation took 3.034 s, which may be a performance bottleneck.
2023-11-15 03:24:20,838	WARNING util.py:315 -- Processing trial results took 3.034 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:24:20,838	WARNING util.py:315 -- The `process_trial_result` operation took 3.034 s, which may be a performance bottleneck.
2023-11-15 03:28:22,092	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000001)
2023-11-15 03:32:26,091	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000002)
2023-11-15 03:36:30,190	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000003)
2023-11-15 03:40:34,548	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000004)
2023-11-15 03:44:38,678	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000005)
2023-11-15 03:48:42,722	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000006)
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000007)
2023-11-15 03:52:47,068	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 03:56:51,066	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000008)
2023-11-15 04:00:55,022	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000009)
2023-11-15 04:04:59,341	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000010)
2023-11-15 04:09:03,688	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000011)
2023-11-15 04:13:07,854	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000012)
2023-11-15 04:17:11,943	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000013)
2023-11-15 04:21:16,151	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000014)
2023-11-15 04:25:21,003	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000015)
2023-11-15 04:29:25,517	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000016)
2023-11-15 04:33:29,642	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000017)
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000018)
2023-11-15 04:37:33,885	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3767357)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:           ptl/train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         ptl/val_accuracy ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             ptl/val_aupr ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:            ptl/val_auroc ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         ptl/val_f1_score ‚ñà‚ñÅ‚ñá‚ñá‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñá
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             ptl/val_loss ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:              ptl/val_mcc ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:        ptl/val_precision ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÜ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:           ptl/val_recall ‚ñá‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       ptl/train_accuracy 0.47356
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:           ptl/train_loss 0.69962
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         ptl/val_accuracy 0.56633
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         ptl/val_f1_score 0.71761
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             ptl/val_loss 0.68982
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:              ptl/val_mcc 0.01728
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:        ptl/val_precision 0.55959
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                     step 2900
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       time_since_restore 4901.9588
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:         time_this_iter_s 243.9443
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:             time_total_s 4901.9588
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:                timestamp 1699983698
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_78f3ccba_5_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0319_2023-11-15_03-14-59/wandb/offline-run-20231115_031957-78f3ccba
[2m[36m(_WandbLoggingActor pid=3767354)[0m wandb: Find logs at: ./wandb/offline-run-20231115_031957-78f3ccba/logs
[2m[36m(TrainTrainable pid=3782449)[0m Trainable.setup took 22.133 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3782449)[0m Starting distributed worker processes: ['3782618 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3782618)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3782618)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3782618)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3782618)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3782618)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3782618)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3782618)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3782618)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3782618)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/lightning_logs
[2m[36m(RayTrainWorker pid=3782618)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3782618)[0m 
[2m[36m(RayTrainWorker pid=3782618)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3782618)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3782618)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3782618)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3782618)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3782618)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3782618)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3782618)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3782618)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3782618)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3782618)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3782618)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3782618)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3782618)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3782618)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3782618)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3782618)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3782618)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 04:47:06,846	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000000)
2023-11-15 04:47:13,161	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 6.315 s, which may be a performance bottleneck.
2023-11-15 04:47:13,163	WARNING util.py:315 -- The `process_trial_result` operation took 6.318 s, which may be a performance bottleneck.
2023-11-15 04:47:13,163	WARNING util.py:315 -- Processing trial results took 6.319 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:47:13,163	WARNING util.py:315 -- The `process_trial_result` operation took 6.319 s, which may be a performance bottleneck.
2023-11-15 04:51:05,634	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000001)
2023-11-15 04:55:04,561	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000002)
2023-11-15 04:59:03,807	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000003)
2023-11-15 05:03:02,926	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000004)
2023-11-15 05:07:02,320	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000005)
2023-11-15 05:11:01,306	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000006)
2023-11-15 05:15:00,319	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000007)
2023-11-15 05:18:59,527	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000008)
2023-11-15 05:22:58,499	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000009)
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000010)
2023-11-15 05:26:57,770	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 05:30:56,814	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000011)
2023-11-15 05:34:56,048	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000012)
2023-11-15 05:38:55,434	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000013)
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000014)
2023-11-15 05:42:54,524	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000015)
2023-11-15 05:46:53,583	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 05:50:53,783	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000016)
2023-11-15 05:54:52,933	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000017)
2023-11-15 05:58:51,911	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000018)
[2m[36m(RayTrainWorker pid=3782618)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       ptl/train_accuracy ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:           ptl/train_loss ‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         ptl/val_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         ptl/val_f1_score ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             ptl/val_loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:              ptl/val_mcc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:        ptl/val_precision ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:           ptl/val_recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       ptl/train_accuracy 0.47032
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:           ptl/train_loss 0.71989
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         ptl/val_accuracy 0.575
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         ptl/val_f1_score 0.71761
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             ptl/val_loss 0.68221
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:              ptl/val_mcc 0.01728
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:        ptl/val_precision 0.55959
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                     step 1460
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       time_since_restore 4801.33804
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:         time_this_iter_s 239.06994
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:             time_total_s 4801.33804
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:                timestamp 1699988571
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_0626f76c_6_batch_size=8,cell_type=forebrain,layer_size=8,lr=0.0000_2023-11-15_03-19-49/wandb/offline-run-20231115_044248-0626f76c
[2m[36m(_WandbLoggingActor pid=3782617)[0m wandb: Find logs at: ./wandb/offline-run-20231115_044248-0626f76c/logs
[2m[36m(TrainTrainable pid=3798828)[0m Trainable.setup took 11.017 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3798828)[0m Starting distributed worker processes: ['3798966 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3798966)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3798966)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3798966)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3798966)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3798966)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3798966)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3798966)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3798966)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3798966)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_90075d0b_7_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0107_2023-11-15_04-42-39/lightning_logs
[2m[36m(RayTrainWorker pid=3798966)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3798966)[0m 
[2m[36m(RayTrainWorker pid=3798966)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3798966)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3798966)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3798966)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3798966)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3798966)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3798966)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3798966)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3798966)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3798966)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3798966)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3798966)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3798966)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3798966)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3798966)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3798966)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3798966)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3798966)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3798966)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_90075d0b_7_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0107_2023-11-15_04-42-39/checkpoint_000000)
2023-11-15 06:07:53,730	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.618 s, which may be a performance bottleneck.
2023-11-15 06:07:53,731	WARNING util.py:315 -- The `process_trial_result` operation took 2.621 s, which may be a performance bottleneck.
2023-11-15 06:07:53,732	WARNING util.py:315 -- Processing trial results took 2.622 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 06:07:53,732	WARNING util.py:315 -- The `process_trial_result` operation took 2.622 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         ptl/val_accuracy 0.43367
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:            ptl/val_auroc 0.49412
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             ptl/val_loss 0.70338
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:              ptl/val_mcc -0.01728
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                     step 145
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:       time_since_restore 268.08873
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:         time_this_iter_s 268.08873
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:             time_total_s 268.08873
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:                timestamp 1699988871
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_90075d0b_7_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0107_2023-11-15_04-42-39/wandb/offline-run-20231115_060330-90075d0b
[2m[36m(_WandbLoggingActor pid=3798963)[0m wandb: Find logs at: ./wandb/offline-run-20231115_060330-90075d0b/logs
[2m[36m(TorchTrainer pid=3800451)[0m Starting distributed worker processes: ['3800581 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3800581)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3800581)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3800581)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3800581)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3800581)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3800581)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3800581)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3800581)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3800581)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/lightning_logs
[2m[36m(RayTrainWorker pid=3800581)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3800581)[0m 
[2m[36m(RayTrainWorker pid=3800581)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3800581)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3800581)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3800581)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=3800581)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3800581)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3800581)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=3800581)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3800581)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=3800581)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3800581)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3800581)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3800581)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3800581)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3800581)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3800581)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3800581)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3800581)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 06:12:33,155	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000000)
2023-11-15 06:12:35,790	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.634 s, which may be a performance bottleneck.
2023-11-15 06:12:35,791	WARNING util.py:315 -- The `process_trial_result` operation took 2.638 s, which may be a performance bottleneck.
2023-11-15 06:12:35,791	WARNING util.py:315 -- Processing trial results took 2.638 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 06:12:35,791	WARNING util.py:315 -- The `process_trial_result` operation took 2.638 s, which may be a performance bottleneck.
2023-11-15 06:16:36,923	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000001)
2023-11-15 06:20:41,387	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000002)
2023-11-15 06:24:45,649	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000003)
2023-11-15 06:28:49,801	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000004)
2023-11-15 06:32:53,814	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000005)
2023-11-15 06:36:58,088	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000006)
2023-11-15 06:41:02,317	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000007)
2023-11-15 06:45:06,464	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000008)
2023-11-15 06:49:10,725	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000009)
2023-11-15 06:53:14,765	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000010)
2023-11-15 06:57:18,849	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000011)
2023-11-15 07:01:23,055	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000012)
2023-11-15 07:05:27,250	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000013)
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000014)
2023-11-15 07:09:31,327	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 07:13:36,053	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000015)
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000016)
2023-11-15 07:17:40,804	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 07:21:45,065	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000017)
2023-11-15 07:25:49,174	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000018)
[2m[36m(RayTrainWorker pid=3800581)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: iterations_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       ptl/train_accuracy ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:           ptl/train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         ptl/val_accuracy ‚ñà‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             ptl/val_aupr ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:            ptl/val_auroc ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         ptl/val_f1_score ‚ñà‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             ptl/val_loss ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:              ptl/val_mcc ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:        ptl/val_precision ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñÖ‚ñÜ
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:           ptl/val_recall ‚ñá‚ñá‚ñÅ‚ñÉ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñà‚ñá
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                     step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       time_since_restore ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             time_total_s ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       training_iteration ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       ptl/train_accuracy 0.86724
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:           ptl/train_loss 0.32004
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         ptl/val_accuracy 0.84694
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             ptl/val_aupr 0.93316
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:            ptl/val_auroc 0.93453
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         ptl/val_f1_score 0.86842
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             ptl/val_loss 0.36262
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:              ptl/val_mcc 0.68547
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:        ptl/val_precision 0.825
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:           ptl/val_recall 0.91667
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                     step 2900
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       time_since_restore 4898.15027
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:         time_this_iter_s 244.41921
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:             time_total_s 4898.15027
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:                timestamp 1699993793
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_f2b3b15a_8_batch_size=4,cell_type=forebrain,layer_size=16,lr=0.0004_2023-11-15_06-03-23/wandb/offline-run-20231115_060817-f2b3b15a
[2m[36m(_WandbLoggingActor pid=3800578)[0m wandb: Find logs at: ./wandb/offline-run-20231115_060817-f2b3b15a/logs
[2m[36m(TrainTrainable pid=3812255)[0m Trainable.setup took 45.564 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=3812255)[0m Starting distributed worker processes: ['3812398 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3812398)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=3812398)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3812398)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3812398)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3812398)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3812398)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3812398)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3812398)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3812398)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_7eb091be_9_batch_size=8,cell_type=forebrain,layer_size=32,lr=0.0003_2023-11-15_06-08-09/lightning_logs
[2m[36m(RayTrainWorker pid=3812398)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3812398)[0m 
[2m[36m(RayTrainWorker pid=3812398)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3812398)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3812398)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3812398)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=3812398)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3812398)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3812398)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=3812398)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3812398)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=3812398)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3812398)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3812398)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3812398)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3812398)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3812398)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3812398)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3812398)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3812398)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3812398)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_7eb091be_9_batch_size=8,cell_type=forebrain,layer_size=32,lr=0.0003_2023-11-15_06-08-09/checkpoint_000000)
2023-11-15 07:36:35,219	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.886 s, which may be a performance bottleneck.
2023-11-15 07:36:35,221	WARNING util.py:315 -- The `process_trial_result` operation took 2.889 s, which may be a performance bottleneck.
2023-11-15 07:36:35,221	WARNING util.py:315 -- Processing trial results took 2.889 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 07:36:35,221	WARNING util.py:315 -- The `process_trial_result` operation took 2.890 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         ptl/val_accuracy 0.575
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             ptl/val_aupr 0.55959
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         ptl/val_f1_score 0.71761
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             ptl/val_loss 0.69223
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:              ptl/val_mcc 0.01728
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:        ptl/val_precision 0.55959
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                     step 73
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:       time_since_restore 283.24473
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:         time_this_iter_s 283.24473
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:             time_total_s 283.24473
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:                timestamp 1699994192
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_7eb091be_9_batch_size=8,cell_type=forebrain,layer_size=32,lr=0.0003_2023-11-15_06-08-09/wandb/offline-run-20231115_073208-7eb091be
[2m[36m(_WandbLoggingActor pid=3812393)[0m wandb: Find logs at: ./wandb/offline-run-20231115_073208-7eb091be/logs
[2m[36m(TorchTrainer pid=3813882)[0m Starting distributed worker processes: ['3814019 (10.6.10.8)']
[2m[36m(RayTrainWorker pid=3814019)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=3814019)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=3814019)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=3814019)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=3814019)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=3814019)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=3814019)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=3814019)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=3814019)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_26ab9ae5_10_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0080_2023-11-15_07-31-49/lightning_logs
[2m[36m(RayTrainWorker pid=3814019)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=3814019)[0m 
[2m[36m(RayTrainWorker pid=3814019)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=3814019)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3814019)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=3814019)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=3814019)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=3814019)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=3814019)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=3814019)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=3814019)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=3814019)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=3814019)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=3814019)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=3814019)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=3814019)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3814019)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=3814019)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=3814019)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=3814019)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=3814019)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_26ab9ae5_10_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0080_2023-11-15_07-31-49/checkpoint_000000)
2023-11-15 07:41:15,064	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.602 s, which may be a performance bottleneck.
2023-11-15 07:41:15,066	WARNING util.py:315 -- The `process_trial_result` operation took 2.605 s, which may be a performance bottleneck.
2023-11-15 07:41:15,066	WARNING util.py:315 -- Processing trial results took 2.606 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 07:41:15,066	WARNING util.py:315 -- The `process_trial_result` operation took 2.606 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                    epoch ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: iterations_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         ptl/val_accuracy ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             ptl/val_aupr ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:            ptl/val_auroc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         ptl/val_f1_score ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             ptl/val_loss ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:              ptl/val_mcc ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:        ptl/val_precision ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:           ptl/val_recall ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                     step ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:       time_since_restore ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         time_this_iter_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             time_total_s ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                timestamp ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:       training_iteration ‚ñÅ
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         ptl/val_accuracy 0.78571
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             ptl/val_aupr 0.90331
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:            ptl/val_auroc 0.90496
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         ptl/val_f1_score 0.82787
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             ptl/val_loss 2.14404
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:              ptl/val_mcc 0.56959
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:        ptl/val_precision 0.74265
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:           ptl/val_recall 0.93519
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                     step 145
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:       time_since_restore 262.71902
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:         time_this_iter_s 262.71902
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:             time_total_s 262.71902
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:                timestamp 1699994472
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-33/TorchTrainer_26ab9ae5_10_batch_size=4,cell_type=forebrain,layer_size=8,lr=0.0080_2023-11-15_07-31-49/wandb/offline-run-20231115_073656-26ab9ae5
[2m[36m(_WandbLoggingActor pid=3814016)[0m wandb: Find logs at: ./wandb/offline-run-20231115_073656-26ab9ae5/logs
