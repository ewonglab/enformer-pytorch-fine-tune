Global seed set to 42
2023-11-15 01:26:39,383	INFO worker.py:1642 -- Started a local Ray instance.
2023-11-15 01:26:46,020	INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2023-11-15 01:26:46,023	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-11-15 01:26:46,050	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TorchTrainer pid=804635)[0m Starting distributed worker processes: ['805358 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=805358)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=805358)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=805358)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=805358)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=805358)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=805358)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=805358)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=805358)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=805358)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/lightning_logs
[2m[36m(RayTrainWorker pid=805358)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=805358)[0m 
[2m[36m(RayTrainWorker pid=805358)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=805358)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=805358)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=805358)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=805358)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=805358)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=805358)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=805358)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=805358)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=805358)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=805358)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=805358)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=805358)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=805358)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=805358)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=805358)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=805358)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=805358)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=805358)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 01:31:42,993	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000000)
2023-11-15 01:31:46,161	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.167 s, which may be a performance bottleneck.
2023-11-15 01:31:46,162	WARNING util.py:315 -- The `process_trial_result` operation took 3.169 s, which may be a performance bottleneck.
2023-11-15 01:31:46,162	WARNING util.py:315 -- Processing trial results took 3.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 01:31:46,163	WARNING util.py:315 -- The `process_trial_result` operation took 3.170 s, which may be a performance bottleneck.
2023-11-15 01:36:02,954	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000001)
2023-11-15 01:40:22,108	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000002)
2023-11-15 01:44:40,949	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000003)
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000004)
2023-11-15 01:49:00,461	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 01:53:19,373	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000005)
2023-11-15 01:57:38,779	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000006)
2023-11-15 02:01:57,742	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000007)
2023-11-15 02:06:16,668	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000008)
2023-11-15 02:10:35,577	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000009)
2023-11-15 02:14:54,449	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000010)
2023-11-15 02:19:13,684	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000011)
2023-11-15 02:23:32,636	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000012)
2023-11-15 02:27:51,832	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000013)
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000014)
2023-11-15 02:32:10,909	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 02:36:30,118	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000015)
2023-11-15 02:40:49,153	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000016)
2023-11-15 02:45:08,299	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000017)
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000018)
2023-11-15 02:49:27,202	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=805358)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       ptl/train_accuracy ▆█▄▂▂▂▃▂▂▂▁▂▃▁▂▂▃▃▂
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:           ptl/train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         ptl/val_accuracy ▆█▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▂▂▂
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             ptl/val_aupr ▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:            ptl/val_auroc ▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         ptl/val_f1_score ▆█▁▂▂▁▇▁▁▁▁▁▇▁▁▁▁▇▇▇
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             ptl/val_loss ▅▁██████████████████
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:              ptl/val_mcc ▆█▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:        ptl/val_precision █▇█▇█▁▅▁▁▁▁▁▅▁▁▁▁▅▅▅
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:           ptl/val_recall ▅▇▁▁▁▁█▁▁▁▁▁█▁▁▁▁███
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       ptl/train_accuracy 0.47448
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:           ptl/train_loss 0.69603
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         ptl/val_accuracy 0.53704
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         ptl/val_f1_score 0.69136
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             ptl/val_loss 0.69091
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:              ptl/val_mcc 0.00779
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:        ptl/val_precision 0.5283
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                     step 1600
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       time_since_restore 5196.74496
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:         time_this_iter_s 258.65731
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:             time_total_s 5196.74496
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:                timestamp 1699977226
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_3f79fea2_1_batch_size=8,cell_type=mesenchyme,layer_size=32,lr=0.0176_2023-11-15_01-26-46/wandb/offline-run-20231115_012710-3f79fea2
[2m[36m(_WandbLoggingActor pid=805353)[0m wandb: Find logs at: ./wandb/offline-run-20231115_012710-3f79fea2/logs
[2m[36m(TrainTrainable pid=819165)[0m Trainable.setup took 13.445 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=819165)[0m Starting distributed worker processes: ['819303 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=819303)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=819303)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=819303)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=819303)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=819303)[0m HPU available: False, using: 0 HPUs
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=819303)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=819303)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=819303)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=819303)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_42e3f847_2_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0005_2023-11-15_01-27-01/lightning_logs
[2m[36m(RayTrainWorker pid=819303)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=819303)[0m 
[2m[36m(RayTrainWorker pid=819303)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=819303)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=819303)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=819303)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=819303)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=819303)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=819303)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=819303)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=819303)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=819303)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=819303)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=819303)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=819303)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=819303)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=819303)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=819303)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=819303)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=819303)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=819303)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=819303)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_42e3f847_2_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0005_2023-11-15_01-27-01/checkpoint_000000)
2023-11-15 02:59:29,564	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 13.969 s, which may be a performance bottleneck.
2023-11-15 02:59:29,565	WARNING util.py:315 -- The `process_trial_result` operation took 13.974 s, which may be a performance bottleneck.
2023-11-15 02:59:29,566	WARNING util.py:315 -- Processing trial results took 13.974 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 02:59:29,566	WARNING util.py:315 -- The `process_trial_result` operation took 13.975 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         ptl/val_accuracy 0.5283
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         ptl/val_f1_score 0.69136
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             ptl/val_loss 0.69155
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:              ptl/val_mcc 0.00779
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:        ptl/val_precision 0.5283
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                     step 159
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:       time_since_restore 293.86003
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:         time_this_iter_s 293.86003
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:             time_total_s 293.86003
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:                timestamp 1699977555
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_42e3f847_2_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0005_2023-11-15_01-27-01/wandb/offline-run-20231115_025432-42e3f847
[2m[36m(_WandbLoggingActor pid=819298)[0m wandb: Find logs at: ./wandb/offline-run-20231115_025432-42e3f847/logs
[2m[36m(TrainTrainable pid=820788)[0m Trainable.setup took 21.774 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=820788)[0m Starting distributed worker processes: ['820929 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=820929)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=820929)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=820929)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=820929)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=820929)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=820929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=820929)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=820929)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=820929)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_320c38d4_3_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0000_2023-11-15_02-54-21/lightning_logs
[2m[36m(RayTrainWorker pid=820929)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=820929)[0m 
[2m[36m(RayTrainWorker pid=820929)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=820929)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=820929)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=820929)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=820929)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=820929)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=820929)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=820929)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=820929)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=820929)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=820929)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=820929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=820929)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=820929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=820929)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=820929)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=820929)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=820929)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=820929)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=820929)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_320c38d4_3_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0000_2023-11-15_02-54-21/checkpoint_000000)
2023-11-15 03:06:03,071	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 8.885 s, which may be a performance bottleneck.
2023-11-15 03:06:03,073	WARNING util.py:315 -- The `process_trial_result` operation took 8.889 s, which may be a performance bottleneck.
2023-11-15 03:06:03,073	WARNING util.py:315 -- Processing trial results took 8.890 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:06:03,074	WARNING util.py:315 -- The `process_trial_result` operation took 8.890 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         ptl/val_accuracy 0.56132
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             ptl/val_aupr 0.87667
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:            ptl/val_auroc 0.86723
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         ptl/val_f1_score 0.30075
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             ptl/val_loss 0.66485
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:              ptl/val_mcc 0.28169
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:        ptl/val_precision 0.95238
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:           ptl/val_recall 0.17857
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                     step 159
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:       time_since_restore 323.56725
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:         time_this_iter_s 323.56725
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:             time_total_s 323.56725
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:                timestamp 1699977954
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_320c38d4_3_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0000_2023-11-15_02-54-21/wandb/offline-run-20231115_030056-320c38d4
[2m[36m(_WandbLoggingActor pid=820924)[0m wandb: Find logs at: ./wandb/offline-run-20231115_030056-320c38d4/logs
[2m[36m(TorchTrainer pid=822412)[0m Starting distributed worker processes: ['822550 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=822550)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=822550)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=822550)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=822550)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=822550)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=822550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=822550)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=822550)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=822550)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/lightning_logs
[2m[36m(RayTrainWorker pid=822550)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=822550)[0m 
[2m[36m(RayTrainWorker pid=822550)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=822550)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=822550)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=822550)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=822550)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=822550)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=822550)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=822550)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=822550)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=822550)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=822550)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=822550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=822550)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=822550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=822550)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=822550)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=822550)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=822550)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=822550)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 03:11:13,017	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000000)
2023-11-15 03:11:15,850	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.833 s, which may be a performance bottleneck.
2023-11-15 03:11:15,851	WARNING util.py:315 -- The `process_trial_result` operation took 2.837 s, which may be a performance bottleneck.
2023-11-15 03:11:15,852	WARNING util.py:315 -- Processing trial results took 2.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 03:11:15,852	WARNING util.py:315 -- The `process_trial_result` operation took 2.837 s, which may be a performance bottleneck.
2023-11-15 03:15:30,955	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000001)
2023-11-15 03:19:49,480	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000002)
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000003)
2023-11-15 03:24:07,655	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 03:28:26,035	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000004)
2023-11-15 03:32:44,272	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000005)
2023-11-15 03:37:02,434	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000006)
2023-11-15 03:41:20,647	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000007)
2023-11-15 03:45:38,890	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000008)
2023-11-15 03:49:57,222	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000009)
2023-11-15 03:54:15,282	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000010)
2023-11-15 03:58:33,535	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000011)
2023-11-15 04:02:51,853	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000012)
2023-11-15 04:07:10,237	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000013)
2023-11-15 04:11:28,409	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000014)
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000015)
2023-11-15 04:15:47,480	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
2023-11-15 04:20:06,184	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000016)
2023-11-15 04:24:24,434	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000017)
2023-11-15 04:28:42,753	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000018)
[2m[36m(RayTrainWorker pid=822550)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/checkpoint_000019)
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                    epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       ptl/train_accuracy ▁▅▅▄▆▇▆▇▇▇▇▇▇▇█▇█▇█
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:           ptl/train_loss █▄▃▄▃▂▂▂▁▂▂▁▃▃▁▂▁▂▁
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         ptl/val_accuracy ▁▅▂▂█▇▃▄█▂█▆▅▇▇▆▆██▇
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             ptl/val_aupr ▄▃▇▅▂▅▅▅▆█▆▂▄▂▄▃▅▁▂▃
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:            ptl/val_auroc ▄▃▄▄▁▄▄▄▆█▆▃▃▃▅▃▆▂▃▄
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         ptl/val_f1_score ▁▄▆▆██▆▄█▇█▆▇▇█▇▇██▇
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             ptl/val_loss ▅▃▅▆▁▂▆▆▂█▁▃▅▂▂▄▄▂▃▂
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:              ptl/val_mcc ▃▆▁▁██▂▆█▃█▇▅▆▇▆▆▇█▇
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:        ptl/val_precision █▇▁▁▅▃▁█▄▁▅▆▂▄▄▃▃▄▄▄
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:           ptl/val_recall ▁▃██▆▇█▂▇█▅▄▇▆▇▇▇▆▇▆
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                     step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         time_this_iter_s █▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                timestamp ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       training_iteration ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                    epoch 19
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: iterations_since_restore 20
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       ptl/train_accuracy 0.89688
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:           ptl/train_loss 0.25903
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         ptl/val_accuracy 0.80556
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             ptl/val_aupr 0.9307
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:            ptl/val_auroc 0.91679
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         ptl/val_f1_score 0.81778
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             ptl/val_loss 0.38329
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:              ptl/val_mcc 0.61178
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:        ptl/val_precision 0.81416
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:           ptl/val_recall 0.82143
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                     step 1600
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       time_since_restore 5183.03673
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:         time_this_iter_s 258.21224
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:             time_total_s 5183.03673
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:                timestamp 1699983181
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb:       training_iteration 20
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_16bc4539_4_batch_size=8,cell_type=mesenchyme,layer_size=8,lr=0.0005_2023-11-15_03-00-30/wandb/offline-run-20231115_030640-16bc4539
[2m[36m(_WandbLoggingActor pid=822547)[0m wandb: Find logs at: ./wandb/offline-run-20231115_030640-16bc4539/logs
[2m[36m(TrainTrainable pid=837841)[0m Trainable.setup took 15.929 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=837841)[0m Starting distributed worker processes: ['837988 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=837988)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=837988)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=837988)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=837988)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=837988)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=837988)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=837988)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=837988)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=837988)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_eebae444_5_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0099_2023-11-15_03-06-31/lightning_logs
[2m[36m(RayTrainWorker pid=837988)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=837988)[0m 
[2m[36m(RayTrainWorker pid=837988)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=837988)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=837988)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=837988)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=837988)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=837988)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=837988)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=837988)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=837988)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=837988)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=837988)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=837988)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=837988)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=837988)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=837988)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=837988)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=837988)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=837988)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=837988)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=837988)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_eebae444_5_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0099_2023-11-15_03-06-31/checkpoint_000000)
2023-11-15 04:38:34,459	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 6.962 s, which may be a performance bottleneck.
2023-11-15 04:38:34,461	WARNING util.py:315 -- The `process_trial_result` operation took 6.966 s, which may be a performance bottleneck.
2023-11-15 04:38:34,461	WARNING util.py:315 -- Processing trial results took 6.966 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:38:34,461	WARNING util.py:315 -- The `process_trial_result` operation took 6.966 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         ptl/val_accuracy 0.46296
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             ptl/val_loss 0.69548
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:              ptl/val_mcc -0.00779
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                     step 80
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:       time_since_restore 282.85269
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:         time_this_iter_s 282.85269
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:             time_total_s 282.85269
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:                timestamp 1699983507
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_eebae444_5_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0099_2023-11-15_03-06-31/wandb/offline-run-20231115_043352-eebae444
[2m[36m(_WandbLoggingActor pid=837985)[0m wandb: Find logs at: ./wandb/offline-run-20231115_043352-eebae444/logs
[2m[36m(TrainTrainable pid=839676)[0m Trainable.setup took 18.414 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=839676)[0m Starting distributed worker processes: ['839838 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=839838)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=839838)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=839838)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=839838)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=839838)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=839838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=839838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=839838)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=839838)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_708ef625_6_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0141_2023-11-15_04-33-44/lightning_logs
[2m[36m(RayTrainWorker pid=839838)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=839838)[0m 
[2m[36m(RayTrainWorker pid=839838)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=839838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=839838)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=839838)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=839838)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=839838)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=839838)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=839838)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=839838)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=839838)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=839838)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=839838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=839838)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=839838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=839838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=839838)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=839838)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=839838)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=839838)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=839838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_708ef625_6_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0141_2023-11-15_04-33-44/checkpoint_000000)
2023-11-15 04:44:31,751	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.077 s, which may be a performance bottleneck.
2023-11-15 04:44:31,752	WARNING util.py:315 -- The `process_trial_result` operation took 5.080 s, which may be a performance bottleneck.
2023-11-15 04:44:31,753	WARNING util.py:315 -- Processing trial results took 5.080 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:44:31,753	WARNING util.py:315 -- The `process_trial_result` operation took 5.081 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         ptl/val_accuracy 0.46296
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             ptl/val_loss 0.70268
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:              ptl/val_mcc -0.00779
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                     step 80
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:       time_since_restore 311.07845
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:         time_this_iter_s 311.07845
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:             time_total_s 311.07845
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:                timestamp 1699983866
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_708ef625_6_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0141_2023-11-15_04-33-44/wandb/offline-run-20231115_043939-708ef625
[2m[36m(_WandbLoggingActor pid=839833)[0m wandb: Find logs at: ./wandb/offline-run-20231115_043939-708ef625/logs
[2m[36m(TorchTrainer pid=841525)[0m Starting distributed worker processes: ['841671 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=841671)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=841671)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=841671)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=841671)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=841671)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=841671)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=841671)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=841671)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=841671)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_437a0dd0_7_batch_size=4,cell_type=mesenchyme,layer_size=16,lr=0.0035_2023-11-15_04-39-15/lightning_logs
[2m[36m(RayTrainWorker pid=841671)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=841671)[0m 
[2m[36m(RayTrainWorker pid=841671)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=841671)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=841671)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=841671)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=841671)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=841671)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=841671)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=841671)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=841671)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=841671)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=841671)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=841671)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=841671)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=841671)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=841671)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=841671)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=841671)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=841671)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=841671)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=841671)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_437a0dd0_7_batch_size=4,cell_type=mesenchyme,layer_size=16,lr=0.0035_2023-11-15_04-39-15/checkpoint_000000)
2023-11-15 04:49:38,917	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 7.808 s, which may be a performance bottleneck.
2023-11-15 04:49:38,918	WARNING util.py:315 -- The `process_trial_result` operation took 7.812 s, which may be a performance bottleneck.
2023-11-15 04:49:38,919	WARNING util.py:315 -- Processing trial results took 7.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:49:38,919	WARNING util.py:315 -- The `process_trial_result` operation took 7.813 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         ptl/val_accuracy 0.4717
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             ptl/val_loss 0.69336
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:              ptl/val_mcc -0.00779
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                     step 159
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:       time_since_restore 283.01439
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:         time_this_iter_s 283.01439
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:             time_total_s 283.01439
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:                timestamp 1699984171
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_437a0dd0_7_batch_size=4,cell_type=mesenchyme,layer_size=16,lr=0.0035_2023-11-15_04-39-15/wandb/offline-run-20231115_044455-437a0dd0
[2m[36m(_WandbLoggingActor pid=841668)[0m wandb: Find logs at: ./wandb/offline-run-20231115_044455-437a0dd0/logs
[2m[36m(TorchTrainer pid=843335)[0m Starting distributed worker processes: ['843481 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=843481)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=843481)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=843481)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=843481)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=843481)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=843481)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=843481)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=843481)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=843481)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_9f83f69c_8_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0071_2023-11-15_04-44-48/lightning_logs
[2m[36m(RayTrainWorker pid=843481)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=843481)[0m 
[2m[36m(RayTrainWorker pid=843481)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=843481)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=843481)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=843481)[0m 1 | model             | BinaryAdapterWrapper   | 254 M 
[2m[36m(RayTrainWorker pid=843481)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=843481)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=843481)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=843481)[0m 254 M     Trainable params
[2m[36m(RayTrainWorker pid=843481)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=843481)[0m 254 M     Total params
[2m[36m(RayTrainWorker pid=843481)[0m 1,017.468 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=843481)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=843481)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=843481)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=843481)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=843481)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=843481)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=843481)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=843481)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=843481)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_9f83f69c_8_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0071_2023-11-15_04-44-48/checkpoint_000000)
2023-11-15 04:55:01,071	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 9.198 s, which may be a performance bottleneck.
2023-11-15 04:55:01,073	WARNING util.py:315 -- The `process_trial_result` operation took 9.202 s, which may be a performance bottleneck.
2023-11-15 04:55:01,073	WARNING util.py:315 -- Processing trial results took 9.202 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 04:55:01,073	WARNING util.py:315 -- The `process_trial_result` operation took 9.202 s, which may be a performance bottleneck.
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         ptl/val_accuracy 0.4717
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             ptl/val_loss 0.6951
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:              ptl/val_mcc -0.00779
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                     step 159
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:       time_since_restore 291.82457
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:         time_this_iter_s 291.82457
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:             time_total_s 291.82457
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:                timestamp 1699984491
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_9f83f69c_8_batch_size=4,cell_type=mesenchyme,layer_size=8,lr=0.0071_2023-11-15_04-44-48/wandb/offline-run-20231115_045013-9f83f69c
[2m[36m(_WandbLoggingActor pid=843478)[0m wandb: Find logs at: ./wandb/offline-run-20231115_045013-9f83f69c/logs
[2m[36m(TrainTrainable pid=845168)[0m Trainable.setup took 22.738 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=845168)[0m Starting distributed worker processes: ['845333 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=845333)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=845333)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=845333)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=845333)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=845333)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=845333)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=845333)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=845333)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=845333)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_58f1047c_9_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0003_2023-11-15_04-50-00/lightning_logs
[2m[36m(RayTrainWorker pid=845333)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=845333)[0m 
[2m[36m(RayTrainWorker pid=845333)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=845333)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=845333)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=845333)[0m 1 | model             | BinaryAdapterWrapper   | 257 M 
[2m[36m(RayTrainWorker pid=845333)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=845333)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=845333)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=845333)[0m 257 M     Trainable params
[2m[36m(RayTrainWorker pid=845333)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=845333)[0m 257 M     Total params
[2m[36m(RayTrainWorker pid=845333)[0m 1,030.051 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=845333)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=845333)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=845333)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=845333)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=845333)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=845333)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=845333)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=845333)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
2023-11-15 05:01:00,550	WARNING bohb_search.py:249 -- BOHB Info not detected in result. Are you using HyperBandForBOHB as a scheduler?
[2m[36m(RayTrainWorker pid=845333)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_58f1047c_9_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0003_2023-11-15_04-50-00/checkpoint_000000)
2023-11-15 05:01:03,344	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.793 s, which may be a performance bottleneck.
2023-11-15 05:01:03,345	WARNING util.py:315 -- The `process_trial_result` operation took 2.797 s, which may be a performance bottleneck.
2023-11-15 05:01:03,345	WARNING util.py:315 -- Processing trial results took 2.797 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 05:01:03,345	WARNING util.py:315 -- The `process_trial_result` operation took 2.797 s, which may be a performance bottleneck.
[2m[36m(RayTrainWorker pid=845333)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_58f1047c_9_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0003_2023-11-15_04-50-00/checkpoint_000001)
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                    epoch ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: iterations_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       ptl/train_accuracy ▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:           ptl/train_loss ▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         ptl/val_accuracy ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             ptl/val_aupr ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:            ptl/val_auroc ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         ptl/val_f1_score ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             ptl/val_loss ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:              ptl/val_mcc ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:        ptl/val_precision ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:           ptl/val_recall ▁▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                     step ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       time_since_restore ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         time_this_iter_s █▁
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             time_total_s ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                timestamp ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       training_iteration ▁█
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                    epoch 1
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: iterations_since_restore 2
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       ptl/train_accuracy 0.48854
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:           ptl/train_loss 0.72369
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         ptl/val_accuracy 0.53704
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         ptl/val_f1_score 0.69136
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             ptl/val_loss 0.6923
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:              ptl/val_mcc 0.00779
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:        ptl/val_precision 0.5283
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:           ptl/val_recall 1.0
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                     step 160
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       time_since_restore 562.96385
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:         time_this_iter_s 255.44554
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:             time_total_s 562.96385
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:                timestamp 1699985118
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb:       training_iteration 2
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_58f1047c_9_batch_size=8,cell_type=mesenchyme,layer_size=16,lr=0.0003_2023-11-15_04-50-00/wandb/offline-run-20231115_045615-58f1047c
[2m[36m(_WandbLoggingActor pid=845328)[0m wandb: Find logs at: ./wandb/offline-run-20231115_045615-58f1047c/logs
[2m[36m(TorchTrainer pid=847489)[0m Starting distributed worker processes: ['847619 (10.6.11.4)']
[2m[36m(RayTrainWorker pid=847619)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Tracking run with wandb version 0.15.10
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: W&B syncing is set to `offline` in this directory.  
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2m[36m(RayTrainWorker pid=847619)[0m GPU available: True (cuda), used: True
[2m[36m(RayTrainWorker pid=847619)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=847619)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=847619)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=847619)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[2m[36m(RayTrainWorker pid=847619)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=847619)[0m [rank: 0] Global seed set to 42
[2m[36m(RayTrainWorker pid=847619)[0m Missing logger folder: /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_62918cd2_10_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0476_2023-11-15_04-55-53/lightning_logs
[2m[36m(RayTrainWorker pid=847619)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[2m[36m(RayTrainWorker pid=847619)[0m 
[2m[36m(RayTrainWorker pid=847619)[0m   | Name              | Type                   | Params
[2m[36m(RayTrainWorker pid=847619)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=847619)[0m 0 | enformer          | Enformer               | 251 M 
[2m[36m(RayTrainWorker pid=847619)[0m 1 | model             | BinaryAdapterWrapper   | 263 M 
[2m[36m(RayTrainWorker pid=847619)[0m 2 | matthews_corrcoef | BinaryMatthewsCorrCoef | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 3 | f1_score          | BinaryF1Score          | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 4 | precision         | BinaryPrecision        | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 5 | recall            | BinaryRecall           | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 6 | aupr              | BinaryAveragePrecision | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 7 | cfm               | BinaryConfusionMatrix  | 0     
[2m[36m(RayTrainWorker pid=847619)[0m 8 | auroc             | BinaryAUROC            | 0     
[2m[36m(RayTrainWorker pid=847619)[0m -------------------------------------------------------------
[2m[36m(RayTrainWorker pid=847619)[0m 263 M     Trainable params
[2m[36m(RayTrainWorker pid=847619)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=847619)[0m 263 M     Total params
[2m[36m(RayTrainWorker pid=847619)[0m 1,055.217 Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=847619)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[2m[36m(RayTrainWorker pid=847619)[0m   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2m[36m(RayTrainWorker pid=847619)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
[2m[36m(RayTrainWorker pid=847619)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=847619)[0m /g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
[2m[36m(RayTrainWorker pid=847619)[0m   warnings.warn(*args, **kwargs)  # noqa: B028
[2m[36m(RayTrainWorker pid=847619)[0m finetune/fine_tune_tidy.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[2m[36m(RayTrainWorker pid=847619)[0m   mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(RayTrainWorker pid=847619)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_62918cd2_10_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0476_2023-11-15_04-55-53/checkpoint_000000)
2023-11-15 05:10:27,295	WARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.786 s, which may be a performance bottleneck.
2023-11-15 05:10:27,297	WARNING util.py:315 -- The `process_trial_result` operation took 2.790 s, which may be a performance bottleneck.
2023-11-15 05:10:27,297	WARNING util.py:315 -- Processing trial results took 2.791 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.
2023-11-15 05:10:27,297	WARNING util.py:315 -- The `process_trial_result` operation took 2.791 s, which may be a performance bottleneck.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/g/data/zk16/zelun/miniconda3/envs/enformer-fine-tune/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028
/g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/finetune/fine_tune_tidy.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mcc = torch.tensor(self.matthews_corrcoef(probs, targets.int()), dtype=torch.float32)
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Waiting for W&B process to finish... (success).
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Run history:
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                    epoch ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: iterations_since_restore ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         ptl/val_accuracy ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             ptl/val_aupr ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:            ptl/val_auroc ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         ptl/val_f1_score ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             ptl/val_loss ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:              ptl/val_mcc ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:        ptl/val_precision ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:           ptl/val_recall ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                     step ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:       time_since_restore ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         time_this_iter_s ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             time_total_s ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                timestamp ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:       training_iteration ▁
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Run summary:
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                    epoch 0
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: iterations_since_restore 1
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         ptl/val_accuracy 0.4717
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             ptl/val_aupr 0.5283
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:            ptl/val_auroc 0.5
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         ptl/val_f1_score 0.0
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             ptl/val_loss 0.69858
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:              ptl/val_mcc -0.00779
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:        ptl/val_precision 0.0
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:           ptl/val_recall 0.0
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                     step 159
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:       time_since_restore 288.33705
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:         time_this_iter_s 288.33705
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:             time_total_s 288.33705
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:                timestamp 1699985424
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb:       training_iteration 1
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: 
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: You can sync this run to the cloud by running:
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: wandb sync /g/data/zk16/zelun/z_li_hon/wonglab_github/enformer-pytorch-fine-tune/ray_results/TorchTrainer_2023-11-15_01-26-35/TorchTrainer_62918cd2_10_batch_size=4,cell_type=mesenchyme,layer_size=32,lr=0.0476_2023-11-15_04-55-53/wandb/offline-run-20231115_050544-62918cd2
[2m[36m(_WandbLoggingActor pid=847616)[0m wandb: Find logs at: ./wandb/offline-run-20231115_050544-62918cd2/logs
